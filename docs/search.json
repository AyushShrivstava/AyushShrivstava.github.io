[
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Gallery",
    "section": "",
    "text": "Welcome to my gallery! Here you’ll find a collection of images from my life, including memorable moments and photos I’ve taken. I’ll continue to add more images as I capture new experiences and milestones along my journey.\n\n\n\n\n\n\nTaking a session on “Seeing Red”. Estimating Heart rate from Smartphone Camera\n\n\n\n\n\n\n\n\n\n\nExplaining our project at the innovation expo I-Inventive\n\n\n\n\n\nPosing with Dr. Rishiraj (left) infront of the Stall, IIT Madras\n\n\n\n\n\nExplaining Apnea project (Right)\n\n\n\n\n\nDeer from IIT Madras\n\n\n\n\n\n\n\n\n\n\nAttending COMSNETS 2025\n\n\n\n\n\nPresenting “Towards Sleep Apnea Screening via Thermal Imagery”\n\n\n\n\n\nInside IIIT Delhi Campus\n\n\n\n\n\n\n\n\n\n\n\nAttending ACM Compass with the Lab\n\n\n\n\n\nACM Compass - infront of conference hall\n\n\n\n\n\nInside IIIT Delhi Campus\n\n\n\n\n\n\n\n\n\n\nAwarded the MTech in Computer Science and Engineering at IITGN convocation June 2024\n\n\n\n\n\nOutside IITGN Convocation Hall June 2024\n\n\n\n\n\nOutside IITGN Convocation Hall with my family\n\n\n\n\n\n\n\n\n\n\nSuraj and I celebrating the successful defense of our MTech thesis\n\n\n\n\n\nCelebrating the successful defense of our MTech thesis"
  },
  {
    "objectID": "gallery.html#i-inventive-2025-iit-madras",
    "href": "gallery.html#i-inventive-2025-iit-madras",
    "title": "Gallery",
    "section": "",
    "text": "Explaining our project at the innovation expo I-Inventive\n\n\n\n\n\nPosing with Dr. Rishiraj (left) infront of the Stall, IIT Madras\n\n\n\n\n\nExplaining Apnea project (Right)\n\n\n\n\n\nDeer from IIT Madras"
  },
  {
    "objectID": "gallery.html#comsnets-2025bangalore",
    "href": "gallery.html#comsnets-2025bangalore",
    "title": "Gallery",
    "section": "",
    "text": "Attending COMSNETS 2025\n\n\n\n\n\nPresenting “Towards Sleep Apnea Screening via Thermal Imagery”\n\n\n\n\n\nInside IIIT Delhi Campus"
  },
  {
    "objectID": "gallery.html#acm-compass-2024-iiit-delhi",
    "href": "gallery.html#acm-compass-2024-iiit-delhi",
    "title": "Gallery",
    "section": "",
    "text": "Attending ACM Compass with the Lab\n\n\n\n\n\nACM Compass - infront of conference hall\n\n\n\n\n\nInside IIIT Delhi Campus"
  },
  {
    "objectID": "gallery.html#mtech-convocation-2024",
    "href": "gallery.html#mtech-convocation-2024",
    "title": "Gallery",
    "section": "",
    "text": "Awarded the MTech in Computer Science and Engineering at IITGN convocation June 2024\n\n\n\n\n\nOutside IITGN Convocation Hall June 2024\n\n\n\n\n\nOutside IITGN Convocation Hall with my family"
  },
  {
    "objectID": "gallery.html#mtech-thesis-defense-2024",
    "href": "gallery.html#mtech-thesis-defense-2024",
    "title": "Gallery",
    "section": "",
    "text": "Suraj and I celebrating the successful defense of our MTech thesis\n\n\n\n\n\nCelebrating the successful defense of our MTech thesis"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Canny Edge Detector\n\n\n\nComputer Vision\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nDec 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPower of Sine Activation\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nNov 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLucas Kanade Optical Flow\n\n\n\nComputer Vision\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nOct 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Conda Environment for ML Projects\n\n\n\nHelpful Tips\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJun 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Processes\n\n\n\nDeep Learning\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publication",
    "section": "",
    "text": "Under review will be added soon."
  },
  {
    "objectID": "posts/2024/SirenDemo.html",
    "href": "posts/2024/SirenDemo.html",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "This notebook is a simple demonstration of the power of the sine activation function in neural networks. We will use a nueral network to learn a function mapping that goes from pixel locations to pixel values.\nIn nut shell we wish to map\n\\[f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\]\n\\[f(x_1, x_2) = R, G, B\\]\n\\[\\text{where } R, G, B \\in [0, 1] \\text{are the pixel intensities for red, green and blue channels respectively}\\] \\[\\text{and } x_1, x_2 \\in [0, 1] \\text{are the pixel locations}\\]\nWe will first use a ReLU activation function and then use a sine activation function to see the difference in the results. This demonstration is a simpler usecase of the paper SIREN: Implicit Neural Representations with Periodic Activation Functions by Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein. Available to read at https://arxiv.org/abs/2006.09661. Here is the blog post by the authors https://vsitzmann.github.io/siren/\nNOTE : This notebook does not implement the SIREN architecture. It is a simple demonstration of the power of sine activation function in neural networks.\n\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport numpy as np\nimport torch.nn as nn\nimport torch\nimport warnings\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Device in use \", device)\n\nDevice in use  cuda\n\n\n\n\nSay Hello to my CaT!\n\n# Read the Image\nimage = plt.imread('Chhotu.jpeg')\n\n# Create a copy of the image\nimage_rec = image.copy()\n\n# Draw a rectangle on the image (w1,h1) and (w2,h2)\nimage_rec = cv.rectangle(image_rec, (800, 300), (1400, 900), (255, 255, 0), 5)\nimage_rec = cv.putText(image_rec, 'Chhotu', (810, 1000), cv.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 0), 5)\n\n# Display the image\nprint(image_rec.shape)\nplt.figure(figsize=(8, 8))\nplt.imshow(image_rec)\nplt.tight_layout()\nplt.show()\n\n(1200, 1600, 3)\n\n\n\n\n\n\n\n\n\nThis Image is too big for my GPU to handle. So I will resize it to a smaller size of 300 \\(\\times\\) 300 pixels.\n\n# Crop the image\ncrop_image = image[300:900, 800:1400]\n\n# Resize the image to make it smaller\ncrop_image = cv.resize(crop_image, (300, 300))\n\n# Normalize the image to be in the range [0, 1]\ncrop_image = crop_image/255.0\n\nprint(crop_image.shape)\nplt.imshow(crop_image)\nplt.tight_layout()\nplt.show()\n\n(300, 300, 3)\n\n\n\n\n\n\n\n\n\n\n\n\nWe basically wish to create a dataset to train our neural network. The input to the neural network will be the pixel location and the output will be the pixel value. We will create a coordinate map of the image. The coordinate map will have the shape (\\(Height \\times Width\\), 2) and Output will have the shape (\\(Height \\times Width\\), 3).\n\\[\\text{Input Coordinate Map to Neural Network} =  \\begin{bmatrix} x_1, y_1 \\\\ x_2, y_2 \\\\ \\vdots \\\\ x_n, y_n \\end{bmatrix} \\text{,where } x_i, y_i \\text{ are the pixel locations}\\]\n\\[\\text{Output Pixel Values} =  \\begin{bmatrix} R_1, G_1, B_1 \\\\ R_2, G_2, B_2 \\\\ \\vdots \\\\ R_n, G_n, B_n \\end{bmatrix} \\text{,where } R_i, G_i, B_i \\text{ are the pixel intensities}\\]\n\n# Create a 2d grid of the image\nheight, width, channels = crop_image.shape\n\nx_coords = range(height)\ny_coords = range(width)\n\nxv, yv = np.meshgrid(y_coords, x_coords)\nxv=xv.flatten()\nyv=yv.flatten()\n\nX = np.vstack((yv, xv)).T\nY = crop_image.reshape((height*width, 3))\n\n\nX.shape, Y.shape\n\n((90000, 2), (90000, 3))\n\n\nLet have a look at what exactly the coordinate map looks like.\n\nX\n\narray([[  0,   0],\n       [  0,   1],\n       [  0,   2],\n       ...,\n       [299, 297],\n       [299, 298],\n       [299, 299]])\n\n\n\n\n\nWe will train a simple neural network with ReLU activation function to learn the mapping from pixel locations to pixel values. We will use the Adam optimizer and Mean Squared Error loss function.\n\\[ Neural Network(x_i, y_i) = \\begin{bmatrix} R_i, G_i, B_i \\end{bmatrix}\\]\nAll the layers in the neural network will have ReLU activation function except the last layer which will have a linear activation function.\n\n# Create a MLP with 5 hidden layers.\n# Input is (x, y) and output is (r, g, b)\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 512)\n        self.fc5 = nn.Linear(512, 256)\n        self.fc6 = nn.Linear(256, 3)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.fc1(x))\n        x = nn.functional.relu(self.fc2(x))\n        x = nn.functional.relu(self.fc3(x))\n        x = nn.functional.relu(self.fc4(x))\n        x = nn.functional.relu(self.fc5(x))\n        return self.fc6(x)\n\n\n\n\n# Training loop function to train the model\n# X: (num_xy, 2) tensor of (x, y) coordinates\n# y: (num_xy, 3) tensor of (r, g, b) pixel values\n# model: MLP model\n# lr: learning rate\n# epochs: number of epochs to train for\n# bs: batch size\n# print_every: print loss every print_every epochs\n# Logs losses\n# Saves the prediction frmo model every print_every epochs\n\ndef train(X, y, model, lr=0.01, epochs=1000, bs=1000, print_every=100):\n    \"\"\"\n    Model Trainer : Trains the model\n\n    Parameters:\n    X : (num_xy, 2) tensor of (x, y) coordinates\n    y : (num_xy, 3) tensor of (r, g, b) pixel values\n    model : MLP model \n    lr : learning rate\n    epochs : number of epochs to train for\n    bs : batch size\n    print_every : print loss every print_every epochs\n\n    Returns:\n    losses : list of losses\n    imgs : list of images predicted by the model every print_every epochs\n    \"\"\"\n    losses = []\n    imgs = []\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n\n        # chosing some random indexes to train the model\n        idx = np.random.choice(X.shape[0], bs)\n        X_batch = torch.tensor(X[idx], dtype=torch.float32).to(device)\n        y_batch = torch.tensor(y[idx], dtype=torch.float32).to(device)\n        y_pred = model(X_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        if epoch % print_every == 0:\n            print(f'Epoch {epoch}, Loss: {loss.item()}')\n            imgs.append(model(torch.tensor(X, dtype=torch.float32).to(device)).detach().cpu().numpy())\n\n    return losses, imgs\n\n\ndef plot_image(model, name=None):\n    \"\"\"\n    Plot the image predicted by the model\n    \"\"\"\n    # Predict the (r, g, b) values\n    pred_y = model(torch.tensor(X, dtype=torch.float32).to(device))\n\n    # Reshape the predictions to be (3, height, width)\n    pred_y = pred_y.transpose(0, 1).reshape(channels, height, width)\n\n    # plot the image\n    plt.imshow(pred_y.permute(1, 2, 0).detach().cpu())\n    if name:\n        plt.savefig(name)\n\n\nMLP_model = MLP()\nMLP_model.to(device)\n\nrelu_losses, relu_imgs = train(X,Y, \n                     MLP_model, lr=0.001, epochs=10000, bs=5000, print_every=1000)\n\nEpoch 0, Loss: 0.6576557755470276\nEpoch 1000, Loss: 0.0114674037322402\nEpoch 2000, Loss: 0.012357354164123535\nEpoch 3000, Loss: 0.008751491084694862\nEpoch 4000, Loss: 0.008101350627839565\nEpoch 5000, Loss: 0.007827701978385448\nEpoch 6000, Loss: 0.010159682482481003\nEpoch 7000, Loss: 0.008207915350794792\nEpoch 8000, Loss: 0.007177131250500679\nEpoch 9000, Loss: 0.006437341216951609\n\n\n\nplt.figure(figsize=(20, 8))\n\nfor i, img in enumerate(relu_imgs):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img.reshape(height, width, 3))\n    plt.axis('off')\n    plt.title(f'After Epoch {i*1000}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(crop_image)\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplot_image(MLP_model)\nplt.title('Reconstructed Image after training')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe will train a simple neural network with ReLU activation function to learn the mapping from pixel locations to pixel values. We will use the Adam optimizer and Mean Squared Error loss function.\n\\[ Neural Network(x_i, y_i) = \\begin{bmatrix} R_i, G_i, B_i \\end{bmatrix}\\]\nAll the layers in the neural network will have Sine activation function except the last layer which will have a linear activation function.\n\n# Create a MLP with 5 hidden layers with 256 neurons each and sine activations.\n# Input is (x, y) and output is (r, g, b)\n\nclass MLP_sin(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 512)\n        self.fc5 = nn.Linear(512, 256)\n        self.fc6 = nn.Linear(256, 3)\n\n    def forward(self, x):\n        x = torch.sin(self.fc1(x))\n        x = torch.sin(self.fc2(x))\n        x = torch.sin(self.fc3(x))\n        x = torch.sin(self.fc4(x))\n        x = torch.sin(self.fc5(x))\n        return self.fc6(x)\n\n\nsin_model = MLP_sin()\nsin_model.to(device)\n\nsin_losses, sin_imgs = train(X,Y,\n                     sin_model, lr=0.001, epochs=10000, bs=5000, print_every=1000)\n\nEpoch 0, Loss: 0.11556914448738098\nEpoch 1000, Loss: 0.0018598262686282396\nEpoch 2000, Loss: 0.001261448604054749\nEpoch 3000, Loss: 0.0007661026320420206\nEpoch 4000, Loss: 0.00033592403633520007\nEpoch 5000, Loss: 0.00014680986350867897\nEpoch 6000, Loss: 9.029130887938663e-05\nEpoch 7000, Loss: 8.067893941188231e-05\nEpoch 8000, Loss: 8.20108616608195e-05\nEpoch 9000, Loss: 7.174971688073128e-05\n\n\n\nplt.figure(figsize=(20, 9))\n\nfor i, img in enumerate(sin_imgs):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img.reshape(height, width, 3))\n    plt.axis('off')\n    plt.title(f'After Epoch {i*1000}', fontsize=14  )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(15, 6))\nplt.subplot(1, 3, 1)\nplt.imshow(crop_image)\nplt.title('Original Image', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplot_image(sin_model)\nplt.title('Reconstructed Image from Sine Activation', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplot_image(MLP_model)\nplt.title('Reconstructed Image from ReLU Activation', fontsize=14)\nplt.axis('off')\n\nplt.suptitle('Comparison of Image reconstruction from ReLU and Sine Activation', fontsize=18)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6),dpi=150)\nplt.plot(relu_losses, label='ReLU')\nplt.plot(sin_losses, label='Sine')\nplt.axhline(np.min(relu_losses), color='C0', linestyle='--', label='ReLU Min Loss')\nplt.axhline(np.min(sin_losses), color='C1', linestyle='--', label='Sine Min Loss')\nplt.annotate(f'Relu Min Loss: {np.min(relu_losses):.5f}', (len(relu_losses)-1, np.min(relu_losses)), textcoords=\"offset points\", xytext=(-25,-10), ha='center')\nplt.annotate(f'Sine Min Loss: {np.min(sin_losses):.5f}', (len(sin_losses)-1, np.min(sin_losses)), textcoords=\"offset points\", xytext=(-25,-10), ha='center')\nplt.xlabel('Epochs')\nplt.ylabel('MSE Loss or Reconstruction Error')\nplt.yscale('log')  # Set y-axis to log scale\nplt.legend()\nplt.title('Comparison of Losses between ReLU and Sine Activation (Log Scale)')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#lets-load-an-image",
    "href": "posts/2024/SirenDemo.html#lets-load-an-image",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "Say Hello to my CaT!\n\n# Read the Image\nimage = plt.imread('Chhotu.jpeg')\n\n# Create a copy of the image\nimage_rec = image.copy()\n\n# Draw a rectangle on the image (w1,h1) and (w2,h2)\nimage_rec = cv.rectangle(image_rec, (800, 300), (1400, 900), (255, 255, 0), 5)\nimage_rec = cv.putText(image_rec, 'Chhotu', (810, 1000), cv.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 0), 5)\n\n# Display the image\nprint(image_rec.shape)\nplt.figure(figsize=(8, 8))\nplt.imshow(image_rec)\nplt.tight_layout()\nplt.show()\n\n(1200, 1600, 3)\n\n\n\n\n\n\n\n\n\nThis Image is too big for my GPU to handle. So I will resize it to a smaller size of 300 \\(\\times\\) 300 pixels.\n\n# Crop the image\ncrop_image = image[300:900, 800:1400]\n\n# Resize the image to make it smaller\ncrop_image = cv.resize(crop_image, (300, 300))\n\n# Normalize the image to be in the range [0, 1]\ncrop_image = crop_image/255.0\n\nprint(crop_image.shape)\nplt.imshow(crop_image)\nplt.tight_layout()\nplt.show()\n\n(300, 300, 3)"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#creating-a-coordinate-map",
    "href": "posts/2024/SirenDemo.html#creating-a-coordinate-map",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "We basically wish to create a dataset to train our neural network. The input to the neural network will be the pixel location and the output will be the pixel value. We will create a coordinate map of the image. The coordinate map will have the shape (\\(Height \\times Width\\), 2) and Output will have the shape (\\(Height \\times Width\\), 3).\n\\[\\text{Input Coordinate Map to Neural Network} =  \\begin{bmatrix} x_1, y_1 \\\\ x_2, y_2 \\\\ \\vdots \\\\ x_n, y_n \\end{bmatrix} \\text{,where } x_i, y_i \\text{ are the pixel locations}\\]\n\\[\\text{Output Pixel Values} =  \\begin{bmatrix} R_1, G_1, B_1 \\\\ R_2, G_2, B_2 \\\\ \\vdots \\\\ R_n, G_n, B_n \\end{bmatrix} \\text{,where } R_i, G_i, B_i \\text{ are the pixel intensities}\\]\n\n# Create a 2d grid of the image\nheight, width, channels = crop_image.shape\n\nx_coords = range(height)\ny_coords = range(width)\n\nxv, yv = np.meshgrid(y_coords, x_coords)\nxv=xv.flatten()\nyv=yv.flatten()\n\nX = np.vstack((yv, xv)).T\nY = crop_image.reshape((height*width, 3))\n\n\nX.shape, Y.shape\n\n((90000, 2), (90000, 3))\n\n\nLet have a look at what exactly the coordinate map looks like.\n\nX\n\narray([[  0,   0],\n       [  0,   1],\n       [  0,   2],\n       ...,\n       [299, 297],\n       [299, 298],\n       [299, 299]])"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#training-a-simple-neural-network-with-relu-activation.",
    "href": "posts/2024/SirenDemo.html#training-a-simple-neural-network-with-relu-activation.",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "We will train a simple neural network with ReLU activation function to learn the mapping from pixel locations to pixel values. We will use the Adam optimizer and Mean Squared Error loss function.\n\\[ Neural Network(x_i, y_i) = \\begin{bmatrix} R_i, G_i, B_i \\end{bmatrix}\\]\nAll the layers in the neural network will have ReLU activation function except the last layer which will have a linear activation function.\n\n# Create a MLP with 5 hidden layers.\n# Input is (x, y) and output is (r, g, b)\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 512)\n        self.fc5 = nn.Linear(512, 256)\n        self.fc6 = nn.Linear(256, 3)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.fc1(x))\n        x = nn.functional.relu(self.fc2(x))\n        x = nn.functional.relu(self.fc3(x))\n        x = nn.functional.relu(self.fc4(x))\n        x = nn.functional.relu(self.fc5(x))\n        return self.fc6(x)\n\n\n\n\n# Training loop function to train the model\n# X: (num_xy, 2) tensor of (x, y) coordinates\n# y: (num_xy, 3) tensor of (r, g, b) pixel values\n# model: MLP model\n# lr: learning rate\n# epochs: number of epochs to train for\n# bs: batch size\n# print_every: print loss every print_every epochs\n# Logs losses\n# Saves the prediction frmo model every print_every epochs\n\ndef train(X, y, model, lr=0.01, epochs=1000, bs=1000, print_every=100):\n    \"\"\"\n    Model Trainer : Trains the model\n\n    Parameters:\n    X : (num_xy, 2) tensor of (x, y) coordinates\n    y : (num_xy, 3) tensor of (r, g, b) pixel values\n    model : MLP model \n    lr : learning rate\n    epochs : number of epochs to train for\n    bs : batch size\n    print_every : print loss every print_every epochs\n\n    Returns:\n    losses : list of losses\n    imgs : list of images predicted by the model every print_every epochs\n    \"\"\"\n    losses = []\n    imgs = []\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n\n        # chosing some random indexes to train the model\n        idx = np.random.choice(X.shape[0], bs)\n        X_batch = torch.tensor(X[idx], dtype=torch.float32).to(device)\n        y_batch = torch.tensor(y[idx], dtype=torch.float32).to(device)\n        y_pred = model(X_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        if epoch % print_every == 0:\n            print(f'Epoch {epoch}, Loss: {loss.item()}')\n            imgs.append(model(torch.tensor(X, dtype=torch.float32).to(device)).detach().cpu().numpy())\n\n    return losses, imgs\n\n\ndef plot_image(model, name=None):\n    \"\"\"\n    Plot the image predicted by the model\n    \"\"\"\n    # Predict the (r, g, b) values\n    pred_y = model(torch.tensor(X, dtype=torch.float32).to(device))\n\n    # Reshape the predictions to be (3, height, width)\n    pred_y = pred_y.transpose(0, 1).reshape(channels, height, width)\n\n    # plot the image\n    plt.imshow(pred_y.permute(1, 2, 0).detach().cpu())\n    if name:\n        plt.savefig(name)\n\n\nMLP_model = MLP()\nMLP_model.to(device)\n\nrelu_losses, relu_imgs = train(X,Y, \n                     MLP_model, lr=0.001, epochs=10000, bs=5000, print_every=1000)\n\nEpoch 0, Loss: 0.6576557755470276\nEpoch 1000, Loss: 0.0114674037322402\nEpoch 2000, Loss: 0.012357354164123535\nEpoch 3000, Loss: 0.008751491084694862\nEpoch 4000, Loss: 0.008101350627839565\nEpoch 5000, Loss: 0.007827701978385448\nEpoch 6000, Loss: 0.010159682482481003\nEpoch 7000, Loss: 0.008207915350794792\nEpoch 8000, Loss: 0.007177131250500679\nEpoch 9000, Loss: 0.006437341216951609\n\n\n\nplt.figure(figsize=(20, 8))\n\nfor i, img in enumerate(relu_imgs):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img.reshape(height, width, 3))\n    plt.axis('off')\n    plt.title(f'After Epoch {i*1000}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(crop_image)\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplot_image(MLP_model)\nplt.title('Reconstructed Image after training')\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#training-a-simple-neural-network-with-sine-activation.",
    "href": "posts/2024/SirenDemo.html#training-a-simple-neural-network-with-sine-activation.",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "We will train a simple neural network with ReLU activation function to learn the mapping from pixel locations to pixel values. We will use the Adam optimizer and Mean Squared Error loss function.\n\\[ Neural Network(x_i, y_i) = \\begin{bmatrix} R_i, G_i, B_i \\end{bmatrix}\\]\nAll the layers in the neural network will have Sine activation function except the last layer which will have a linear activation function.\n\n# Create a MLP with 5 hidden layers with 256 neurons each and sine activations.\n# Input is (x, y) and output is (r, g, b)\n\nclass MLP_sin(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 512)\n        self.fc5 = nn.Linear(512, 256)\n        self.fc6 = nn.Linear(256, 3)\n\n    def forward(self, x):\n        x = torch.sin(self.fc1(x))\n        x = torch.sin(self.fc2(x))\n        x = torch.sin(self.fc3(x))\n        x = torch.sin(self.fc4(x))\n        x = torch.sin(self.fc5(x))\n        return self.fc6(x)\n\n\nsin_model = MLP_sin()\nsin_model.to(device)\n\nsin_losses, sin_imgs = train(X,Y,\n                     sin_model, lr=0.001, epochs=10000, bs=5000, print_every=1000)\n\nEpoch 0, Loss: 0.11556914448738098\nEpoch 1000, Loss: 0.0018598262686282396\nEpoch 2000, Loss: 0.001261448604054749\nEpoch 3000, Loss: 0.0007661026320420206\nEpoch 4000, Loss: 0.00033592403633520007\nEpoch 5000, Loss: 0.00014680986350867897\nEpoch 6000, Loss: 9.029130887938663e-05\nEpoch 7000, Loss: 8.067893941188231e-05\nEpoch 8000, Loss: 8.20108616608195e-05\nEpoch 9000, Loss: 7.174971688073128e-05\n\n\n\nplt.figure(figsize=(20, 9))\n\nfor i, img in enumerate(sin_imgs):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img.reshape(height, width, 3))\n    plt.axis('off')\n    plt.title(f'After Epoch {i*1000}', fontsize=14  )\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#side-by-side-comparison-of-relu-and-sine-activation",
    "href": "posts/2024/SirenDemo.html#side-by-side-comparison-of-relu-and-sine-activation",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "plt.figure(figsize=(15, 6))\nplt.subplot(1, 3, 1)\nplt.imshow(crop_image)\nplt.title('Original Image', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplot_image(sin_model)\nplt.title('Reconstructed Image from Sine Activation', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplot_image(MLP_model)\nplt.title('Reconstructed Image from ReLU Activation', fontsize=14)\nplt.axis('off')\n\nplt.suptitle('Comparison of Image reconstruction from ReLU and Sine Activation', fontsize=18)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6),dpi=150)\nplt.plot(relu_losses, label='ReLU')\nplt.plot(sin_losses, label='Sine')\nplt.axhline(np.min(relu_losses), color='C0', linestyle='--', label='ReLU Min Loss')\nplt.axhline(np.min(sin_losses), color='C1', linestyle='--', label='Sine Min Loss')\nplt.annotate(f'Relu Min Loss: {np.min(relu_losses):.5f}', (len(relu_losses)-1, np.min(relu_losses)), textcoords=\"offset points\", xytext=(-25,-10), ha='center')\nplt.annotate(f'Sine Min Loss: {np.min(sin_losses):.5f}', (len(sin_losses)-1, np.min(sin_losses)), textcoords=\"offset points\", xytext=(-25,-10), ha='center')\nplt.xlabel('Epochs')\nplt.ylabel('MSE Loss or Reconstruction Error')\nplt.yscale('log')  # Set y-axis to log scale\nplt.legend()\nplt.title('Comparison of Losses between ReLU and Sine Activation (Log Scale)')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/Canny_Edge.html",
    "href": "posts/2024/Canny_Edge.html",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The Canny Edge detector is named after its inventor, John F. Canny. It is among the most popular edge detection algorithms due to its simplicity and effectiveness. The algorithm uses gradient information to detect edges. It follow the following steps:\n\n\n\n\nThe algorithm uses a Gaussian filter to smooth the image and reduce noise. This is done to prevent the algorithm from detecting false edges due to noise. It also smoothens the image to reduce the abrupt changes in pixel values. These abrupt changes may end up being detected as edges.\n\\[I'(x,y) = G(x,y) * I(x,y)\\]\nwhere \\(I(x,y)\\) is the input image, \\(G(x,y)\\) is the Gaussian filter and \\(I'(x,y)\\) is the smoothed image. ’*’ represents convolution operation between the image and the filter. Reading the image and convert it to grayscale. Image is converted to grayscale because the edge detection is done on the intensity values of the image. The color information is not required for edge detection.\n\n# Importing libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Read the image\nimage = cv.imread('Flower.jpeg')\n\nplt.figure(figsize=(12, 4))\n# Convert the image to RGB (OpenCV loads it into BGR by default)\nplt.subplot(1, 3, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\n# Convert the image to grayscale\ngray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nplt.subplot(1, 3, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.axis('off')\nplt.title('Grayscaled Image')\n\n# Apply Gaussian Blur\nblurred_image = cv.GaussianBlur(gray_image, (3, 3), 0)\nplt.subplot(1, 3, 3)\nplt.imshow(blurred_image, cmap='gray')\nplt.axis('off')\nplt.title('Gaussian Blurred Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then calculates the gradient of the image. The gradient is calculated using the Sobel operator. The Sobel operator is a discrete differentiation operator. It computes an approximation of gradient of the image. The operator uses two 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives - one for horizontal changes, and one for vertical. \\(G_x\\) and \\(G_y\\) are the horizontal and vertical Sobel operators respectively.\n\\[G_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\] \\[G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\]\n\\[I_y = I' * G_y\\] \\[I_x = I' * G_x\\]\nwhere \\(I_x\\) and \\(I_y\\) are the horizontal and vertical gradients respectively.\nNow the combined gradient magnitude could be calculated using two methods. * Using L1 Norm:\n\\[Magnitude = |I_x| + |I_y|\\]\n\nUsing L2 Norm:\n\n\\[Magnitude =  \\sqrt{I_x^2 + I_y^2}\\]\n\\[Direction = \\arctan(\\frac{I_y}{I_x})\\]\nMagnitude signifies the strength of the edge. The direction is the angle of the edge. The direction is rounded to one of four angles representing vertical, horizontal and two diagonals. The angles are 0, 45, 90 and 135 degrees. We are assuming the edges are in one of these four directions.\n\n# Compute the gradients by applying Sobel operator\nI_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=3)\nI_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=3)\n\n# Compute the magnitude of the gradients\nmagnitude = np.sqrt(I_x**2 + I_y**2)\n\n# Compute the magnitude of the gradients using the L1 norm\n# magnitude = np.abs(I_x) + np.abs(I_y)\n\ndirection = np.arctan2(I_y, I_x)\n\nplt.figure(figsize=(9, 7))\n\nplt.subplot(2, 2, 1)\nplt.imshow(cv.convertScaleAbs(I_x), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in X direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 2)\nplt.imshow(cv.convertScaleAbs(I_y), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in Y direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 3)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Combined Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 4)\nplt.imshow(direction)\nplt.axis('off')\nplt.title('Gradient Direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Direction')\ncbar.set_ticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\ncbar.set_ticklabels(['-180°', '-90°', '0°', '90°', '180°'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then performs non-maximum suppression. The idea is to thin out the edges. We want to keep only the local maxima in the gradient direction. The algorithm goes through all the points on the gradient magnitude image and keeps only the points which are local maxima in the direction of the gradient. The pixel are checked in the direction of the gradient. If the pixel is not the maximum, it is set to zero.\nLet \\(M(x, y)\\) be defined as:\n\\[\n\\\nM(x, y) =\n\\begin{cases}\n0, & \\text{if } M(x, y) &lt; M(x + \\Delta x, y + \\Delta y) \\text{ or } M(x, y) &lt; M(x - \\Delta x, y - \\Delta y), \\\\\nM(x, y), & \\text{otherwise.}\n\\end{cases}\n\\\n\\]\nwhere \\(M(x,y)\\) is the gradient magnitude image and \\(\\Delta x\\) and \\(\\Delta y\\) are the unit vectors in the direction of the gradient.\n\n\n\n\nEdge Direction\n\n\nThe image on the left shows the gradient direction. The image on the right shows the process of non-maximum suppression. The point B (marked in Red) is the local maxima in the direction of the gradient while the points A and C (marked in black) are not. The points A and C are set to zero and the point B is kept. This operation will result in thinning of the edges.\n\nnms_image = np.zeros_like(magnitude)\n\n# Define the window size\nfor i in range(1, magnitude.shape[0] - 1):\n    for j in range(1, magnitude.shape[1] - 1):\n        # Define the angle interval\n        angle = direction[i, j] if direction[i, j] &gt;= 0 else direction[i, j] + np.pi\n        angle = np.rad2deg(angle)\n        angle = angle % 180\n\n        # Perform Non-Maximum Suppression\n        if (0 &lt;= angle &lt; 22.5) or (157.5 &lt;= angle &lt;= 180):\n            prev = magnitude[i, j - 1]\n            nxt = magnitude[i, j + 1]\n        elif 22.5 &lt;= angle &lt; 67.5:\n            prev = magnitude[i + 1, j - 1]\n            nxt = magnitude[i - 1, j + 1]\n        elif 67.5 &lt;= angle &lt; 112.5:\n            prev = magnitude[i - 1, j]\n            nxt = magnitude[i + 1, j]\n        else:\n            prev = magnitude[i - 1, j - 1]\n            nxt = magnitude[i + 1, j + 1]\n\n        if magnitude[i, j] &gt;= prev and magnitude[i, j] &gt;= nxt:\n            nms_image[i, j] = magnitude[i, j]\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(1, 2, 2)\nplt.imshow(nms_image, cmap='gray')\nplt.axis('off')\nplt.title('Non-Maximum Suppression')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNext step is to apply hysteresis thresholding or also known as double thresholding. The algorithm uses two thresholds, a high threshold and a low threshold. All the points above the high threshold are considered to be strong edges and all the points below the low threshold are considered to be non-edges. The points between the two thresholds are considered to be weak edges. The algorithm then tracks the weak edges and checks if they are connected to strong edges. If they are connected to strong edges, they are considered to be edges. If they are not connected to strong edges, they are considered to be non-edges. This is done to remove the weak edges which are not connected to any of the strong edges.\nLet:\n\n\\(S\\) denote the set of strong edges\n\\(W\\) denote the set of weak edge\n\\(E(x, y)\\) represent whether a pixel at \\((x, y)\\) is classified as an edge \\(1\\) or not \\(0\\).\n\nThe classification can be expressed as:\n\\[\nE(x, y) =\n\\begin{cases}\n1, & \\text{if } (x, y) \\in S, \\text{ or } (x, y) \\in W \\text{ and connected to } S, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nConnected to \\(S\\) implies there exists a path from \\((x, y)\\) in \\(W\\) to some \\((x', y')\\) in \\(S\\) that satisfies the connectivity criteria (e.g., adjacency).\n\nEdges = np.zeros_like(nms_image)\n\nthreshold_high = 100\nthreshold_low = 50\nvicinity = 2\n\n# Define the weak and strong edges\nstrong_edges = nms_image &gt; threshold_high\nweak_edges = (nms_image &gt;= threshold_low) & (nms_image &lt;= threshold_high)\n\n\n# Define the edge map\nEdges[strong_edges] = 255\n\n# Define the weak edges that are connected to strong edges\nfor i in range(1, nms_image.shape[0] - 1):\n    for j in range(1, nms_image.shape[1] - 1):\n        if weak_edges[i, j]:\n            if np.sum(strong_edges[i-vicinity:i+vicinity+1, j-vicinity:j+vicinity+1]) &gt; 0:\n                Edges[i, j] = 255\n\n# Dilate the edges\nEdges = cv.dilate(Edges, np.ones((3, 3), np.uint8), iterations=1)\n\n\n# Display the Original Image and the Detected Edges\nplt.figure(figsize=(7, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(Edges, cmap='gray')\nplt.axis('off')\nplt.title('Detected Edges')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Final output is the edge image where the edges are detected. The edges are detected using the gradient information. The algorithm is very effective in detecting edges and is widely used in computer vision applications."
  },
  {
    "objectID": "posts/2024/Canny_Edge.html#algorithm",
    "href": "posts/2024/Canny_Edge.html#algorithm",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The algorithm uses a Gaussian filter to smooth the image and reduce noise. This is done to prevent the algorithm from detecting false edges due to noise. It also smoothens the image to reduce the abrupt changes in pixel values. These abrupt changes may end up being detected as edges.\n\\[I'(x,y) = G(x,y) * I(x,y)\\]\nwhere \\(I(x,y)\\) is the input image, \\(G(x,y)\\) is the Gaussian filter and \\(I'(x,y)\\) is the smoothed image. ’*’ represents convolution operation between the image and the filter. Reading the image and convert it to grayscale. Image is converted to grayscale because the edge detection is done on the intensity values of the image. The color information is not required for edge detection.\n\n# Importing libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Read the image\nimage = cv.imread('Flower.jpeg')\n\nplt.figure(figsize=(12, 4))\n# Convert the image to RGB (OpenCV loads it into BGR by default)\nplt.subplot(1, 3, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\n# Convert the image to grayscale\ngray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nplt.subplot(1, 3, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.axis('off')\nplt.title('Grayscaled Image')\n\n# Apply Gaussian Blur\nblurred_image = cv.GaussianBlur(gray_image, (3, 3), 0)\nplt.subplot(1, 3, 3)\nplt.imshow(blurred_image, cmap='gray')\nplt.axis('off')\nplt.title('Gaussian Blurred Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then calculates the gradient of the image. The gradient is calculated using the Sobel operator. The Sobel operator is a discrete differentiation operator. It computes an approximation of gradient of the image. The operator uses two 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives - one for horizontal changes, and one for vertical. \\(G_x\\) and \\(G_y\\) are the horizontal and vertical Sobel operators respectively.\n\\[G_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\] \\[G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\]\n\\[I_y = I' * G_y\\] \\[I_x = I' * G_x\\]\nwhere \\(I_x\\) and \\(I_y\\) are the horizontal and vertical gradients respectively.\nNow the combined gradient magnitude could be calculated using two methods. * Using L1 Norm:\n\\[Magnitude = |I_x| + |I_y|\\]\n\nUsing L2 Norm:\n\n\\[Magnitude =  \\sqrt{I_x^2 + I_y^2}\\]\n\\[Direction = \\arctan(\\frac{I_y}{I_x})\\]\nMagnitude signifies the strength of the edge. The direction is the angle of the edge. The direction is rounded to one of four angles representing vertical, horizontal and two diagonals. The angles are 0, 45, 90 and 135 degrees. We are assuming the edges are in one of these four directions.\n\n# Compute the gradients by applying Sobel operator\nI_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=3)\nI_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=3)\n\n# Compute the magnitude of the gradients\nmagnitude = np.sqrt(I_x**2 + I_y**2)\n\n# Compute the magnitude of the gradients using the L1 norm\n# magnitude = np.abs(I_x) + np.abs(I_y)\n\ndirection = np.arctan2(I_y, I_x)\n\nplt.figure(figsize=(9, 7))\n\nplt.subplot(2, 2, 1)\nplt.imshow(cv.convertScaleAbs(I_x), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in X direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 2)\nplt.imshow(cv.convertScaleAbs(I_y), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in Y direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 3)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Combined Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 4)\nplt.imshow(direction)\nplt.axis('off')\nplt.title('Gradient Direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Direction')\ncbar.set_ticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\ncbar.set_ticklabels(['-180°', '-90°', '0°', '90°', '180°'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then performs non-maximum suppression. The idea is to thin out the edges. We want to keep only the local maxima in the gradient direction. The algorithm goes through all the points on the gradient magnitude image and keeps only the points which are local maxima in the direction of the gradient. The pixel are checked in the direction of the gradient. If the pixel is not the maximum, it is set to zero.\nLet \\(M(x, y)\\) be defined as:\n\\[\n\\\nM(x, y) =\n\\begin{cases}\n0, & \\text{if } M(x, y) &lt; M(x + \\Delta x, y + \\Delta y) \\text{ or } M(x, y) &lt; M(x - \\Delta x, y - \\Delta y), \\\\\nM(x, y), & \\text{otherwise.}\n\\end{cases}\n\\\n\\]\nwhere \\(M(x,y)\\) is the gradient magnitude image and \\(\\Delta x\\) and \\(\\Delta y\\) are the unit vectors in the direction of the gradient.\n\n\n\n\nEdge Direction\n\n\nThe image on the left shows the gradient direction. The image on the right shows the process of non-maximum suppression. The point B (marked in Red) is the local maxima in the direction of the gradient while the points A and C (marked in black) are not. The points A and C are set to zero and the point B is kept. This operation will result in thinning of the edges.\n\nnms_image = np.zeros_like(magnitude)\n\n# Define the window size\nfor i in range(1, magnitude.shape[0] - 1):\n    for j in range(1, magnitude.shape[1] - 1):\n        # Define the angle interval\n        angle = direction[i, j] if direction[i, j] &gt;= 0 else direction[i, j] + np.pi\n        angle = np.rad2deg(angle)\n        angle = angle % 180\n\n        # Perform Non-Maximum Suppression\n        if (0 &lt;= angle &lt; 22.5) or (157.5 &lt;= angle &lt;= 180):\n            prev = magnitude[i, j - 1]\n            nxt = magnitude[i, j + 1]\n        elif 22.5 &lt;= angle &lt; 67.5:\n            prev = magnitude[i + 1, j - 1]\n            nxt = magnitude[i - 1, j + 1]\n        elif 67.5 &lt;= angle &lt; 112.5:\n            prev = magnitude[i - 1, j]\n            nxt = magnitude[i + 1, j]\n        else:\n            prev = magnitude[i - 1, j - 1]\n            nxt = magnitude[i + 1, j + 1]\n\n        if magnitude[i, j] &gt;= prev and magnitude[i, j] &gt;= nxt:\n            nms_image[i, j] = magnitude[i, j]\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(1, 2, 2)\nplt.imshow(nms_image, cmap='gray')\nplt.axis('off')\nplt.title('Non-Maximum Suppression')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNext step is to apply hysteresis thresholding or also known as double thresholding. The algorithm uses two thresholds, a high threshold and a low threshold. All the points above the high threshold are considered to be strong edges and all the points below the low threshold are considered to be non-edges. The points between the two thresholds are considered to be weak edges. The algorithm then tracks the weak edges and checks if they are connected to strong edges. If they are connected to strong edges, they are considered to be edges. If they are not connected to strong edges, they are considered to be non-edges. This is done to remove the weak edges which are not connected to any of the strong edges.\nLet:\n\n\\(S\\) denote the set of strong edges\n\\(W\\) denote the set of weak edge\n\\(E(x, y)\\) represent whether a pixel at \\((x, y)\\) is classified as an edge \\(1\\) or not \\(0\\).\n\nThe classification can be expressed as:\n\\[\nE(x, y) =\n\\begin{cases}\n1, & \\text{if } (x, y) \\in S, \\text{ or } (x, y) \\in W \\text{ and connected to } S, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nConnected to \\(S\\) implies there exists a path from \\((x, y)\\) in \\(W\\) to some \\((x', y')\\) in \\(S\\) that satisfies the connectivity criteria (e.g., adjacency).\n\nEdges = np.zeros_like(nms_image)\n\nthreshold_high = 100\nthreshold_low = 50\nvicinity = 2\n\n# Define the weak and strong edges\nstrong_edges = nms_image &gt; threshold_high\nweak_edges = (nms_image &gt;= threshold_low) & (nms_image &lt;= threshold_high)\n\n\n# Define the edge map\nEdges[strong_edges] = 255\n\n# Define the weak edges that are connected to strong edges\nfor i in range(1, nms_image.shape[0] - 1):\n    for j in range(1, nms_image.shape[1] - 1):\n        if weak_edges[i, j]:\n            if np.sum(strong_edges[i-vicinity:i+vicinity+1, j-vicinity:j+vicinity+1]) &gt; 0:\n                Edges[i, j] = 255\n\n# Dilate the edges\nEdges = cv.dilate(Edges, np.ones((3, 3), np.uint8), iterations=1)\n\n\n# Display the Original Image and the Detected Edges\nplt.figure(figsize=(7, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(Edges, cmap='gray')\nplt.axis('off')\nplt.title('Detected Edges')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/Canny_Edge.html#final-output",
    "href": "posts/2024/Canny_Edge.html#final-output",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The Final output is the edge image where the edges are detected. The edges are detected using the gradient information. The algorithm is very effective in detecting edges and is widely used in computer vision applications."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "2024\n\n\nSpring and Fall : Machine Learning,\n\n\nProfessor Incharge: Nipun Batra  Organised and evaluated quizzes, assignments, took vivas for a classroom of 300 students, while also playing a pivotal role in supporting classroom logistics.\n\n\n2023\n\n\nFall : Computer Systems\n\n\nProfessor Incharge: Abhishek Bichhawat, Sameer G Kulkarni  Graded assignments and assisted the professor run a class of 40 students smoothly.\n\n\nSummer : World of Engineering\n\n\nProfessor Incharge: Udit Bhatia  Mentored a group of 30 students in the identification, conceptualization, and modeling of a prototype to address a real-world problem, fostering their problem-solving abilities and teamwork skills.\n\n\nSpring : Probability, Statistics and Data Visualization\n\n\nProfessor Incharge: Shanmuganathan Raman  Successfully guided over 30 students in Python libraries, including Numpy, Pandas, Matplotlib, Scipy, and Scikit-learn, enhancing their data visualization skills.\n\n\n2022\n\n\nWinter : Introduction to Computing\n\n\nProfessor Incharge: Nipun Batra, Balagopal Komarath  Led a Python lab for 30 students, teaching them essential Python concepts, and helped manage logistics, invigilation, and quiz evaluation for a class of 300 students."
  },
  {
    "objectID": "posts/2024/Neural_Process.html",
    "href": "posts/2024/Neural_Process.html",
    "title": "Neural Processes",
    "section": "",
    "text": "This notebook demonstrates the implementation of the Neural Processes. The model is trained to predict the distribution of the target function given a few examples of the function. In this notebook we will attempt to reconstruct an entire image given a few pixels of the image.\n\\[Model(\\{x_c, y_c\\}_{c=1}^n, \\{x_t\\}_{t=1}^m) \\rightarrow \\{y_t\\}_{t=1}^m\\]\n\\(\\{x_c, y_c\\}_{c=1}^n\\) are the context points where \\(x_i\\) are the input features and \\(y_i\\) are the labels.\n\\(\\{x_t\\}_{t=1}^m\\) are the target features where we want to predict the label values \\(y_j\\).\n\n\n\nNeural Processes Block Diagram\n\n\n\nThe model is composed of two neural networks: the encoder and the decoder. The encoder takes the context set as the input and encodes them into a fixed-size representation. The decoder takes the representation and the target features and predicts the pixel labels of the given target features."
  },
  {
    "objectID": "posts/2024/Neural_Process.html#generating-tasks-from-the-mnist-dataset",
    "href": "posts/2024/Neural_Process.html#generating-tasks-from-the-mnist-dataset",
    "title": "Neural Processes",
    "section": "Generating Tasks from the MNIST Dataset",
    "text": "Generating Tasks from the MNIST Dataset\n\nDownloading the MNIST Dataset\nWe will be using the MNIST dataset for this task. The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9). We will download the dataset using the torchvision library. We will convert the images to tensors and no further preprocessing is being performed on the images.\n\n# Downlaoding the MNIST dataset\ntrain_data = MNIST(root='./data', train=True, download=True, transform=ToTensor())\ntest_data = MNIST(root='./data', train=False, download=True, transform= ToTensor())\n\n\n\nCreating Context and Target sets\nWe will sample a random number of pixel coordinates \\((x^1, x^2)\\) and the pixel intensity values \\(I\\) for these coordinates. These coordinates and pixel intensity values will be used as context points. The Test set will contain all the pixel coordinates and Intensities of the image. The model will predict the pixel intensity values for all the pixel coordinates in the image.\n\ndef get_Context_Target_Sets(images_data, no_context_points,Image_shape):\n\n    m,n = Image_shape\n\n    # All possible coordinates in the image. Size of MNIST image is 28x28\n    All_corrdinates = np.array([(i,j) for i in range(m) for j in range(n)])\n\n    # Iterate over the dataset and create the context set and target set\n    task_set = []\n    for i in tqdm(range(len(images_data))):\n\n        # Images in the dataset are of shape (1, 28, 28). We need to remove the channel dimension to get the shape (28, 28)\n        image, _ = images_data[i]\n        image = image.squeeze().numpy()     # un-squeeze the image to remove the channel dimension\n\n        # Sample random context points indices\n        context_idx = np.random.choice(range(len(All_corrdinates)), no_context_points, replace=False)\n\n        # Get the context points and their corresponding pixel values from the generated context indices\n        context_points = All_corrdinates[context_idx]\n        context_pixels = image[context_points[:,0], context_points[:,1]]\n\n        # Concatenate the context points and their corresponding pixel values\n        context_set = np.concatenate([context_points, context_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n        # For training the model, we need to predict the pixel values for all the pixels in the image\n        target_points = All_corrdinates\n        target_pixels = image[target_points[:,0], target_points[:,1]]\n\n        # Concatenate the target points and their corresponding pixel values\n        target_set = np.concatenate([target_points, target_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n        # Append the context set and target set to the train_set\n        task_set.append((context_set, target_set))\n\n    return task_set\n\n\ntrain_set = get_Context_Target_Sets(images_data=train_data, no_context_points=200, Image_shape=(28,28))\ntrain_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in train_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\n100%|██████████| 60000/60000 [00:04&lt;00:00, 12578.65it/s]\n\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])\n\n\n\ntest_set = get_Context_Target_Sets(images_data=test_data, no_context_points=200, Image_shape=(28,28))\ntest_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in test_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\n100%|██████████| 10000/10000 [00:00&lt;00:00, 11855.53it/s]\n\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])"
  },
  {
    "objectID": "posts/2024/Neural_Process.html#intializing-the-neural-process-class",
    "href": "posts/2024/Neural_Process.html#intializing-the-neural-process-class",
    "title": "Neural Processes",
    "section": "Intializing the Neural Process Class",
    "text": "Intializing the Neural Process Class\n\nEncoder and Decoder Networks\nOur Model class will contain the Encoder and Decoder networks. The Encoder network will take the context points and encode them into a fixed-size representation. The Decoder network will take the representation and the pixel coordinates and predict the pixel intensity value of the given coordinate.\n\n\nTraining Method\nMethod to train the model. The model is trained to predict the pixel intensity values of the target pixel coordinates given the context points. The loss function is the Mean Squared Error loss between the predicted pixel intensity values and the actual pixel intensity values.\n\n\nTesting Method\nMethod to test the model. The model is tested on the test set. The model is provided with the context points and the model predicts the pixel intensity values of the target pixel coordinates. The Mean Squared Error loss is calculated between the predicted pixel intensity values and the actual pixel intensity values.\n\nclass Neural_Procss_Model(nn.Module):\n    \"\"\"\n    Neural Process Model has two parts: encoder and decoder\n\n    Encoder takes in the context pairs (X1_c, X2_c, PixelIntensity) and encode them into a latent representation\n\n    Decoder takes in the latent representation and the target pairs (X1_t, X2_t) and output the predicted target pairs\n    \"\"\"\n    def __init__(self,device):\n        super(Neural_Procss_Model, self).__init__()\n\n        if device == 'cuda':\n            if torch.cuda.is_available():\n                device = torch.device('cuda')\n        \n        if device == 'mps':\n            device = torch.device('mps')\n\n        self.device = device\n\n        # Encoder takes in the context pairs (X1c,X2c,PixelIntensity) and encode them into a latent representation\n        self.encoder = nn.Sequential(\n            nn.Linear(3,64),\n            nn.ReLU(),\n            nn.Linear(64,128),\n            nn.ReLU(),\n            nn.Linear(128,256),\n        )\n\n        # Decoder takes in the latent representation and the target pairs and output the predicted target pairs\n        self.decoder = nn.Sequential(\n            nn.Linear(256+2,128),\n            nn.ReLU(),\n            nn.Linear(128,64),\n            nn.ReLU(),\n            nn.Linear(64,32),\n            nn.ReLU(),\n            nn.Linear(32,1)\n        )\n\n    def forward(self, context_pairs, target_pairs):\n        \"\"\"\n        Forward pass of the Neural Process Model. \n        \n        It takes in the context cordinate pairs (X1_c, X2_c, PixelIntensity) and target cordinate pairs (X1_t, X2_t, PixelIntensity) and output the predicted target pixel intensity for the target cordinate pairs\n\n        Parameters:\n        context_pairs: torch.Tensor of shape (batch_size, num_context_pairs, 3)\n        target_pairs: torch.Tensor of shape (batch_size, num_target_pairs, 2)\n\n        Returns:\n        predicted_target_Pixel Intensity: torch.Tensor of shape (batch_size, num_target_pairs, 1)\n        \"\"\"\n        \n        # Encode the context pairs into a latent representation\n        latent_representation = self.encoder(context_pairs)\n\n        # Average the latent representation\n        latent_representation = torch.mean(latent_representation,dim=1)\n\n        # Repeat the latent representation for each target pair\n        latent_representation = latent_representation.unsqueeze(1).repeat(1,target_pairs.size(1),1)\n\n        # Concatenate the latent representation with the target pixel locations\n        target_pixel_locations = target_pairs[:,:,:2]\n        target = torch.cat([latent_representation,target_pixel_locations],dim=-1)\n\n        # Decode the target pairs to obtain the predicted target pixel intensity\n        predicted_target_pixel_intensity = self.decoder(target)\n\n        return predicted_target_pixel_intensity\n    \n\n    def train(self, train_dataloader, num_epochs=100, optim = torch.optim.Adam, lr=3e-4, criterion = nn.MSELoss(),verbose=True):\n        \"\"\"\n        Train the Neural Process Model\n\n        Parameters:\n        train_dataloader: DataLoader object for the training data\n        num_epochs: int, number of epochs to train the model\n        optimer: str, optimer to use for training the model\n        lr: float, learning rate for the optimer\n        criterion: loss function to use for training the model\n        \"\"\"\n\n        device = self.device\n\n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the optimizer\n        optimizer = optim(self.parameters(), lr=lr)\n\n        for epoch in range(num_epochs):\n            for i, (context_pairs, target_pairs) in enumerate(train_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Zero the gradients\n                optimizer.zero_grad()\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                # Backward pass\n                loss.backward()\n\n                # Update the weights\n                optimizer.step()\n\n            if verbose:\n                print(\"Epoch: {}/{} Loss: {:.5f}\".format(epoch+1,num_epochs,loss.item()))\n\n\n\n    def test(self, test_dataloader, criterion = nn.MSELoss()):\n        \"\"\"\n        Test the Neural Process Model\n\n        Parameters:\n        test_dataloader: DataLoader object for the test data\n        \"\"\"\n\n        device = self.device\n    \n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the loss\n        test_loss = 0\n\n        with torch.no_grad():\n            for i, (context_pairs, target_pairs) in enumerate(test_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                test_loss += loss.item()\n\n        print(f'Test Loss: {test_loss/len(test_dataloader)}')\n\n        \n\n\n\nTraining the Neural Process model\n\n# Set the device. If cuda is available, use cuda. \nif torch.cuda.is_available():\n    device = torch.device('cuda')\n\n# If mps is available, use mps. \nelif torch.mps.is_available():\n    device = torch.device('mps')\n\n# Else use cpu\nelse:\n    device = torch.device('cpu')\n\nprint(\"Device in use: \", device)\n\nDevice in use:  mps\n\n\n\nmodel = Neural_Procss_Model(device=device)\n\n# model.to('cuda')\n\nmodel.train(train_dataloader, num_epochs=30)\n\nEpoch: 1/30 Loss: 0.06018\nEpoch: 2/30 Loss: 0.06170\nEpoch: 3/30 Loss: 0.05950\nEpoch: 4/30 Loss: 0.06060\nEpoch: 5/30 Loss: 0.05958\nEpoch: 6/30 Loss: 0.06148\nEpoch: 7/30 Loss: 0.05642\nEpoch: 8/30 Loss: 0.05425\nEpoch: 9/30 Loss: 0.05090\nEpoch: 10/30 Loss: 0.04823\nEpoch: 11/30 Loss: 0.04689\nEpoch: 12/30 Loss: 0.04165\nEpoch: 13/30 Loss: 0.04482\nEpoch: 14/30 Loss: 0.04253\nEpoch: 15/30 Loss: 0.03567\nEpoch: 16/30 Loss: 0.04409\nEpoch: 17/30 Loss: 0.03556\nEpoch: 18/30 Loss: 0.03798\nEpoch: 19/30 Loss: 0.03793\nEpoch: 20/30 Loss: 0.03933\nEpoch: 21/30 Loss: 0.03644\nEpoch: 22/30 Loss: 0.03336\nEpoch: 23/30 Loss: 0.03567\nEpoch: 24/30 Loss: 0.03594\nEpoch: 25/30 Loss: 0.03620\nEpoch: 26/30 Loss: 0.03798\nEpoch: 27/30 Loss: 0.03487\nEpoch: 28/30 Loss: 0.03350\nEpoch: 29/30 Loss: 0.03534\nEpoch: 30/30 Loss: 0.03392\n\n\n\nmodel.test(test_dataloader)\n\nTest Loss: 0.03589645992762174"
  },
  {
    "objectID": "posts/2024/Lucas_Kanade.html",
    "href": "posts/2024/Lucas_Kanade.html",
    "title": "Lucas Kanade Optical Flow",
    "section": "",
    "text": "Optical Flow\nOptical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movement of object or camera. It is vector field where each vector denotes the movement of points from first frame to second. Generally, optical flow is used to track the motion of objects in a video. Lets assume you have captured two frames with a small time difference \\(\\Delta t\\). You wish to track the motion of a pixel located at \\((x, y)\\) in first frame. In the second frame, the pixel has moved to \\((x + \\Delta x, y + \\Delta y)\\). The vector \\((\\Delta x, \\Delta y)\\) is the optical flow vector.\n\n\n\n\n\nOptical Flow : Brightness Constancy Constraint\n\n\n\\[I(x, y, t) = I(x + \\Delta x, y + \\Delta y, t + \\Delta t)\\]\nThe above equation is the basic assumption in optical flow. It assumes that intensity of an object does not change between two consecutive frames. This constraint is called brightness constancy constraint. This equation can be expanded using taylor series to get the optical flow equation.\n\\[I(x + \\Delta x, y + \\Delta y, t + \\Delta t) \\approx I(x, y, t) + \\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t = I(x, y, t)\\]\n\\[\\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t = 0\\]\n\\[\\frac{\\partial I}{\\partial x} u + \\frac{\\partial I}{\\partial y} v + \\frac{\\partial I}{\\partial t} = 0\\]\nwhere \\(u = \\frac{\\Delta x}{\\Delta t}\\) and \\(v = \\frac{\\Delta y}{\\Delta t}\\) are the optical flow velocities in x and y directions respectively.\nThe above equation is called optical flow equation. It is a single equation with two unknowns \\(u\\) and \\(v\\). Hence, it is an ill-posed problem. To solve this problem, we need to make some more assumptions. One of the assumption to arrive here was brightness constancy assumption. It assumes that the intensity of an object does not change between two consecutive frames. The other assumption made was given by lucas and kanade. They assumed that the optical flow is same for all the pixels in a neighborhood. This assumption is called spatial coherence assumption.\n\n\nLucas Kanade Optical Flow\nThe Lucas-Kanade method is a widely used differential method for optical flow estimation developed by Bruce D. Lucas and Takeo Kanade. It assumes that the flow is essentially constant in a local neighbourhood of the pixel under consideration, and solves the basic optical flow equations for all the pixels in that neighbourhood by the least squares criterion.\n\n\n\nLucas Kanade : spatial coherence assumption\n\n\n\\[\\frac{\\partial I}{\\partial x} u + \\frac{\\partial I}{\\partial y} v + \\frac{\\partial I}{\\partial t} = 0\\]\n\\[E_x = \\frac{\\partial I}{\\partial x}, E_y = \\frac{\\partial I}{\\partial y}, E_t = \\frac{\\partial I}{\\partial t}\\]\n\\[E_x u + E_y v + E_t = 0\\]\n\\[\\begin{bmatrix} E_x & E_y \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = -E_t\\]\n\\[ \\begin{bmatrix} E_x(i-1,j-1) & E_y(i-1,j-1) \\\\ E_x(i-1,j) & E_y(i-1,j) \\\\ E_x(i-1,j+1) & E_y(i-1,j+1) \\\\ E_x(i,j-1) & E_y(i,j-1) \\\\ E_x(i,j) & E_y(i,j) \\\\ E_x(i,j+1) & E_y(i,j+1) \\\\ E_x(i+1,j-1) & E_y(i+1,j-1) \\\\ E_x(i+1,j) & E_y(i+1,j) \\\\ E_x(i+1,j+1) & E_y(i+1,j+1) \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} -E_t(i-1,j-1) \\\\ -E_t(i-1,j) \\\\ -E_t(i-1,j+1) \\\\ -E_t(i,j-1) \\\\ -E_t(i,j) \\\\ -E_t(i,j+1) \\\\ -E_t(i+1,j-1) \\\\ -E_t(i+1,j) \\\\ -E_t(i+1,j+1) \\end{bmatrix}\\]\n\\[A_{(9,2)}  x_{(2,1)} = b_{(9,1)}\\]\n\\[A \\overrightarrow{x} = \\overrightarrow{b}\\]\n\\[x = (A^T A)^{-1} A^T b\\]\nSo for a chosen patch we can estimate the optical flow using above equation. This could also be written as\n\\[\\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} \\sum E_x E_x & \\sum E_x E_y \\\\ \\sum E_y E_x & \\sum E_y E_y \\end{bmatrix}^{-1} \\begin{bmatrix} -\\sum E_x E_t \\\\ -\\sum E_y E_t \\end{bmatrix}\\]\nThis is the final equation used to estimate the optical flow using Lucas Kanade method. The above equation is solved for each patch in the image to get the optical flow vectors. Optical flow algorithm could be summarized as follows:\n\nCompute image gradients \\(E_x\\) and \\(E_y\\).\nCompute temporal gradient \\(E_t\\).\nCompute the matrix \\(A\\) and vector \\(b\\) for each patch.\nSolve the equation \\(A \\overrightarrow{x} = \\overrightarrow{b}\\) to get the optical flow vectors.\n\n\n\nLucas Kanade Implementation\nUNDER CONSTRUCTION !!\n\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.signal import convolve2d\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport imageio\nimport cv2 \n\n\ndef compute_derivatives(frame1, frame2):\n    \"\"\"\n    Compute spatial and temporal derivatives between two consecutive frames for optical flow estimation.\n    \n    This function calculates the following derivatives:\n    - Ix: Spatial derivative in x direction (horizontal)\n    - Iy: Spatial derivative in y direction (vertical)\n    - It: Temporal derivative between frames\n    \n    The spatial derivatives are computed using Sobel operators on the average of both frames.\n    The temporal derivative is computed as the difference between frames.\n    \n    Args:\n        frame1 (numpy.ndarray): First frame (earlier time point)\n        frame2 (numpy.ndarray): Second frame (later time point)\n        \n    Returns:\n        tuple: A tuple containing three numpy.ndarray:\n            - Ix: Spatial derivative in x direction\n            - Iy: Spatial derivative in y direction\n            - It: Temporal derivative\n    \n    Note:\n        Both input frames should have the same dimensions and be in grayscale format.\n    \"\"\"\n    # Define Sobel operator for x direction (horizontal)\n    kernel_x = np.array([[1, 0, -1], \n                        [2, 0, -2], \n                        [1, 0, -1]]) /8.0\n    \n    # Create y direction kernel by transposing x direction kernel\n    kernel_y = kernel_x.T\n    \n    # Calculate average frame to reduce noise in derivative estimation\n    calc_frame = (frame1 + frame2)//2\n    \n    # Compute spatial derivatives using convolution with Sobel operators\n    # 'same' mode maintains original dimensions, 'symm' handles boundaries by reflection\n    Ix = convolve2d(calc_frame, kernel_x, mode='same', boundary='symm')\n    Iy = convolve2d(calc_frame, kernel_y, mode='same', boundary='symm')\n    \n    # Compute temporal derivative as difference between frames\n    # Convert to float to avoid integer overflow\n    It = frame2.astype(float) - frame1.astype(float)\n    \n    return Ix, Iy, It\n\n\ndef lk_flow(frame1, frame2, window_size=16, tau=1e-2):\n    \"\"\"\n    Compute optical flow using the Lucas-Kanade method with improved robustness features.\n    \n    This function implements the Lucas-Kanade algorithm for computing optical flow between\n    two consecutive frames. It includes several improvements for robustness:\n    - Gaussian smoothing for noise reduction\n    - Eigenvalue-based condition checking\n    - Flow magnitude thresholding\n    - Optional outlier removal (commented out in post-processing)\n    \n    Args:\n        frame1 (numpy.ndarray): First frame (earlier time point)\n        frame2 (numpy.ndarray): Second frame (later time point)\n        window_size (int, optional): Size of the local window for flow computation. \n                                   Should be odd. Defaults to 16.\n        tau (float, optional): Threshold for eigenvalue condition checking. \n                             Higher values mean stricter filtering. Defaults to 1e-2.\n    \n    Returns:\n        tuple: Two numpy.ndarray containing:\n            - u: Horizontal component of the optical flow\n            - v: Vertical component of the optical flow\n            \n    Note:\n        - Frames should be in grayscale format\n        - The function includes eigenvalue-based filtering to handle aperture problem\n        - Flow vectors with magnitude &gt; 50 are filtered out\n    \"\"\"\n    # Apply Gaussian smoothing to reduce noise\n    # Higher sigma (2) for more aggressive smoothing\n    frame1 = gaussian_filter(frame1, sigma=2)\n    frame2 = gaussian_filter(frame2, sigma=2)\n    \n    # Compute spatial and temporal derivatives\n    Ix, Iy, It = compute_derivatives(frame1, frame2)\n    \n    # Initialize flow fields with zeros\n    u = np.zeros_like(frame1, dtype=float)\n    v = np.zeros_like(frame1, dtype=float)\n    \n    # Pad images to handle border pixels\n    # Using edge padding to avoid border artifacts\n    pad = window_size // 2\n    Ix_pad = np.pad(Ix, ((pad, pad), (pad, pad)), mode='edge')\n    Iy_pad = np.pad(Iy, ((pad, pad), (pad, pad)), mode='edge')\n    It_pad = np.pad(It, ((pad, pad), (pad, pad)), mode='edge')\n    \n    # Iterate through each pixel in the frame\n    for y in range(pad, frame1.shape[0] + pad):\n        for x in range(pad, frame1.shape[1] + pad):\n            # Extract local windows for derivatives\n            # Flatten to create system of equations\n            Ix_win = Ix_pad[y-pad:y+pad+1, x-pad:x+pad+1].flatten()\n            Iy_win = Iy_pad[y-pad:y+pad+1, x-pad:x+pad+1].flatten()\n            It_win = It_pad[y-pad:y+pad+1, x-pad:x+pad+1].flatten()\n            \n            # Construct system of equations Av = b\n            # A: spatial gradients, b: negative temporal gradient\n            A = np.vstack((Ix_win, Iy_win)).T\n            b = -It_win\n            \n            # Compute ATA for solving least squares\n            ATA = np.dot(A.T, A)\n            \n            # Check eigenvalues for aperture problem\n            eigenvalues = np.linalg.eigvals(ATA)\n            \n            # Skip if eigenvalues indicate ill-conditioned system\n            # Two conditions: minimum eigenvalue threshold and condition number check\n            if np.min(eigenvalues) &lt; tau or np.max(eigenvalues) / (np.min(eigenvalues) + 1e-10) &gt; 100:\n                continue\n                \n            # Solve the system of equations\n            try:\n                flow = np.linalg.solve(ATA, np.dot(A.T, b))\n                \n                # Apply flow magnitude threshold to filter out large motions\n                if np.sqrt(flow[0]**2 + flow[1]**2) &lt; 50:  # Threshold can be adjusted\n                    u[y-pad, x-pad] = flow[0]\n                    v[y-pad, x-pad] = flow[1]\n                    \n            except np.linalg.LinAlgError:\n                continue\n\n    return u, v\n\n\nfile = 0\ngif_path = \"../../images/blogs/Lucas-Kanade/\"+str(file)+\".gif\"\ngif_frames = np.array(imageio.mimread(gif_path))        # Read GIF file\n\nwriter = cv2.VideoWriter(\"../../images/blogs/Lucas-Kanade/Flow\"+str(file)+\".AVI\",\n                         cv2.VideoWriter_fourcc(*'XVID'), 10, (2*gif_frames[0].shape[1], 2*gif_frames[0].shape[0]))\n\nfor i in range(1, len(gif_frames)):\n    # Convert frames to grayscale\n    frame1_gray = cv2.cvtColor(gif_frames[i-1], cv2.COLOR_RGB2GRAY)\n    frame2_gray = cv2.cvtColor(gif_frames[i], cv2.COLOR_RGB2GRAY)\n    \n    # Compute optical flow using Lucas-Kanade method.\n    u, v = lk_flow(frame1_gray, frame2_gray, window_size=16, tau=1e-2)\n    u,v = u*2, v*2\n    \n    # Display optical flow vectors on frame.\n    flow_img = gif_frames[i-1].copy()\n    flow_img = cv2.cvtColor(flow_img, cv2.COLOR_RGB2BGR)\n    step = 8\n\n    for y in range(0, flow_img.shape[0], step):\n        for x in range(0, flow_img.shape[1], step):\n            cv2.arrowedLine(flow_img, (x, y), (int(x+u[y, x]), int(y+v[y, x])), (0, 0, 255), 1)\n\n    flow_img = cv2.resize(flow_img, (2*flow_img.shape[1], 2*flow_img.shape[0]))\n    # Display frame with optical flow vectors.\n    cv2.imshow('Optical Flow', flow_img)\n\n    # Write frame to output video\n    writer.write(flow_img)\n    \n    # Press 'q' to quit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        cv2.destroyAllWindows()\n        cv2.waitKey(1)\n\ncv2.destroyAllWindows()\ncv2.waitKey(1)"
  },
  {
    "objectID": "posts/2024/CondaPost.html",
    "href": "posts/2024/CondaPost.html",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "",
    "text": "This blogpost is written for myself as I keep forgetting the steps to set up a conda environment for my ML projects."
  },
  {
    "objectID": "posts/2024/CondaPost.html#install-conda",
    "href": "posts/2024/CondaPost.html#install-conda",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Install Conda",
    "text": "Install Conda\nThese four commands quickly and quietly install the latest 64-bit version of the installer and then clean up after themselves. To install a different version or architecture of Miniconda for Linux, change the name of the .sh installer in the wget command.\nVisit the Miniconda website to find the latest version of Miniconda for Linux.\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\nAfter installing, initialize your newly-installed Miniconda. The following commands initialize for bash and zsh shells:\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\nyou can run conda -V to check if conda is installed correctly."
  },
  {
    "objectID": "posts/2024/CondaPost.html#creating-conda-environment",
    "href": "posts/2024/CondaPost.html#creating-conda-environment",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Creating Conda Environment",
    "text": "Creating Conda Environment\nThe below command will create the conda environment with the name EnvName and python version 3.9.\nconda create --name EnvName python=3.9 jupyter\nTo activate the environment, run the below command:\nconda activate EnvName"
  },
  {
    "objectID": "posts/2024/CondaPost.html#installing-pytorch",
    "href": "posts/2024/CondaPost.html#installing-pytorch",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Installing Pytorch",
    "text": "Installing Pytorch\nVisit the Pytorch website to find the latest version of Pytorch. You can choose your system configuration and get the command to install Pytorch.\n\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\ncheck if everything is installed correctly by running the below command:\npython -c \"import torch; print(torch.__version__)\"\nInstalling other libraries and packages."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ayush Shrivastava",
    "section": "",
    "text": "I am a first-year PhD student in Computer Science and Engineering at IIT Gandhinagar, working at the intersection of machine learning, sustainability, and health sensing. I am an active member of the Sustainability Lab, led by Prof. Nipun Batra, and the Smash Lab, led by Prof. Mayank Goel.\nI hold a master’s degree in Computer Science and Engineering from IIT Gandhinagar and a bachelor’s degree in Electronics and Telecommunications from Jabalpur Engineering College. My research focuses on applying machine learning to develop innovative solutions in sustainability and health sensing. I am passionate about solving challenging problems using machine learning to create meaningful impact."
  },
  {
    "objectID": "index.html#my-journey-so-far",
    "href": "index.html#my-journey-so-far",
    "title": "Ayush Shrivastava",
    "section": "My Journey So Far",
    "text": "My Journey So Far\n\n\n\n\n\n\n27-1 March 2025\n\n\nShowcased our work in iInventive 2025, IIT Madras\n\n\n\n\n6-11 Jan 2025\n\n\nPresented my work Towards Sleep Apnea Screening via Thermal Imagery” in Graduate Forum, COMSNETS 2025\n\n\n\n\n31 Aug 2024\n\n\nJoined Sustainability Lab and Smash Lab as a PhD student\n\n\n\n\n15 Jul 2024\n\n\nJoined IIT Gandhinagar as PhD student under the supervision of Prof Nipun Batra and Prof Mayank Goel\n\n\n\n\n8-11 Jul 2024\n\n\nAttended 7th ACM SIGCAS/SIGCHI Conference of Computing and Sustainable Societies ACM COMPASS 2024\n\n\n\n\n29 Jun 2024\n\n\nConvocation! Got Masters degree from IIT Gandhinagar\n\n\n\n\n\n\n2 Feb 2024\n\n\nSubmitted the first draft of our work to IMWUT ’24, marking my first research paper submission ever.\n\n\n\n\n\n\n01 Jan 2023\n\n\nJoined Sustainability Lab as a masters student at IIT Gandhinagar\n\n\n\n\n18 jul 2022\n\n\nJoined IIT Gandhinagar for M.Tech in Computer Science and Engineering\n\n\n\n\n15 Mar 2022\n\n\nAttained 98.16 percentile in Graduate Aptitude Test in Engineering 2022 (GATE’22)\n\n\n\n\n17 jun 2019\n\n\nJoined IBM India as an Application Developer, Enterprise Resource Planning (ERP)\n\n\n\n\n15 May 2019\n\n\nCompleted B.E in Electronics and Telecommunication from Jabalpur Engineering College, Jabalpur"
  },
  {
    "objectID": "gallery.html#comsnets-2025-bangalore",
    "href": "gallery.html#comsnets-2025-bangalore",
    "title": "Gallery",
    "section": "",
    "text": "Attending COMSNETS 2025\n\n\n\n\n\nPresenting “Towards Sleep Apnea Screening via Thermal Imagery”\n\n\n\n\n\nInside IIIT Delhi Campus"
  },
  {
    "objectID": "gallery.html#acm-summer-school-2025-iit-gandhinagar",
    "href": "gallery.html#acm-summer-school-2025-iit-gandhinagar",
    "title": "Gallery",
    "section": "",
    "text": "Taking a session on “Seeing Red”. Estimating Heart rate from Smartphone Camera"
  }
]