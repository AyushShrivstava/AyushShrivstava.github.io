[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ayush Shrivastava",
    "section": "",
    "text": "I am a first-year PhD student in Computer Science and Engineering at IIT Gandhinagar, working at the intersection of machine learning, sustainability, and health sensing. I am an active member of the Sustainability Lab, led by Prof. Nipun Batra, and the Smash Lab, led by Prof. Mayank Goel.\nI hold a master’s degree in Computer Science and Engineering from IIT Gandhinagar and a bachelor’s degree in Electronics and Telecommunications from Jabalpur Engineering College. My research focuses on applying machine learning to develop innovative solutions in sustainability and health sensing. I am passionate about solving challenging problems using machine learning to create meaningful impact."
  },
  {
    "objectID": "index.html#my-journey-so-far",
    "href": "index.html#my-journey-so-far",
    "title": "Ayush Shrivastava",
    "section": "My Journey So Far",
    "text": "My Journey So Far\n\n\n\n\n\n\n31 Aug 2024\n\n\nJoined Sustainability Lab and Smash Lab as a PhD student\n\n\n\n\n15 Jul 2024\n\n\nJoined IIT Gandhinagar as PhD student under the supervision of Prof Nipun Batra and Prof Mayank Goel\n\n\n\n\n8-11 Jul 2024\n\n\nAttended 7th ACM SIGCAS/SIGCHI Conference of Computing and Sustainable Societies ACM COMPASS 2024\n\n\n\n\n29 Jun 2024\n\n\nConvocation! Got Masters degree from IIT Gandhinagar\n\n\n\n\n14 Jun 2024\n\n\nGot brutally attacked but successfully defended my Masters Thesis\n\n\n\n\n\n2 Feb 2024\n\n\nSubmitted the first draft of our work to IMWUT ’24, marking my first research paper submission ever.\n\n\n\n\n10 Oct 2023\n\n\nGot Placed in Atlas Copco as Software Developer\n\n\n\n\n\n01 Jan 2023\n\n\nJoined Sustainability Lab as a masters student at IIT Gandhinagar\n\n\n\n\n18 jul 2022\n\n\nJoined IIT Gandhinagar for M.Tech in Computer Science and Engineering\n\n\n\n\n15 Mar 2022\n\n\nAttained 98.16 percentile in Graduate Aptitude Test in Engineering 2022 (GATE’22)\n\n\n\n\n17 jun 2019\n\n\nJoined IBM India as an Application Developer, Enterprise Resource Planning (ERP)\n\n\n\n\n15 May 2019\n\n\nCompleted B.E in Electronics and Telecommunication from Jabalpur Engineering College, Jabalpur"
  },
  {
    "objectID": "posts/2024/CondaPost.html",
    "href": "posts/2024/CondaPost.html",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "",
    "text": "This blogpost is written for myself as I keep forgetting the steps to set up a conda environment for my ML projects."
  },
  {
    "objectID": "posts/2024/CondaPost.html#install-conda",
    "href": "posts/2024/CondaPost.html#install-conda",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Install Conda",
    "text": "Install Conda\nThese four commands quickly and quietly install the latest 64-bit version of the installer and then clean up after themselves. To install a different version or architecture of Miniconda for Linux, change the name of the .sh installer in the wget command.\nVisit the Miniconda website to find the latest version of Miniconda for Linux.\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\nAfter installing, initialize your newly-installed Miniconda. The following commands initialize for bash and zsh shells:\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\nyou can run conda -V to check if conda is installed correctly."
  },
  {
    "objectID": "posts/2024/CondaPost.html#creating-conda-environment",
    "href": "posts/2024/CondaPost.html#creating-conda-environment",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Creating Conda Environment",
    "text": "Creating Conda Environment\nThe below command will create the conda environment with the name EnvName and python version 3.9.\nconda create --name EnvName python=3.9 jupyter\nTo activate the environment, run the below command:\nconda activate EnvName"
  },
  {
    "objectID": "posts/2024/CondaPost.html#installing-pytorch",
    "href": "posts/2024/CondaPost.html#installing-pytorch",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Installing Pytorch",
    "text": "Installing Pytorch\nVisit the Pytorch website to find the latest version of Pytorch. You can choose your system configuration and get the command to install Pytorch.\n\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\ncheck if everything is installed correctly by running the below command:\npython -c \"import torch; print(torch.__version__)\"\nInstalling other libraries and packages."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "2024\n\n\nSpring and Fall : Machine Learning,\n\n\nProfessor Incharge: Nipun Batra  Organised and evaluated quizzes, assignments, took vivas for a classroom of 300 students, while also playing a pivotal role in supporting classroom logistics.\n\n\n2023\n\n\nFall : Computer Systems\n\n\nProfessor Incharge: Abhishek Bichhawat, Sameer G Kulkarni  Graded assignments and assisted the professor run a class of 40 students smoothly.\n\n\nSummer : World of Engineering\n\n\nProfessor Incharge: Udit Bhatia  Mentored a group of 30 students in the identification, conceptualization, and modeling of a prototype to address a real-world problem, fostering their problem-solving abilities and teamwork skills.\n\n\nSpring : Probability, Statistics and Data Visualization\n\n\nProfessor Incharge: Shanmuganathan Raman  Successfully guided over 30 students in Python libraries, including Numpy, Pandas, Matplotlib, Scipy, and Scikit-learn, enhancing their data visualization skills.\n\n\n2022\n\n\nWinter : Introduction to Computing\n\n\nProfessor Incharge: Nipun Batra, Balagopal Komarath  Led a Python lab for 30 students, teaching them essential Python concepts, and helped manage logistics, invigilation, and quiz evaluation for a class of 300 students."
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Gallery",
    "section": "",
    "text": "Welcome to my gallery! Here you’ll find a collection of images from my life, including memorable moments and photos I’ve taken. I’ll continue to add more images as I capture new experiences and milestones along my journey.\n\n\n\n\n![compass24.jpg](./images/gallery/compass24.jpg)\n&lt;p&gt;Attending ACM Compass with the Lab&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/ACM_Compass1.jpg\" alt=\"ACM Compass inside Conference Hall\"&gt;\n&lt;p&gt;ACM Compass - infront of conference hall&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/IIITD1.jpg\" alt=\"Near IIITD gate\"&gt;\n&lt;p&gt;Inside IIIT Delhi Campus&lt;/p&gt;\n\n\n\n\n\n\n\n&lt;img src=\"images/gallery/convocation2.jpg\" alt=\"Awarded the MTech\"&gt;\n&lt;p&gt;Awarded the MTech in Computer Science and Engineering at IITGN convocation June 2024&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/convocation.jpg\" alt=\"Outside IITGN Convocation Hall 2024\"&gt;\n&lt;p&gt;Outside IITGN Convocation Hall June 2024&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/convocation3.jpg\" alt=\"Outside IITGN Convocation Hall 2024\"&gt;\n&lt;p&gt;Outside IITGN Convocation Hall with my family&lt;/p&gt;\n\n\n\n\n\n\n\n\n&lt;img src=\"images/gallery/MtechThesisDefense.jpg\" alt=\"2024 June Mtech defense\"&gt;\n&lt;p&gt;Suraj and I celebrating the successful defense of our MTech thesis&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/MtechThesisDefense3.jpg\" alt=\"2024 June Mtech defense\"&gt;\n&lt;p&gt;Celebrating the successful defense of our MTech thesis&lt;/p&gt;"
  },
  {
    "objectID": "gallery.html#acm-compass-2024-iiit_delhi",
    "href": "gallery.html#acm-compass-2024-iiit_delhi",
    "title": "Gallery",
    "section": "",
    "text": "![compass24.jpg](./images/gallery/compass24.jpg)\n&lt;p&gt;Attending ACM Compass with the Lab&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/ACM_Compass1.jpg\" alt=\"ACM Compass inside Conference Hall\"&gt;\n&lt;p&gt;ACM Compass - infront of conference hall&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/IIITD1.jpg\" alt=\"Near IIITD gate\"&gt;\n&lt;p&gt;Inside IIIT Delhi Campus&lt;/p&gt;"
  },
  {
    "objectID": "gallery.html#mtech-convocation-2024",
    "href": "gallery.html#mtech-convocation-2024",
    "title": "Gallery",
    "section": "",
    "text": "&lt;img src=\"images/gallery/convocation2.jpg\" alt=\"Awarded the MTech\"&gt;\n&lt;p&gt;Awarded the MTech in Computer Science and Engineering at IITGN convocation June 2024&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/convocation.jpg\" alt=\"Outside IITGN Convocation Hall 2024\"&gt;\n&lt;p&gt;Outside IITGN Convocation Hall June 2024&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/convocation3.jpg\" alt=\"Outside IITGN Convocation Hall 2024\"&gt;\n&lt;p&gt;Outside IITGN Convocation Hall with my family&lt;/p&gt;"
  },
  {
    "objectID": "gallery.html#mtech-thesis-defense-2024",
    "href": "gallery.html#mtech-thesis-defense-2024",
    "title": "Gallery",
    "section": "",
    "text": "&lt;img src=\"images/gallery/MtechThesisDefense.jpg\" alt=\"2024 June Mtech defense\"&gt;\n&lt;p&gt;Suraj and I celebrating the successful defense of our MTech thesis&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/MtechThesisDefense3.jpg\" alt=\"2024 June Mtech defense\"&gt;\n&lt;p&gt;Celebrating the successful defense of our MTech thesis&lt;/p&gt;"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Canny Edge Detector\n\n\n\nComputer Vision\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nDec 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLucas Kanade Optical Flow\n\n\n\nComputer Vision\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nOct 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Conda Environment for ML Projects\n\n\n\nHelpful Tips\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJun 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Processes\n\n\n\nDeep Learning\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publication",
    "section": "",
    "text": "Under review will be added soon."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/2024/Canny_Edge.html",
    "href": "posts/2024/Canny_Edge.html",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The Canny Edge detector is named after its inventor, John F. Canny. It is among the most popular edge detection algorithms due to its simplicity and effectiveness. The algorithm uses gradient information to detect edges. It follow the following steps:\n\n\n\n\nThe algorithm uses a Gaussian filter to smooth the image and reduce noise. This is done to prevent the algorithm from detecting false edges due to noise. It also smoothens the image to reduce the abrupt changes in pixel values. These abrupt changes may end up being detected as edges.\n\\[I'(x,y) = G(x,y) * I(x,y)\\]\nwhere \\(I(x,y)\\) is the input image, \\(G(x,y)\\) is the Gaussian filter and \\(I'(x,y)\\) is the smoothed image. ’*’ represents convolution operation between the image and the filter. Reading the image and convert it to grayscale. Image is converted to grayscale because the edge detection is done on the intensity values of the image. The color information is not required for edge detection.\n\n# Importing libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Read the image\nimage = cv.imread('Flower.jpeg')\n\nplt.figure(figsize=(12, 4))\n# Convert the image to RGB (OpenCV loads it into BGR by default)\nplt.subplot(1, 3, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\n# Convert the image to grayscale\ngray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nplt.subplot(1, 3, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.axis('off')\nplt.title('Grayscaled Image')\n\n# Apply Gaussian Blur\nblurred_image = cv.GaussianBlur(gray_image, (3, 3), 0)\nplt.subplot(1, 3, 3)\nplt.imshow(blurred_image, cmap='gray')\nplt.axis('off')\nplt.title('Gaussian Blurred Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then calculates the gradient of the image. The gradient is calculated using the Sobel operator. The Sobel operator is a discrete differentiation operator. It computes an approximation of gradient of the image. The operator uses two 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives - one for horizontal changes, and one for vertical. \\(G_x\\) and \\(G_y\\) are the horizontal and vertical Sobel operators respectively.\n\\[G_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\] \\[G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\]\n\\[I_y = I' * G_y\\] \\[I_x = I' * G_x\\]\nwhere \\(I_x\\) and \\(I_y\\) are the horizontal and vertical gradients respectively.\nNow the combined gradient magnitude could be calculated using two methods. * Using L1 Norm:\n\\[Magnitude = |I_x| + |I_y|\\]\n\nUsing L2 Norm:\n\n\\[Magnitude =  \\sqrt{I_x^2 + I_y^2}\\]\n\\[Direction = \\arctan(\\frac{I_y}{I_x})\\]\nMagnitude signifies the strength of the edge. The direction is the angle of the edge. The direction is rounded to one of four angles representing vertical, horizontal and two diagonals. The angles are 0, 45, 90 and 135 degrees. We are assuming the edges are in one of these four directions.\n\n# Compute the gradients by applying Sobel operator\nI_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=3)\nI_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=3)\n\n# Compute the magnitude of the gradients\nmagnitude = np.sqrt(I_x**2 + I_y**2)\n\n# Compute the magnitude of the gradients using the L1 norm\n# magnitude = np.abs(I_x) + np.abs(I_y)\n\ndirection = np.arctan2(I_y, I_x)\n\nplt.figure(figsize=(9, 7))\n\nplt.subplot(2, 2, 1)\nplt.imshow(cv.convertScaleAbs(I_x), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in X direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 2)\nplt.imshow(cv.convertScaleAbs(I_y), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in Y direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 3)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Combined Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 4)\nplt.imshow(direction)\nplt.axis('off')\nplt.title('Gradient Direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Direction')\ncbar.set_ticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\ncbar.set_ticklabels(['-180°', '-90°', '0°', '90°', '180°'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then performs non-maximum suppression. The idea is to thin out the edges. We want to keep only the local maxima in the gradient direction. The algorithm goes through all the points on the gradient magnitude image and keeps only the points which are local maxima in the direction of the gradient. The pixel are checked in the direction of the gradient. If the pixel is not the maximum, it is set to zero.\nLet \\(M(x, y)\\) be defined as:\n\\[\n\\\nM(x, y) =\n\\begin{cases}\n0, & \\text{if } M(x, y) &lt; M(x + \\Delta x, y + \\Delta y) \\text{ or } M(x, y) &lt; M(x - \\Delta x, y - \\Delta y), \\\\\nM(x, y), & \\text{otherwise.}\n\\end{cases}\n\\\n\\]\nwhere \\(M(x,y)\\) is the gradient magnitude image and \\(\\Delta x\\) and \\(\\Delta y\\) are the unit vectors in the direction of the gradient.\n\n\n\n\nEdge Direction\n\n\nThe image on the left shows the gradient direction. The image on the right shows the process of non-maximum suppression. The point B (marked in Red) is the local maxima in the direction of the gradient while the points A and C (marked in black) are not. The points A and C are set to zero and the point B is kept. This operation will result in thinning of the edges.\n\nnms_image = np.zeros_like(magnitude)\n\n# Define the window size\nfor i in range(1, magnitude.shape[0] - 1):\n    for j in range(1, magnitude.shape[1] - 1):\n        # Define the angle interval\n        angle = direction[i, j] if direction[i, j] &gt;= 0 else direction[i, j] + np.pi\n        angle = np.rad2deg(angle)\n        angle = angle % 180\n\n        # Perform Non-Maximum Suppression\n        if (0 &lt;= angle &lt; 22.5) or (157.5 &lt;= angle &lt;= 180):\n            prev = magnitude[i, j - 1]\n            nxt = magnitude[i, j + 1]\n        elif 22.5 &lt;= angle &lt; 67.5:\n            prev = magnitude[i + 1, j - 1]\n            nxt = magnitude[i - 1, j + 1]\n        elif 67.5 &lt;= angle &lt; 112.5:\n            prev = magnitude[i - 1, j]\n            nxt = magnitude[i + 1, j]\n        else:\n            prev = magnitude[i - 1, j - 1]\n            nxt = magnitude[i + 1, j + 1]\n\n        if magnitude[i, j] &gt;= prev and magnitude[i, j] &gt;= nxt:\n            nms_image[i, j] = magnitude[i, j]\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(1, 2, 2)\nplt.imshow(nms_image, cmap='gray')\nplt.axis('off')\nplt.title('Non-Maximum Suppression')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNext step is to apply hysteresis thresholding or also known as double thresholding. The algorithm uses two thresholds, a high threshold and a low threshold. All the points above the high threshold are considered to be strong edges and all the points below the low threshold are considered to be non-edges. The points between the two thresholds are considered to be weak edges. The algorithm then tracks the weak edges and checks if they are connected to strong edges. If they are connected to strong edges, they are considered to be edges. If they are not connected to strong edges, they are considered to be non-edges. This is done to remove the weak edges which are not connected to any of the strong edges.\nLet:\n\n\\(S\\) denote the set of strong edges\n\\(W\\) denote the set of weak edge\n\\(E(x, y)\\) represent whether a pixel at \\((x, y)\\) is classified as an edge \\(1\\) or not \\(0\\).\n\nThe classification can be expressed as:\n\\[\nE(x, y) =\n\\begin{cases}\n1, & \\text{if } (x, y) \\in S, \\text{ or } (x, y) \\in W \\text{ and connected to } S, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nConnected to \\(S\\) implies there exists a path from \\((x, y)\\) in \\(W\\) to some \\((x', y')\\) in \\(S\\) that satisfies the connectivity criteria (e.g., adjacency).\n\nEdges = np.zeros_like(nms_image)\n\nthreshold_high = 100\nthreshold_low = 50\nvicinity = 2\n\n# Define the weak and strong edges\nstrong_edges = nms_image &gt; threshold_high\nweak_edges = (nms_image &gt;= threshold_low) & (nms_image &lt;= threshold_high)\n\n\n# Define the edge map\nEdges[strong_edges] = 255\n\n# Define the weak edges that are connected to strong edges\nfor i in range(1, nms_image.shape[0] - 1):\n    for j in range(1, nms_image.shape[1] - 1):\n        if weak_edges[i, j]:\n            if np.sum(strong_edges[i-vicinity:i+vicinity+1, j-vicinity:j+vicinity+1]) &gt; 0:\n                Edges[i, j] = 255\n\n# Dilate the edges\nEdges = cv.dilate(Edges, np.ones((3, 3), np.uint8), iterations=1)\n\n\n# Display the Original Image and the Detected Edges\nplt.figure(figsize=(7, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(Edges, cmap='gray')\nplt.axis('off')\nplt.title('Detected Edges')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Final output is the edge image where the edges are detected. The edges are detected using the gradient information. The algorithm is very effective in detecting edges and is widely used in computer vision applications."
  },
  {
    "objectID": "posts/2024/Canny_Edge.html#algorithm",
    "href": "posts/2024/Canny_Edge.html#algorithm",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The algorithm uses a Gaussian filter to smooth the image and reduce noise. This is done to prevent the algorithm from detecting false edges due to noise. It also smoothens the image to reduce the abrupt changes in pixel values. These abrupt changes may end up being detected as edges.\n\\[I'(x,y) = G(x,y) * I(x,y)\\]\nwhere \\(I(x,y)\\) is the input image, \\(G(x,y)\\) is the Gaussian filter and \\(I'(x,y)\\) is the smoothed image. ’*’ represents convolution operation between the image and the filter. Reading the image and convert it to grayscale. Image is converted to grayscale because the edge detection is done on the intensity values of the image. The color information is not required for edge detection.\n\n# Importing libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Read the image\nimage = cv.imread('Flower.jpeg')\n\nplt.figure(figsize=(12, 4))\n# Convert the image to RGB (OpenCV loads it into BGR by default)\nplt.subplot(1, 3, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\n# Convert the image to grayscale\ngray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nplt.subplot(1, 3, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.axis('off')\nplt.title('Grayscaled Image')\n\n# Apply Gaussian Blur\nblurred_image = cv.GaussianBlur(gray_image, (3, 3), 0)\nplt.subplot(1, 3, 3)\nplt.imshow(blurred_image, cmap='gray')\nplt.axis('off')\nplt.title('Gaussian Blurred Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then calculates the gradient of the image. The gradient is calculated using the Sobel operator. The Sobel operator is a discrete differentiation operator. It computes an approximation of gradient of the image. The operator uses two 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives - one for horizontal changes, and one for vertical. \\(G_x\\) and \\(G_y\\) are the horizontal and vertical Sobel operators respectively.\n\\[G_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\] \\[G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\]\n\\[I_y = I' * G_y\\] \\[I_x = I' * G_x\\]\nwhere \\(I_x\\) and \\(I_y\\) are the horizontal and vertical gradients respectively.\nNow the combined gradient magnitude could be calculated using two methods. * Using L1 Norm:\n\\[Magnitude = |I_x| + |I_y|\\]\n\nUsing L2 Norm:\n\n\\[Magnitude =  \\sqrt{I_x^2 + I_y^2}\\]\n\\[Direction = \\arctan(\\frac{I_y}{I_x})\\]\nMagnitude signifies the strength of the edge. The direction is the angle of the edge. The direction is rounded to one of four angles representing vertical, horizontal and two diagonals. The angles are 0, 45, 90 and 135 degrees. We are assuming the edges are in one of these four directions.\n\n# Compute the gradients by applying Sobel operator\nI_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=3)\nI_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=3)\n\n# Compute the magnitude of the gradients\nmagnitude = np.sqrt(I_x**2 + I_y**2)\n\n# Compute the magnitude of the gradients using the L1 norm\n# magnitude = np.abs(I_x) + np.abs(I_y)\n\ndirection = np.arctan2(I_y, I_x)\n\nplt.figure(figsize=(9, 7))\n\nplt.subplot(2, 2, 1)\nplt.imshow(cv.convertScaleAbs(I_x), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in X direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 2)\nplt.imshow(cv.convertScaleAbs(I_y), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in Y direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 3)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Combined Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 4)\nplt.imshow(direction)\nplt.axis('off')\nplt.title('Gradient Direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Direction')\ncbar.set_ticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\ncbar.set_ticklabels(['-180°', '-90°', '0°', '90°', '180°'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then performs non-maximum suppression. The idea is to thin out the edges. We want to keep only the local maxima in the gradient direction. The algorithm goes through all the points on the gradient magnitude image and keeps only the points which are local maxima in the direction of the gradient. The pixel are checked in the direction of the gradient. If the pixel is not the maximum, it is set to zero.\nLet \\(M(x, y)\\) be defined as:\n\\[\n\\\nM(x, y) =\n\\begin{cases}\n0, & \\text{if } M(x, y) &lt; M(x + \\Delta x, y + \\Delta y) \\text{ or } M(x, y) &lt; M(x - \\Delta x, y - \\Delta y), \\\\\nM(x, y), & \\text{otherwise.}\n\\end{cases}\n\\\n\\]\nwhere \\(M(x,y)\\) is the gradient magnitude image and \\(\\Delta x\\) and \\(\\Delta y\\) are the unit vectors in the direction of the gradient.\n\n\n\n\nEdge Direction\n\n\nThe image on the left shows the gradient direction. The image on the right shows the process of non-maximum suppression. The point B (marked in Red) is the local maxima in the direction of the gradient while the points A and C (marked in black) are not. The points A and C are set to zero and the point B is kept. This operation will result in thinning of the edges.\n\nnms_image = np.zeros_like(magnitude)\n\n# Define the window size\nfor i in range(1, magnitude.shape[0] - 1):\n    for j in range(1, magnitude.shape[1] - 1):\n        # Define the angle interval\n        angle = direction[i, j] if direction[i, j] &gt;= 0 else direction[i, j] + np.pi\n        angle = np.rad2deg(angle)\n        angle = angle % 180\n\n        # Perform Non-Maximum Suppression\n        if (0 &lt;= angle &lt; 22.5) or (157.5 &lt;= angle &lt;= 180):\n            prev = magnitude[i, j - 1]\n            nxt = magnitude[i, j + 1]\n        elif 22.5 &lt;= angle &lt; 67.5:\n            prev = magnitude[i + 1, j - 1]\n            nxt = magnitude[i - 1, j + 1]\n        elif 67.5 &lt;= angle &lt; 112.5:\n            prev = magnitude[i - 1, j]\n            nxt = magnitude[i + 1, j]\n        else:\n            prev = magnitude[i - 1, j - 1]\n            nxt = magnitude[i + 1, j + 1]\n\n        if magnitude[i, j] &gt;= prev and magnitude[i, j] &gt;= nxt:\n            nms_image[i, j] = magnitude[i, j]\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(1, 2, 2)\nplt.imshow(nms_image, cmap='gray')\nplt.axis('off')\nplt.title('Non-Maximum Suppression')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNext step is to apply hysteresis thresholding or also known as double thresholding. The algorithm uses two thresholds, a high threshold and a low threshold. All the points above the high threshold are considered to be strong edges and all the points below the low threshold are considered to be non-edges. The points between the two thresholds are considered to be weak edges. The algorithm then tracks the weak edges and checks if they are connected to strong edges. If they are connected to strong edges, they are considered to be edges. If they are not connected to strong edges, they are considered to be non-edges. This is done to remove the weak edges which are not connected to any of the strong edges.\nLet:\n\n\\(S\\) denote the set of strong edges\n\\(W\\) denote the set of weak edge\n\\(E(x, y)\\) represent whether a pixel at \\((x, y)\\) is classified as an edge \\(1\\) or not \\(0\\).\n\nThe classification can be expressed as:\n\\[\nE(x, y) =\n\\begin{cases}\n1, & \\text{if } (x, y) \\in S, \\text{ or } (x, y) \\in W \\text{ and connected to } S, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nConnected to \\(S\\) implies there exists a path from \\((x, y)\\) in \\(W\\) to some \\((x', y')\\) in \\(S\\) that satisfies the connectivity criteria (e.g., adjacency).\n\nEdges = np.zeros_like(nms_image)\n\nthreshold_high = 100\nthreshold_low = 50\nvicinity = 2\n\n# Define the weak and strong edges\nstrong_edges = nms_image &gt; threshold_high\nweak_edges = (nms_image &gt;= threshold_low) & (nms_image &lt;= threshold_high)\n\n\n# Define the edge map\nEdges[strong_edges] = 255\n\n# Define the weak edges that are connected to strong edges\nfor i in range(1, nms_image.shape[0] - 1):\n    for j in range(1, nms_image.shape[1] - 1):\n        if weak_edges[i, j]:\n            if np.sum(strong_edges[i-vicinity:i+vicinity+1, j-vicinity:j+vicinity+1]) &gt; 0:\n                Edges[i, j] = 255\n\n# Dilate the edges\nEdges = cv.dilate(Edges, np.ones((3, 3), np.uint8), iterations=1)\n\n\n# Display the Original Image and the Detected Edges\nplt.figure(figsize=(7, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(Edges, cmap='gray')\nplt.axis('off')\nplt.title('Detected Edges')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/Canny_Edge.html#final-output",
    "href": "posts/2024/Canny_Edge.html#final-output",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The Final output is the edge image where the edges are detected. The edges are detected using the gradient information. The algorithm is very effective in detecting edges and is widely used in computer vision applications."
  },
  {
    "objectID": "posts/2024/Neural_Process.html",
    "href": "posts/2024/Neural_Process.html",
    "title": "Neural Processes",
    "section": "",
    "text": "This notebook demonstrates the implementation of the Neural Processes. The model is trained to predict the distribution of the target function given a few examples of the function. In this notebook we will attempt to reconstruct an entire image given a few pixels of the image.\n\\[Model(\\{x_c, y_c\\}_{c=1}^n, \\{x_t\\}_{t=1}^m) \\rightarrow \\{y_t\\}_{t=1}^m\\]\n\\(\\{x_c, y_c\\}_{c=1}^n\\) are the context points where \\(x_i\\) are the input features and \\(y_i\\) are the labels.\n\\(\\{x_t\\}_{t=1}^m\\) are the target features where we want to predict the label values \\(y_j\\).\n\n\n\nNeural Processes Block Diagram\n\n\n\nThe model is composed of two neural networks: the encoder and the decoder. The encoder takes the context set as the input and encodes them into a fixed-size representation. The decoder takes the representation and the target features and predicts the pixel labels of the given target features."
  },
  {
    "objectID": "posts/2024/Neural_Process.html#image-reconstruction-using-neural-processes",
    "href": "posts/2024/Neural_Process.html#image-reconstruction-using-neural-processes",
    "title": "Neural Processes",
    "section": "Image Reconstruction using Neural Processes",
    "text": "Image Reconstruction using Neural Processes\nWe wish to reconstruct an entire image given a few pixels of the image. The few pixels of the image are the context points and the entire image is the target function.\nSo for our case, the pixel locations \\(x = (x^1, x^2)\\) and the pixel intensity values \\(y = Img(x^1, x^2)\\) or just \\(I\\) denoting intensity for simplicity.\n\\[Model(\\{(x^1_c, x^2_c, I_c)\\}_{c=1}^n, \\{(x^1_t, x^2_t)\\}_{t=1}^m) \\rightarrow \\{I_t\\}_{t=1}^m\\]\nFor model training we will randomly sample a fixed number of pixel locations from the image and use them as context points. The model is trained to predict the pixel intensity value at all the other pixel locations. At the time of prediction, We will provide few pixel locations and the model will predict the pixel intensity values for all the pixel locations in the image.\n\n\n\nElaborate Neural Processes Block Diagram\n\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\nfrom torchsummary import summary\n\nfrom tqdm import tqdm\n\n\nGenerating Tasks from the MNIST Dataset\n\nDownloading the MNIST Dataset\nWe will be using the MNIST dataset for this task. The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9). We will download the dataset using the torchvision library. We will convert the images to tensors and no further preprocessing is being performed on the images.\n\n# Downlaoding the MNIST dataset\ntrain_data = MNIST(root='./data', train=True, download=True, transform=ToTensor())\ntest_data = MNIST(root='./data', train=False, download=True, transform= ToTensor())\n\n\n\nCreating Context and Target sets\nWe will sample a random number of pixel coordinates \\((x^1, x^2)\\) and the pixel intensity values \\(I\\) for these coordinates. These coordinates and pixel intensity values will be used as context points. The Test set will contain all the pixel coordinates and Intensities of the image. The model will predict the pixel intensity values for all the pixel coordinates in the image.\n\ndef get_Context_Target_Sets(images_data, no_context_points,Image_shape):\n\n    m,n = Image_shape\n\n    # All possible coordinates in the image. Size of MNIST image is 28x28\n    All_corrdinates = np.array([(i,j) for i in range(m) for j in range(n)])\n\n    # Iterate over the dataset and create the context set and target set\n    task_set = []\n    for i in tqdm(range(len(images_data))):\n\n        # Images in the dataset are of shape (1, 28, 28). We need to remove the channel dimension to get the shape (28, 28)\n        image, _ = images_data[i]\n        image = image.squeeze().numpy()     # un-squeeze the image to remove the channel dimension\n\n        # Sample random context points indices\n        context_idx = np.random.choice(range(len(All_corrdinates)), no_context_points, replace=False)\n\n        # Get the context points and their corresponding pixel values from the generated context indices\n        context_points = All_corrdinates[context_idx]\n        context_pixels = image[context_points[:,0], context_points[:,1]]\n\n        # Concatenate the context points and their corresponding pixel values\n        context_set = np.concatenate([context_points, context_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n        # For training the model, we need to predict the pixel values for all the pixels in the image\n        target_points = All_corrdinates\n        target_pixels = image[target_points[:,0], target_points[:,1]]\n\n        # Concatenate the target points and their corresponding pixel values\n        target_set = np.concatenate([target_points, target_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n        # Append the context set and target set to the train_set\n        task_set.append((context_set, target_set))\n\n    return task_set\n\n\ntrain_set = get_Context_Target_Sets(images_data=train_data, no_context_points=200, Image_shape=(28,28))\ntrain_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in train_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\n100%|██████████| 60000/60000 [00:04&lt;00:00, 12578.65it/s]\n\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])\n\n\n\ntest_set = get_Context_Target_Sets(images_data=test_data, no_context_points=200, Image_shape=(28,28))\ntest_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in test_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\n100%|██████████| 10000/10000 [00:00&lt;00:00, 12543.38it/s]\n\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])\n\n\n\n\n\nIntializing the Model Class\n\nEncoder and Decoder Networks\nOur Model class will contain the Encoder and Decoder networks. The Encoder network will take the context points and encode them into a fixed-size representation. The Decoder network will take the representation and the pixel coordinates and predict the pixel intensity value of the given coordinate.\n\n\nTraining Method\nMethod to train the model. The model is trained to predict the pixel intensity values of the target pixel coordinates given the context points. The loss function is the Mean Squared Error loss between the predicted pixel intensity values and the actual pixel intensity values.\n\n\nTesting Method\nMethod to test the model. The model is tested on the test set. The model is provided with the context points and the model predicts the pixel intensity values of the target pixel coordinates. The Mean Squared Error loss is calculated between the predicted pixel intensity values and the actual pixel intensity values.\n\nclass Neural_Procss_Model(nn.Module):\n    \"\"\"\n    Neural Process Model has two parts: encoder and decoder\n\n    Encoder takes in the context pairs (X1_c, X2_c, PixelIntensity) and encode them into a latent representation\n\n    Decoder takes in the latent representation and the target pairs (X1_t, X2_t) and output the predicted target pairs\n    \"\"\"\n    def __init__(self,device):\n        super(Neural_Procss_Model, self).__init__()\n\n        if device == 'cuda':\n            if torch.cuda.is_available():\n                device = torch.device('cuda')\n        \n        if device == 'mps':\n            device = torch.device('mps')\n\n        self.device = device\n\n        # Encoder takes in the context pairs (X1c,X2c,PixelIntensity) and encode them into a latent representation\n        self.encoder = nn.Sequential(\n            nn.Linear(3,64),\n            nn.ReLU(),\n            nn.Linear(64,128),\n            nn.ReLU(),\n            nn.Linear(128,256),\n        )\n\n        # Decoder takes in the latent representation and the target pairs and output the predicted target pairs\n        self.decoder = nn.Sequential(\n            nn.Linear(256+2,128),\n            nn.ReLU(),\n            nn.Linear(128,64),\n            nn.ReLU(),\n            nn.Linear(64,32),\n            nn.ReLU(),\n            nn.Linear(32,1)\n        )\n\n    def forward(self, context_pairs, target_pairs):\n        \"\"\"\n        Forward pass of the Neural Process Model. \n        \n        It takes in the context cordinate pairs (X1_c, X2_c, PixelIntensity) and target cordinate pairs (X1_t, X2_t, PixelIntensity) and output the predicted target pixel intensity for the target cordinate pairs\n\n        Parameters:\n        context_pairs: torch.Tensor of shape (batch_size, num_context_pairs, 3)\n        target_pairs: torch.Tensor of shape (batch_size, num_target_pairs, 2)\n\n        Returns:\n        predicted_target_Pixel Intensity: torch.Tensor of shape (batch_size, num_target_pairs, 1)\n        \"\"\"\n        \n        # Encode the context pairs into a latent representation\n        latent_representation = self.encoder(context_pairs)\n\n        # Average the latent representation\n        latent_representation = torch.mean(latent_representation,dim=1)\n\n        # Repeat the latent representation for each target pair\n        latent_representation = latent_representation.unsqueeze(1).repeat(1,target_pairs.size(1),1)\n\n        # Concatenate the latent representation with the target pixel locations\n        target_pixel_locations = target_pairs[:,:,:2]\n        target = torch.cat([latent_representation,target_pixel_locations],dim=-1)\n\n        # Decode the target pairs to obtain the predicted target pixel intensity\n        predicted_target_pixel_intensity = self.decoder(target)\n\n        return predicted_target_pixel_intensity\n    \n\n    def train(self, train_dataloader, num_epochs=100, optim = torch.optim.Adam, lr=3e-4, criterion = nn.MSELoss(),verbose=True):\n        \"\"\"\n        Train the Neural Process Model\n\n        Parameters:\n        train_dataloader: DataLoader object for the training data\n        num_epochs: int, number of epochs to train the model\n        optimer: str, optimer to use for training the model\n        lr: float, learning rate for the optimer\n        criterion: loss function to use for training the model\n        \"\"\"\n\n        device = self.device\n\n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the optimizer\n        optimizer = optim(self.parameters(), lr=lr)\n\n        for epoch in range(num_epochs):\n            for i, (context_pairs, target_pairs) in enumerate(train_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Zero the gradients\n                optimizer.zero_grad()\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                # Backward pass\n                loss.backward()\n\n                # Update the weights\n                optimizer.step()\n\n            if verbose:\n                print(\"Epoch: {}/{} Loss: {:.5f}\".format(epoch+1,num_epochs,loss.item()))\n\n\n\n    def test(self, test_dataloader, criterion = nn.MSELoss()):\n        \"\"\"\n        Test the Neural Process Model\n\n        Parameters:\n        test_dataloader: DataLoader object for the test data\n        \"\"\"\n\n        device = self.device\n    \n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the loss\n        test_loss = 0\n\n        with torch.no_grad():\n            for i, (context_pairs, target_pairs) in enumerate(test_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                test_loss += loss.item()\n\n        print(f'Test Loss: {test_loss/len(test_dataloader)}')\n\n        \n\n\n\n\nTraining the model\n\n# Set the device. If cuda is available, use cuda. \nif torch.cuda.is_available():\n    device = torch.device('cuda')\n\n# If mps is available, use mps. \nelif torch.mps.is_available():\n    device = torch.device('mps')\n\n# Else use cpu\nelse:\n    device = torch.device('cpu')\n\nprint(\"Device in use: \", device)\n\nDevice in use:  mps\n\n\n\nmodel = Neural_Procss_Model(device=device)\n\n# model.to('cuda')\n\nmodel.train(train_dataloader, num_epochs=30)\n\nEpoch: 1/30 Loss: 0.06018\nEpoch: 2/30 Loss: 0.06170\nEpoch: 3/30 Loss: 0.05950\nEpoch: 4/30 Loss: 0.06060\nEpoch: 5/30 Loss: 0.05958\nEpoch: 6/30 Loss: 0.06148\nEpoch: 7/30 Loss: 0.05642\nEpoch: 8/30 Loss: 0.05425\nEpoch: 9/30 Loss: 0.05090\nEpoch: 10/30 Loss: 0.04823\nEpoch: 11/30 Loss: 0.04689\nEpoch: 12/30 Loss: 0.04165\nEpoch: 13/30 Loss: 0.04482\nEpoch: 14/30 Loss: 0.04253\nEpoch: 15/30 Loss: 0.03567\nEpoch: 16/30 Loss: 0.04409\nEpoch: 17/30 Loss: 0.03556\nEpoch: 18/30 Loss: 0.03798\nEpoch: 19/30 Loss: 0.03793\nEpoch: 20/30 Loss: 0.03933\nEpoch: 21/30 Loss: 0.03644\nEpoch: 22/30 Loss: 0.03336\nEpoch: 23/30 Loss: 0.03567\nEpoch: 24/30 Loss: 0.03594\nEpoch: 25/30 Loss: 0.03620\nEpoch: 26/30 Loss: 0.03798\nEpoch: 27/30 Loss: 0.03487\nEpoch: 28/30 Loss: 0.03350\nEpoch: 29/30 Loss: 0.03534\nEpoch: 30/30 Loss: 0.03392\n\n\n\nmodel.test(test_dataloader)\n\nTest Loss: 0.03589645992762174\n\n\n\n\nTesting the model\n\nfor context_pairs, target_pairs in test_dataloader:\n    context_pairs = context_pairs.to(device)\n    target_pairs = target_pairs.to(device)\n\n    predicted_target_pixel_intensity = model(context_pairs, target_pairs)\n\n    for i in range(5):\n        predicted_pixel = predicted_target_pixel_intensity[i].detach().cpu().numpy()\n        predicted_image = predicted_pixel.reshape(28,28)\n\n        actual_pixel = target_pairs[i][:,2].detach().cpu().numpy()\n        actual_image = actual_pixel.reshape(28,28)\n\n        context_image = np.zeros((28,28))\n        context_pixel_locations = context_pairs[i][:,:2].detach().cpu().numpy().astype(int) \n        context_image[context_pixel_locations[:,0], context_pixel_locations[:,1]] = context_pairs[i][:,2].detach().cpu().numpy()\n\n        plt.figure(figsize=(9,4))\n\n        plt.subplot(1,3,1)\n        plt.imshow(context_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Context Image')\n        \n        plt.subplot(1,3,2)\n        plt.imshow(predicted_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Predicted Image')\n\n        plt.subplot(1,3,3)\n        plt.imshow(actual_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Actual Image')\n        \n        plt.tight_layout()\n        plt.show()\n\n    break"
  },
  {
    "objectID": "posts/2024/Neural_Process.html#neural-processes",
    "href": "posts/2024/Neural_Process.html#neural-processes",
    "title": "Neural Processes",
    "section": "",
    "text": "This notebook demonstrates the implementation of the Neural Processes. The model is trained to predict the distribution of the target function given a few examples of the function. In this notebook we will attempt to reconstruct an entire image given a few pixels of the image.\n\\[Model(\\{x_c, y_c\\}_{c=1}^n, \\{x_t\\}_{t=1}^m) \\rightarrow \\{y_t\\}_{t=1}^m\\]\n\\(\\{x_c, y_c\\}_{c=1}^n\\) are the context points where \\(x_i\\) are the pixel coordinates and \\(y_i\\) are the pixel intensity values.\n\\(\\{x_t\\}_{t=1}^m\\) are the target pixel coordinates where we want to predict the pixel intensity values \\(y_j\\).\n\n\n\nNeural Processes Block Diagram\n\n\n\nThe model is composed of two neural networks: the encoder and the decoder. The encoder takes pixel coordinates and pixel intensitiy value of that coordinate as input and encodes them into a fixed-size representation. The decoder takes the representation and the pixel coordinates and predicts the pixel intensity value of the given coordinate."
  },
  {
    "objectID": "posts/2024/Neural_Process.html#generating-tasks-from-the-mnist-dataset",
    "href": "posts/2024/Neural_Process.html#generating-tasks-from-the-mnist-dataset",
    "title": "Neural Processes",
    "section": "Generating Tasks from the MNIST Dataset",
    "text": "Generating Tasks from the MNIST Dataset\n\nDownloading the MNIST Dataset\nWe will be using the MNIST dataset for this task. The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9). We will download the dataset using the torchvision library. We will convert the images to tensors and no further preprocessing is being performed on the images.\n\n# Downlaoding the MNIST dataset\ntrain_data = MNIST(root='./data', train=True, download=True, transform=ToTensor())\ntest_data = MNIST(root='./data', train=False, download=True, transform= ToTensor())\n\n\n\nCreating Context and Target sets\nWe will sample a random number of pixel coordinates \\((x^1, x^2)\\) and the pixel intensity values \\(I\\) for these coordinates. These coordinates and pixel intensity values will be used as context points. The Test set will contain all the pixel coordinates and Intensities of the image. The model will predict the pixel intensity values for all the pixel coordinates in the image.\n\ndef get_Context_Target_Sets(images_data, no_context_points,Image_shape):\n\n    m,n = Image_shape\n\n    # All possible coordinates in the image. Size of MNIST image is 28x28\n    All_corrdinates = np.array([(i,j) for i in range(m) for j in range(n)])\n\n    # Iterate over the dataset and create the context set and target set\n    task_set = []\n    for i in tqdm(range(len(images_data))):\n\n        # Images in the dataset are of shape (1, 28, 28). We need to remove the channel dimension to get the shape (28, 28)\n        image, _ = images_data[i]\n        image = image.squeeze().numpy()     # un-squeeze the image to remove the channel dimension\n\n        # Sample random context points indices\n        context_idx = np.random.choice(range(len(All_corrdinates)), no_context_points, replace=False)\n\n        # Get the context points and their corresponding pixel values from the generated context indices\n        context_points = All_corrdinates[context_idx]\n        context_pixels = image[context_points[:,0], context_points[:,1]]\n\n        # Concatenate the context points and their corresponding pixel values\n        context_set = np.concatenate([context_points, context_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n        # For training the model, we need to predict the pixel values for all the pixels in the image\n        target_points = All_corrdinates\n        target_pixels = image[target_points[:,0], target_points[:,1]]\n\n        # Concatenate the target points and their corresponding pixel values\n        target_set = np.concatenate([target_points, target_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n        # Append the context set and target set to the train_set\n        task_set.append((context_set, target_set))\n\n    return task_set\n\n\ntrain_set = get_Context_Target_Sets(images_data=train_data, no_context_points=200, Image_shape=(28,28))\ntrain_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in train_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\n100%|██████████| 60000/60000 [00:04&lt;00:00, 12578.65it/s]\n\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])\n\n\n\ntest_set = get_Context_Target_Sets(images_data=test_data, no_context_points=200, Image_shape=(28,28))\ntest_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in test_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\n100%|██████████| 10000/10000 [00:00&lt;00:00, 11855.53it/s]\n\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])"
  },
  {
    "objectID": "posts/2024/Neural_Process.html#intializing-the-neural-process-class",
    "href": "posts/2024/Neural_Process.html#intializing-the-neural-process-class",
    "title": "Neural Processes",
    "section": "Intializing the Neural Process Class",
    "text": "Intializing the Neural Process Class\n\nEncoder and Decoder Networks\nOur Model class will contain the Encoder and Decoder networks. The Encoder network will take the context points and encode them into a fixed-size representation. The Decoder network will take the representation and the pixel coordinates and predict the pixel intensity value of the given coordinate.\n\n\nTraining Method\nMethod to train the model. The model is trained to predict the pixel intensity values of the target pixel coordinates given the context points. The loss function is the Mean Squared Error loss between the predicted pixel intensity values and the actual pixel intensity values.\n\n\nTesting Method\nMethod to test the model. The model is tested on the test set. The model is provided with the context points and the model predicts the pixel intensity values of the target pixel coordinates. The Mean Squared Error loss is calculated between the predicted pixel intensity values and the actual pixel intensity values.\n\nclass Neural_Procss_Model(nn.Module):\n    \"\"\"\n    Neural Process Model has two parts: encoder and decoder\n\n    Encoder takes in the context pairs (X1_c, X2_c, PixelIntensity) and encode them into a latent representation\n\n    Decoder takes in the latent representation and the target pairs (X1_t, X2_t) and output the predicted target pairs\n    \"\"\"\n    def __init__(self,device):\n        super(Neural_Procss_Model, self).__init__()\n\n        if device == 'cuda':\n            if torch.cuda.is_available():\n                device = torch.device('cuda')\n        \n        if device == 'mps':\n            device = torch.device('mps')\n\n        self.device = device\n\n        # Encoder takes in the context pairs (X1c,X2c,PixelIntensity) and encode them into a latent representation\n        self.encoder = nn.Sequential(\n            nn.Linear(3,64),\n            nn.ReLU(),\n            nn.Linear(64,128),\n            nn.ReLU(),\n            nn.Linear(128,256),\n        )\n\n        # Decoder takes in the latent representation and the target pairs and output the predicted target pairs\n        self.decoder = nn.Sequential(\n            nn.Linear(256+2,128),\n            nn.ReLU(),\n            nn.Linear(128,64),\n            nn.ReLU(),\n            nn.Linear(64,32),\n            nn.ReLU(),\n            nn.Linear(32,1)\n        )\n\n    def forward(self, context_pairs, target_pairs):\n        \"\"\"\n        Forward pass of the Neural Process Model. \n        \n        It takes in the context cordinate pairs (X1_c, X2_c, PixelIntensity) and target cordinate pairs (X1_t, X2_t, PixelIntensity) and output the predicted target pixel intensity for the target cordinate pairs\n\n        Parameters:\n        context_pairs: torch.Tensor of shape (batch_size, num_context_pairs, 3)\n        target_pairs: torch.Tensor of shape (batch_size, num_target_pairs, 2)\n\n        Returns:\n        predicted_target_Pixel Intensity: torch.Tensor of shape (batch_size, num_target_pairs, 1)\n        \"\"\"\n        \n        # Encode the context pairs into a latent representation\n        latent_representation = self.encoder(context_pairs)\n\n        # Average the latent representation\n        latent_representation = torch.mean(latent_representation,dim=1)\n\n        # Repeat the latent representation for each target pair\n        latent_representation = latent_representation.unsqueeze(1).repeat(1,target_pairs.size(1),1)\n\n        # Concatenate the latent representation with the target pixel locations\n        target_pixel_locations = target_pairs[:,:,:2]\n        target = torch.cat([latent_representation,target_pixel_locations],dim=-1)\n\n        # Decode the target pairs to obtain the predicted target pixel intensity\n        predicted_target_pixel_intensity = self.decoder(target)\n\n        return predicted_target_pixel_intensity\n    \n\n    def train(self, train_dataloader, num_epochs=100, optim = torch.optim.Adam, lr=3e-4, criterion = nn.MSELoss(),verbose=True):\n        \"\"\"\n        Train the Neural Process Model\n\n        Parameters:\n        train_dataloader: DataLoader object for the training data\n        num_epochs: int, number of epochs to train the model\n        optimer: str, optimer to use for training the model\n        lr: float, learning rate for the optimer\n        criterion: loss function to use for training the model\n        \"\"\"\n\n        device = self.device\n\n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the optimizer\n        optimizer = optim(self.parameters(), lr=lr)\n\n        for epoch in range(num_epochs):\n            for i, (context_pairs, target_pairs) in enumerate(train_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Zero the gradients\n                optimizer.zero_grad()\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                # Backward pass\n                loss.backward()\n\n                # Update the weights\n                optimizer.step()\n\n            if verbose:\n                print(\"Epoch: {}/{} Loss: {:.5f}\".format(epoch+1,num_epochs,loss.item()))\n\n\n\n    def test(self, test_dataloader, criterion = nn.MSELoss()):\n        \"\"\"\n        Test the Neural Process Model\n\n        Parameters:\n        test_dataloader: DataLoader object for the test data\n        \"\"\"\n\n        device = self.device\n    \n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the loss\n        test_loss = 0\n\n        with torch.no_grad():\n            for i, (context_pairs, target_pairs) in enumerate(test_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                test_loss += loss.item()\n\n        print(f'Test Loss: {test_loss/len(test_dataloader)}')\n\n        \n\n\n\nTraining the Neural Process model\n\n# Set the device. If cuda is available, use cuda. \nif torch.cuda.is_available():\n    device = torch.device('cuda')\n\n# If mps is available, use mps. \nelif torch.mps.is_available():\n    device = torch.device('mps')\n\n# Else use cpu\nelse:\n    device = torch.device('cpu')\n\nprint(\"Device in use: \", device)\n\nDevice in use:  mps\n\n\n\nmodel = Neural_Procss_Model(device=device)\n\n# model.to('cuda')\n\nmodel.train(train_dataloader, num_epochs=30)\n\nEpoch: 1/30 Loss: 0.06018\nEpoch: 2/30 Loss: 0.06170\nEpoch: 3/30 Loss: 0.05950\nEpoch: 4/30 Loss: 0.06060\nEpoch: 5/30 Loss: 0.05958\nEpoch: 6/30 Loss: 0.06148\nEpoch: 7/30 Loss: 0.05642\nEpoch: 8/30 Loss: 0.05425\nEpoch: 9/30 Loss: 0.05090\nEpoch: 10/30 Loss: 0.04823\nEpoch: 11/30 Loss: 0.04689\nEpoch: 12/30 Loss: 0.04165\nEpoch: 13/30 Loss: 0.04482\nEpoch: 14/30 Loss: 0.04253\nEpoch: 15/30 Loss: 0.03567\nEpoch: 16/30 Loss: 0.04409\nEpoch: 17/30 Loss: 0.03556\nEpoch: 18/30 Loss: 0.03798\nEpoch: 19/30 Loss: 0.03793\nEpoch: 20/30 Loss: 0.03933\nEpoch: 21/30 Loss: 0.03644\nEpoch: 22/30 Loss: 0.03336\nEpoch: 23/30 Loss: 0.03567\nEpoch: 24/30 Loss: 0.03594\nEpoch: 25/30 Loss: 0.03620\nEpoch: 26/30 Loss: 0.03798\nEpoch: 27/30 Loss: 0.03487\nEpoch: 28/30 Loss: 0.03350\nEpoch: 29/30 Loss: 0.03534\nEpoch: 30/30 Loss: 0.03392\n\n\n\nmodel.test(test_dataloader)\n\nTest Loss: 0.03589645992762174"
  },
  {
    "objectID": "posts/2024/Neural_Process.html#testing-the-neural-process-model",
    "href": "posts/2024/Neural_Process.html#testing-the-neural-process-model",
    "title": "Neural Processes",
    "section": "Testing the Neural Process model",
    "text": "Testing the Neural Process model\n\nfor context_pairs, target_pairs in test_dataloader:\n    context_pairs = context_pairs.to(device)\n    target_pairs = target_pairs.to(device)\n\n    predicted_target_pixel_intensity = model(context_pairs, target_pairs)\n\n    for i in range(5):\n        predicted_pixel = predicted_target_pixel_intensity[i].detach().cpu().numpy()\n        predicted_image = predicted_pixel.reshape(28,28)\n\n        actual_pixel = target_pairs[i][:,2].detach().cpu().numpy()\n        actual_image = actual_pixel.reshape(28,28)\n\n        context_image = np.zeros((28,28))\n        context_pixel_locations = context_pairs[i][:,:2].detach().cpu().numpy().astype(int) \n        context_image[context_pixel_locations[:,0], context_pixel_locations[:,1]] = context_pairs[i][:,2].detach().cpu().numpy()\n\n        plt.figure(figsize=(9,4))\n\n        plt.subplot(1,3,1)\n        plt.imshow(context_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Context Image')\n        \n        plt.subplot(1,3,2)\n        plt.imshow(predicted_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Predicted Image')\n\n        plt.subplot(1,3,3)\n        plt.imshow(actual_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Actual Image')\n        \n        plt.tight_layout()\n        plt.show()\n\n    break"
  },
  {
    "objectID": "posts/2024/Lucas_Kanade.html",
    "href": "posts/2024/Lucas_Kanade.html",
    "title": "Lucas Kanade Optical Flow",
    "section": "",
    "text": "Optical Flow\nOptical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movement of object or camera. It is vector field where each vector denotes the movement of points from first frame to second. Generally, optical flow is used to track the motion of objects in a video. Lets assume you have captured two frames with a small time difference \\(\\Delta t\\). You wish to track the motion of a pixel located at \\((x, y)\\) in first frame. In the second frame, the pixel has moved to \\((x + \\Delta x, y + \\Delta y)\\). The vector \\((\\Delta x, \\Delta y)\\) is the optical flow vector.\n\n\n\n\n\nOptical Flow : Brightness Constancy Constraint\n\n\n\\[I(x, y, t) = I(x + \\Delta x, y + \\Delta y, t + \\Delta t)\\]\nThe above equation is the basic assumption in optical flow. It assumes that intensity of an object does not change between two consecutive frames. This constraint is called brightness constancy constraint. This equation can be expanded using taylor series to get the optical flow equation.\n\\[I(x + \\Delta x, y + \\Delta y, t + \\Delta t) \\approx I(x, y, t) + \\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t = I(x, y, t)\\]\n\\[\\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t = 0\\]\n\\[\\frac{\\partial I}{\\partial x} u + \\frac{\\partial I}{\\partial y} v + \\frac{\\partial I}{\\partial t} = 0\\]\nwhere \\(u = \\frac{\\Delta x}{\\Delta t}\\) and \\(v = \\frac{\\Delta y}{\\Delta t}\\) are the optical flow velocities in x and y directions respectively.\nThe above equation is called optical flow equation. It is a single equation with two unknowns \\(u\\) and \\(v\\). Hence, it is an ill-posed problem. To solve this problem, we need to make some more assumptions. One of the assumption to arrive here was brightness constancy assumption. It assumes that the intensity of an object does not change between two consecutive frames. The other assumption made was given by lucas and kanade. They assumed that the optical flow is same for all the pixels in a neighborhood. This assumption is called spatial coherence assumption.\n\n\nLucas Kanade Optical Flow\nThe Lucas-Kanade method is a widely used differential method for optical flow estimation developed by Bruce D. Lucas and Takeo Kanade. It assumes that the flow is essentially constant in a local neighbourhood of the pixel under consideration, and solves the basic optical flow equations for all the pixels in that neighbourhood by the least squares criterion.\n\n\n\nLucas Kanade : spatial coherence assumption\n\n\n\\[\\frac{\\partial I}{\\partial x} u + \\frac{\\partial I}{\\partial y} v + \\frac{\\partial I}{\\partial t} = 0\\]\n\\[E_x = \\frac{\\partial I}{\\partial x}, E_y = \\frac{\\partial I}{\\partial y}, E_t = \\frac{\\partial I}{\\partial t}\\]\n\\[E_x u + E_y v + E_t = 0\\]\n\\[\\begin{bmatrix} E_x & E_y \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = -E_t\\]\n\\[ \\begin{bmatrix} E_x(i-1,j-1) & E_y(i-1,j-1) \\\\ E_x(i-1,j) & E_y(i-1,j) \\\\ E_x(i-1,j+1) & E_y(i-1,j+1) \\\\ E_x(i,j-1) & E_y(i,j-1) \\\\ E_x(i,j) & E_y(i,j) \\\\ E_x(i,j+1) & E_y(i,j+1) \\\\ E_x(i+1,j-1) & E_y(i+1,j-1) \\\\ E_x(i+1,j) & E_y(i+1,j) \\\\ E_x(i+1,j+1) & E_y(i+1,j+1) \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} -E_t(i-1,j-1) \\\\ -E_t(i-1,j) \\\\ -E_t(i-1,j+1) \\\\ -E_t(i,j-1) \\\\ -E_t(i,j) \\\\ -E_t(i,j+1) \\\\ -E_t(i+1,j-1) \\\\ -E_t(i+1,j) \\\\ -E_t(i+1,j+1) \\end{bmatrix}\\]\n\\[A_{(9,2)}  x_{(2,1)} = b_{(9,1)}\\]\n\\[A \\overrightarrow{x} = \\overrightarrow{b}\\]\n\\[x = (A^T A)^{-1} A^T b\\]\nSo for a chosen patch we can estimate the optical flow using above equation. This could also be written as\n\\[\\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} \\sum E_x E_x & \\sum E_x E_y \\\\ \\sum E_y E_x & \\sum E_y E_y \\end{bmatrix}^{-1} \\begin{bmatrix} -\\sum E_x E_t \\\\ -\\sum E_y E_t \\end{bmatrix}\\]\nThis is the final equation used to estimate the optical flow using Lucas Kanade method. The above equation is solved for each patch in the image to get the optical flow vectors. Optical flow algorithm could be summarized as follows:\n\nCompute image gradients \\(E_x\\) and \\(E_y\\).\nCompute temporal gradient \\(E_t\\).\nCompute the matrix \\(A\\) and vector \\(b\\) for each patch.\nSolve the equation \\(A \\overrightarrow{x} = \\overrightarrow{b}\\) to get the optical flow vectors.\n\n\n\nLucas Kanade Implementation\nComing soon…!"
  }
]