[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ayush Shrivastava",
    "section": "",
    "text": "I am a first-year PhD student in Computer Science and Engineering at IIT Gandhinagar, working at the intersection of machine learning, sustainability, and health sensing. I am an active member of the Sustainability Lab, led by Prof. Nipun Batra, and the Smash Lab, led by Prof. Mayank Goel.\nI hold a master’s degree in Computer Science and Engineering from IIT Gandhinagar and a bachelor’s degree in Electronics and Telecommunications from Jabalpur Engineering College. My research focuses on applying machine learning to develop innovative solutions in sustainability and health sensing. I am passionate about solving challenging problems using machine learning to create meaningful impact."
  },
  {
    "objectID": "index.html#my-journey-so-far",
    "href": "index.html#my-journey-so-far",
    "title": "Ayush Shrivastava",
    "section": "My Journey So Far",
    "text": "My Journey So Far\n\n\n\n\n\n\n31 Aug 2024\n\n\nJoined Sustainability Lab and Smash Lab as a PhD student\n\n\n\n\n15 Jul 2024\n\n\nJoined IIT Gandhinagar as PhD student under the supervision of Prof Nipun Batra and Prof Mayank Goel\n\n\n\n\n8-11 Jul 2024\n\n\nAttended 7th ACM SIGCAS/SIGCHI Conference of Computing and Sustainable Societies ACM COMPASS 2024\n\n\n\n\n29 Jun 2024\n\n\nConvocation! Got Masters degree from IIT Gandhinagar\n\n\n\n\n14 Jun 2024\n\n\nGot brutally attacked but successfully defended my Masters Thesis\n\n\n\n\n\n2 Feb 2024\n\n\nSubmitted the first draft of our work to IMWUT ’24, marking my first research paper submission ever.\n\n\n\n\n10 Oct 2023\n\n\nGot Placed in Atlas Copco as Software Developer\n\n\n\n\n\n01 Jan 2023\n\n\nJoined Sustainability Lab as a masters student at IIT Gandhinagar\n\n\n\n\n18 jul 2022\n\n\nJoined IIT Gandhinagar for M.Tech in Computer Science and Engineering\n\n\n\n\n15 Mar 2022\n\n\nAttained 98.16 percentile in Graduate Aptitude Test in Engineering 2022 (GATE’22)\n\n\n\n\n17 jun 2019\n\n\nJoined IBM India as an Application Developer, Enterprise Resource Planning (ERP)\n\n\n\n\n15 May 2019\n\n\nCompleted B.E in Electronics and Telecommunication from Jabalpur Engineering College, Jabalpur"
  },
  {
    "objectID": "posts/2024/CondaPost.html",
    "href": "posts/2024/CondaPost.html",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "",
    "text": "This blogpost is written for myself as I keep forgetting the steps to set up a conda environment for my ML projects."
  },
  {
    "objectID": "posts/2024/CondaPost.html#install-conda",
    "href": "posts/2024/CondaPost.html#install-conda",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Install Conda",
    "text": "Install Conda\nThese four commands quickly and quietly install the latest 64-bit version of the installer and then clean up after themselves. To install a different version or architecture of Miniconda for Linux, change the name of the .sh installer in the wget command.\nVisit the Miniconda website to find the latest version of Miniconda for Linux.\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\nAfter installing, initialize your newly-installed Miniconda. The following commands initialize for bash and zsh shells:\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\nyou can run conda -V to check if conda is installed correctly."
  },
  {
    "objectID": "posts/2024/CondaPost.html#creating-conda-environment",
    "href": "posts/2024/CondaPost.html#creating-conda-environment",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Creating Conda Environment",
    "text": "Creating Conda Environment\nThe below command will create the conda environment with the name EnvName and python version 3.9.\nconda create --name EnvName python=3.9 jupyter\nTo activate the environment, run the below command:\nconda activate EnvName"
  },
  {
    "objectID": "posts/2024/CondaPost.html#installing-pytorch",
    "href": "posts/2024/CondaPost.html#installing-pytorch",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Installing Pytorch",
    "text": "Installing Pytorch\nVisit the Pytorch website to find the latest version of Pytorch. You can choose your system configuration and get the command to install Pytorch.\n\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\ncheck if everything is installed correctly by running the below command:\npython -c \"import torch; print(torch.__version__)\"\nInstalling other libraries and packages."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "2024\n\n\nSpring and Fall : Machine Learning,\n\n\nProfessor Incharge: Nipun Batra  Organised and evaluated quizzes, assignments, took vivas for a classroom of 300 students, while also playing a pivotal role in supporting classroom logistics.\n\n\n2023\n\n\nFall : Computer Systems\n\n\nProfessor Incharge: Abhishek Bichhawat, Sameer G Kulkarni  Graded assignments and assisted the professor run a class of 40 students smoothly.\n\n\nSummer : World of Engineering\n\n\nProfessor Incharge: Udit Bhatia  Mentored a group of 30 students in the identification, conceptualization, and modeling of a prototype to address a real-world problem, fostering their problem-solving abilities and teamwork skills.\n\n\nSpring : Probability, Statistics and Data Visualization\n\n\nProfessor Incharge: Shanmuganathan Raman  Successfully guided over 30 students in Python libraries, including Numpy, Pandas, Matplotlib, Scipy, and Scikit-learn, enhancing their data visualization skills.\n\n\n2022\n\n\nWinter : Introduction to Computing\n\n\nProfessor Incharge: Nipun Batra, Balagopal Komarath  Led a Python lab for 30 students, teaching them essential Python concepts, and helped manage logistics, invigilation, and quiz evaluation for a class of 300 students."
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Gallery",
    "section": "",
    "text": "Welcome to my gallery! Here you’ll find a collection of images from my life, including memorable moments and photos I’ve taken. I’ll continue to add more images as I capture new experiences and milestones along my journey.\n\n\n\n\n![compass24.jpg](./images/gallery/compass24.jpg)\n&lt;p&gt;Attending ACM Compass with the Lab&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/ACM_Compass1.jpg\" alt=\"ACM Compass inside Conference Hall\"&gt;\n&lt;p&gt;ACM Compass - infront of conference hall&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/IIITD1.jpg\" alt=\"Near IIITD gate\"&gt;\n&lt;p&gt;Inside IIIT Delhi Campus&lt;/p&gt;\n\n\n\n\n\n\n\n&lt;img src=\"images/gallery/convocation2.jpg\" alt=\"Awarded the MTech\"&gt;\n&lt;p&gt;Awarded the MTech in Computer Science and Engineering at IITGN convocation June 2024&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/convocation.jpg\" alt=\"Outside IITGN Convocation Hall 2024\"&gt;\n&lt;p&gt;Outside IITGN Convocation Hall June 2024&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/convocation3.jpg\" alt=\"Outside IITGN Convocation Hall 2024\"&gt;\n&lt;p&gt;Outside IITGN Convocation Hall with my family&lt;/p&gt;\n\n\n\n\n\n\n\n\n&lt;img src=\"images/gallery/MtechThesisDefense.jpg\" alt=\"2024 June Mtech defense\"&gt;\n&lt;p&gt;Suraj and I celebrating the successful defense of our MTech thesis&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/MtechThesisDefense3.jpg\" alt=\"2024 June Mtech defense\"&gt;\n&lt;p&gt;Celebrating the successful defense of our MTech thesis&lt;/p&gt;"
  },
  {
    "objectID": "gallery.html#acm-compass-2024-iiit_delhi",
    "href": "gallery.html#acm-compass-2024-iiit_delhi",
    "title": "Gallery",
    "section": "",
    "text": "![compass24.jpg](./images/gallery/compass24.jpg)\n&lt;p&gt;Attending ACM Compass with the Lab&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/ACM_Compass1.jpg\" alt=\"ACM Compass inside Conference Hall\"&gt;\n&lt;p&gt;ACM Compass - infront of conference hall&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/IIITD1.jpg\" alt=\"Near IIITD gate\"&gt;\n&lt;p&gt;Inside IIIT Delhi Campus&lt;/p&gt;"
  },
  {
    "objectID": "gallery.html#mtech-convocation-2024",
    "href": "gallery.html#mtech-convocation-2024",
    "title": "Gallery",
    "section": "",
    "text": "&lt;img src=\"images/gallery/convocation2.jpg\" alt=\"Awarded the MTech\"&gt;\n&lt;p&gt;Awarded the MTech in Computer Science and Engineering at IITGN convocation June 2024&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/convocation.jpg\" alt=\"Outside IITGN Convocation Hall 2024\"&gt;\n&lt;p&gt;Outside IITGN Convocation Hall June 2024&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/convocation3.jpg\" alt=\"Outside IITGN Convocation Hall 2024\"&gt;\n&lt;p&gt;Outside IITGN Convocation Hall with my family&lt;/p&gt;"
  },
  {
    "objectID": "gallery.html#mtech-thesis-defense-2024",
    "href": "gallery.html#mtech-thesis-defense-2024",
    "title": "Gallery",
    "section": "",
    "text": "&lt;img src=\"images/gallery/MtechThesisDefense.jpg\" alt=\"2024 June Mtech defense\"&gt;\n&lt;p&gt;Suraj and I celebrating the successful defense of our MTech thesis&lt;/p&gt;\n\n\n&lt;img src=\"images/gallery/MtechThesisDefense3.jpg\" alt=\"2024 June Mtech defense\"&gt;\n&lt;p&gt;Celebrating the successful defense of our MTech thesis&lt;/p&gt;"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Neural Processes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanny Edge Detector\n\n\n\nComputer Vision\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nDec 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Conda Environment for ML Projects\n\n\n\nHelpful Tips\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJun 16, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publication",
    "section": "",
    "text": "Under review will be added soon."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/2024/Canny_Edge.html",
    "href": "posts/2024/Canny_Edge.html",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The Canny Edge detector is named after its inventor, John F. Canny. It is among the most popular edge detection algorithms due to its simplicity and effectiveness. The algorithm uses gradient information to detect edges. It follow the following steps:\n\n\n\n\nThe algorithm uses a Gaussian filter to smooth the image and reduce noise. This is done to prevent the algorithm from detecting false edges due to noise. It also smoothens the image to reduce the abrupt changes in pixel values. These abrupt changes may end up being detected as edges.\n\\[I'(x,y) = G(x,y) * I(x,y)\\]\nwhere \\(I(x,y)\\) is the input image, \\(G(x,y)\\) is the Gaussian filter and \\(I'(x,y)\\) is the smoothed image. ’*’ represents convolution operation between the image and the filter. Reading the image and convert it to grayscale. Image is converted to grayscale because the edge detection is done on the intensity values of the image. The color information is not required for edge detection.\n\n# Importing libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Read the image\nimage = cv.imread('Flower.jpeg')\n\nplt.figure(figsize=(12, 4))\n# Convert the image to RGB (OpenCV loads it into BGR by default)\nplt.subplot(1, 3, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\n# Convert the image to grayscale\ngray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nplt.subplot(1, 3, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.axis('off')\nplt.title('Grayscaled Image')\n\n# Apply Gaussian Blur\nblurred_image = cv.GaussianBlur(gray_image, (3, 3), 0)\nplt.subplot(1, 3, 3)\nplt.imshow(blurred_image, cmap='gray')\nplt.axis('off')\nplt.title('Gaussian Blurred Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then calculates the gradient of the image. The gradient is calculated using the Sobel operator. The Sobel operator is a discrete differentiation operator. It computes an approximation of gradient of the image. The operator uses two 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives - one for horizontal changes, and one for vertical. \\(G_x\\) and \\(G_y\\) are the horizontal and vertical Sobel operators respectively.\n\\[G_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\] \\[G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\]\n\\[I_y = I' * G_y\\] \\[I_x = I' * G_x\\]\nwhere \\(I_x\\) and \\(I_y\\) are the horizontal and vertical gradients respectively.\nNow the combined gradient magnitude could be calculated using two methods. * Using L1 Norm:\n\\[Magnitude = |I_x| + |I_y|\\]\n\nUsing L2 Norm:\n\n\\[Magnitude =  \\sqrt{I_x^2 + I_y^2}\\]\n\\[Direction = \\arctan(\\frac{I_y}{I_x})\\]\nMagnitude signifies the strength of the edge. The direction is the angle of the edge. The direction is rounded to one of four angles representing vertical, horizontal and two diagonals. The angles are 0, 45, 90 and 135 degrees. We are assuming the edges are in one of these four directions.\n\n# Compute the gradients by applying Sobel operator\nI_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=3)\nI_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=3)\n\n# Compute the magnitude of the gradients\nmagnitude = np.sqrt(I_x**2 + I_y**2)\n\n# Compute the magnitude of the gradients using the L1 norm\n# magnitude = np.abs(I_x) + np.abs(I_y)\n\ndirection = np.arctan2(I_y, I_x)\n\nplt.figure(figsize=(9, 7))\n\nplt.subplot(2, 2, 1)\nplt.imshow(cv.convertScaleAbs(I_x), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in X direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 2)\nplt.imshow(cv.convertScaleAbs(I_y), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in Y direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 3)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Combined Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 4)\nplt.imshow(direction)\nplt.axis('off')\nplt.title('Gradient Direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Direction')\ncbar.set_ticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\ncbar.set_ticklabels(['-180°', '-90°', '0°', '90°', '180°'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then performs non-maximum suppression. The idea is to thin out the edges. We want to keep only the local maxima in the gradient direction. The algorithm goes through all the points on the gradient magnitude image and keeps only the points which are local maxima in the direction of the gradient. The pixel are checked in the direction of the gradient. If the pixel is not the maximum, it is set to zero.\nLet \\(M(x, y)\\) be defined as:\n\\[\n\\\nM(x, y) =\n\\begin{cases}\n0, & \\text{if } M(x, y) &lt; M(x + \\Delta x, y + \\Delta y) \\text{ or } M(x, y) &lt; M(x - \\Delta x, y - \\Delta y), \\\\\nM(x, y), & \\text{otherwise.}\n\\end{cases}\n\\\n\\]\nwhere \\(M(x,y)\\) is the gradient magnitude image and \\(\\Delta x\\) and \\(\\Delta y\\) are the unit vectors in the direction of the gradient.\n\n\n\n\nEdge Direction\n\n\nThe image on the left shows the gradient direction. The image on the right shows the process of non-maximum suppression. The point B (marked in Red) is the local maxima in the direction of the gradient while the points A and C (marked in black) are not. The points A and C are set to zero and the point B is kept. This operation will result in thinning of the edges.\n\nnms_image = np.zeros_like(magnitude)\n\n# Define the window size\nfor i in range(1, magnitude.shape[0] - 1):\n    for j in range(1, magnitude.shape[1] - 1):\n        # Define the angle interval\n        angle = direction[i, j] if direction[i, j] &gt;= 0 else direction[i, j] + np.pi\n        angle = np.rad2deg(angle)\n        angle = angle % 180\n\n        # Perform Non-Maximum Suppression\n        if (0 &lt;= angle &lt; 22.5) or (157.5 &lt;= angle &lt;= 180):\n            prev = magnitude[i, j - 1]\n            nxt = magnitude[i, j + 1]\n        elif 22.5 &lt;= angle &lt; 67.5:\n            prev = magnitude[i + 1, j - 1]\n            nxt = magnitude[i - 1, j + 1]\n        elif 67.5 &lt;= angle &lt; 112.5:\n            prev = magnitude[i - 1, j]\n            nxt = magnitude[i + 1, j]\n        else:\n            prev = magnitude[i - 1, j - 1]\n            nxt = magnitude[i + 1, j + 1]\n\n        if magnitude[i, j] &gt;= prev and magnitude[i, j] &gt;= nxt:\n            nms_image[i, j] = magnitude[i, j]\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(1, 2, 2)\nplt.imshow(nms_image, cmap='gray')\nplt.axis('off')\nplt.title('Non-Maximum Suppression')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNext step is to apply hysteresis thresholding or also known as double thresholding. The algorithm uses two thresholds, a high threshold and a low threshold. All the points above the high threshold are considered to be strong edges and all the points below the low threshold are considered to be non-edges. The points between the two thresholds are considered to be weak edges. The algorithm then tracks the weak edges and checks if they are connected to strong edges. If they are connected to strong edges, they are considered to be edges. If they are not connected to strong edges, they are considered to be non-edges. This is done to remove the weak edges which are not connected to any of the strong edges.\nLet:\n\n\\(S\\) denote the set of strong edges\n\\(W\\) denote the set of weak edge\n\\(E(x, y)\\) represent whether a pixel at \\((x, y)\\) is classified as an edge \\(1\\) or not \\(0\\).\n\nThe classification can be expressed as:\n\\[\nE(x, y) =\n\\begin{cases}\n1, & \\text{if } (x, y) \\in S, \\text{ or } (x, y) \\in W \\text{ and connected to } S, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nConnected to \\(S\\) implies there exists a path from \\((x, y)\\) in \\(W\\) to some \\((x', y')\\) in \\(S\\) that satisfies the connectivity criteria (e.g., adjacency).\n\nEdges = np.zeros_like(nms_image)\n\nthreshold_high = 100\nthreshold_low = 50\nvicinity = 2\n\n# Define the weak and strong edges\nstrong_edges = nms_image &gt; threshold_high\nweak_edges = (nms_image &gt;= threshold_low) & (nms_image &lt;= threshold_high)\n\n\n# Define the edge map\nEdges[strong_edges] = 255\n\n# Define the weak edges that are connected to strong edges\nfor i in range(1, nms_image.shape[0] - 1):\n    for j in range(1, nms_image.shape[1] - 1):\n        if weak_edges[i, j]:\n            if np.sum(strong_edges[i-vicinity:i+vicinity+1, j-vicinity:j+vicinity+1]) &gt; 0:\n                Edges[i, j] = 255\n\n# Dilate the edges\nEdges = cv.dilate(Edges, np.ones((3, 3), np.uint8), iterations=1)\n\n\n# Display the Original Image and the Detected Edges\nplt.figure(figsize=(7, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(Edges, cmap='gray')\nplt.axis('off')\nplt.title('Detected Edges')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Final output is the edge image where the edges are detected. The edges are detected using the gradient information. The algorithm is very effective in detecting edges and is widely used in computer vision applications."
  },
  {
    "objectID": "posts/2024/Canny_Edge.html#algorithm",
    "href": "posts/2024/Canny_Edge.html#algorithm",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The algorithm uses a Gaussian filter to smooth the image and reduce noise. This is done to prevent the algorithm from detecting false edges due to noise. It also smoothens the image to reduce the abrupt changes in pixel values. These abrupt changes may end up being detected as edges.\n\\[I'(x,y) = G(x,y) * I(x,y)\\]\nwhere \\(I(x,y)\\) is the input image, \\(G(x,y)\\) is the Gaussian filter and \\(I'(x,y)\\) is the smoothed image. ’*’ represents convolution operation between the image and the filter. Reading the image and convert it to grayscale. Image is converted to grayscale because the edge detection is done on the intensity values of the image. The color information is not required for edge detection.\n\n# Importing libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Read the image\nimage = cv.imread('Flower.jpeg')\n\nplt.figure(figsize=(12, 4))\n# Convert the image to RGB (OpenCV loads it into BGR by default)\nplt.subplot(1, 3, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\n# Convert the image to grayscale\ngray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nplt.subplot(1, 3, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.axis('off')\nplt.title('Grayscaled Image')\n\n# Apply Gaussian Blur\nblurred_image = cv.GaussianBlur(gray_image, (3, 3), 0)\nplt.subplot(1, 3, 3)\nplt.imshow(blurred_image, cmap='gray')\nplt.axis('off')\nplt.title('Gaussian Blurred Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then calculates the gradient of the image. The gradient is calculated using the Sobel operator. The Sobel operator is a discrete differentiation operator. It computes an approximation of gradient of the image. The operator uses two 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives - one for horizontal changes, and one for vertical. \\(G_x\\) and \\(G_y\\) are the horizontal and vertical Sobel operators respectively.\n\\[G_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\] \\[G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\]\n\\[I_y = I' * G_y\\] \\[I_x = I' * G_x\\]\nwhere \\(I_x\\) and \\(I_y\\) are the horizontal and vertical gradients respectively.\nNow the combined gradient magnitude could be calculated using two methods. * Using L1 Norm:\n\\[Magnitude = |I_x| + |I_y|\\]\n\nUsing L2 Norm:\n\n\\[Magnitude =  \\sqrt{I_x^2 + I_y^2}\\]\n\\[Direction = \\arctan(\\frac{I_y}{I_x})\\]\nMagnitude signifies the strength of the edge. The direction is the angle of the edge. The direction is rounded to one of four angles representing vertical, horizontal and two diagonals. The angles are 0, 45, 90 and 135 degrees. We are assuming the edges are in one of these four directions.\n\n# Compute the gradients by applying Sobel operator\nI_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=3)\nI_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=3)\n\n# Compute the magnitude of the gradients\nmagnitude = np.sqrt(I_x**2 + I_y**2)\n\n# Compute the magnitude of the gradients using the L1 norm\n# magnitude = np.abs(I_x) + np.abs(I_y)\n\ndirection = np.arctan2(I_y, I_x)\n\nplt.figure(figsize=(9, 7))\n\nplt.subplot(2, 2, 1)\nplt.imshow(cv.convertScaleAbs(I_x), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in X direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 2)\nplt.imshow(cv.convertScaleAbs(I_y), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in Y direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 3)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Combined Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 4)\nplt.imshow(direction)\nplt.axis('off')\nplt.title('Gradient Direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Direction')\ncbar.set_ticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\ncbar.set_ticklabels(['-180°', '-90°', '0°', '90°', '180°'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then performs non-maximum suppression. The idea is to thin out the edges. We want to keep only the local maxima in the gradient direction. The algorithm goes through all the points on the gradient magnitude image and keeps only the points which are local maxima in the direction of the gradient. The pixel are checked in the direction of the gradient. If the pixel is not the maximum, it is set to zero.\nLet \\(M(x, y)\\) be defined as:\n\\[\n\\\nM(x, y) =\n\\begin{cases}\n0, & \\text{if } M(x, y) &lt; M(x + \\Delta x, y + \\Delta y) \\text{ or } M(x, y) &lt; M(x - \\Delta x, y - \\Delta y), \\\\\nM(x, y), & \\text{otherwise.}\n\\end{cases}\n\\\n\\]\nwhere \\(M(x,y)\\) is the gradient magnitude image and \\(\\Delta x\\) and \\(\\Delta y\\) are the unit vectors in the direction of the gradient.\n\n\n\n\nEdge Direction\n\n\nThe image on the left shows the gradient direction. The image on the right shows the process of non-maximum suppression. The point B (marked in Red) is the local maxima in the direction of the gradient while the points A and C (marked in black) are not. The points A and C are set to zero and the point B is kept. This operation will result in thinning of the edges.\n\nnms_image = np.zeros_like(magnitude)\n\n# Define the window size\nfor i in range(1, magnitude.shape[0] - 1):\n    for j in range(1, magnitude.shape[1] - 1):\n        # Define the angle interval\n        angle = direction[i, j] if direction[i, j] &gt;= 0 else direction[i, j] + np.pi\n        angle = np.rad2deg(angle)\n        angle = angle % 180\n\n        # Perform Non-Maximum Suppression\n        if (0 &lt;= angle &lt; 22.5) or (157.5 &lt;= angle &lt;= 180):\n            prev = magnitude[i, j - 1]\n            nxt = magnitude[i, j + 1]\n        elif 22.5 &lt;= angle &lt; 67.5:\n            prev = magnitude[i + 1, j - 1]\n            nxt = magnitude[i - 1, j + 1]\n        elif 67.5 &lt;= angle &lt; 112.5:\n            prev = magnitude[i - 1, j]\n            nxt = magnitude[i + 1, j]\n        else:\n            prev = magnitude[i - 1, j - 1]\n            nxt = magnitude[i + 1, j + 1]\n\n        if magnitude[i, j] &gt;= prev and magnitude[i, j] &gt;= nxt:\n            nms_image[i, j] = magnitude[i, j]\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(1, 2, 2)\nplt.imshow(nms_image, cmap='gray')\nplt.axis('off')\nplt.title('Non-Maximum Suppression')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNext step is to apply hysteresis thresholding or also known as double thresholding. The algorithm uses two thresholds, a high threshold and a low threshold. All the points above the high threshold are considered to be strong edges and all the points below the low threshold are considered to be non-edges. The points between the two thresholds are considered to be weak edges. The algorithm then tracks the weak edges and checks if they are connected to strong edges. If they are connected to strong edges, they are considered to be edges. If they are not connected to strong edges, they are considered to be non-edges. This is done to remove the weak edges which are not connected to any of the strong edges.\nLet:\n\n\\(S\\) denote the set of strong edges\n\\(W\\) denote the set of weak edge\n\\(E(x, y)\\) represent whether a pixel at \\((x, y)\\) is classified as an edge \\(1\\) or not \\(0\\).\n\nThe classification can be expressed as:\n\\[\nE(x, y) =\n\\begin{cases}\n1, & \\text{if } (x, y) \\in S, \\text{ or } (x, y) \\in W \\text{ and connected to } S, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nConnected to \\(S\\) implies there exists a path from \\((x, y)\\) in \\(W\\) to some \\((x', y')\\) in \\(S\\) that satisfies the connectivity criteria (e.g., adjacency).\n\nEdges = np.zeros_like(nms_image)\n\nthreshold_high = 100\nthreshold_low = 50\nvicinity = 2\n\n# Define the weak and strong edges\nstrong_edges = nms_image &gt; threshold_high\nweak_edges = (nms_image &gt;= threshold_low) & (nms_image &lt;= threshold_high)\n\n\n# Define the edge map\nEdges[strong_edges] = 255\n\n# Define the weak edges that are connected to strong edges\nfor i in range(1, nms_image.shape[0] - 1):\n    for j in range(1, nms_image.shape[1] - 1):\n        if weak_edges[i, j]:\n            if np.sum(strong_edges[i-vicinity:i+vicinity+1, j-vicinity:j+vicinity+1]) &gt; 0:\n                Edges[i, j] = 255\n\n# Dilate the edges\nEdges = cv.dilate(Edges, np.ones((3, 3), np.uint8), iterations=1)\n\n\n# Display the Original Image and the Detected Edges\nplt.figure(figsize=(7, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(Edges, cmap='gray')\nplt.axis('off')\nplt.title('Detected Edges')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/Canny_Edge.html#final-output",
    "href": "posts/2024/Canny_Edge.html#final-output",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The Final output is the edge image where the edges are detected. The edges are detected using the gradient information. The algorithm is very effective in detecting edges and is widely used in computer vision applications."
  },
  {
    "objectID": "posts/2024/Neural_Process.html",
    "href": "posts/2024/Neural_Process.html",
    "title": "Neural Processes",
    "section": "",
    "text": "This notebook demonstrates the implementation of the Neural Processes. The model is trained to predict the distribution of the target function given a few examples of the function. In this notebook we will attempt to reconstruct an entire image given a few pixels of the image. we will be using the MNIST dataset for this task. The few pixels of the image are the context points and the entire image is the target function.\n\\[Model(\\{x_c, y_c\\}_{c=1}^n, \\{x_t\\}_{t=1}^m) \\rightarrow \\{y_t\\}_{t=1}^m\\]\n\\(\\{x_c, y_c\\}_{c=1}^n\\) are the context points where \\(x_i\\) are the pixel coordinates and \\(y_i\\) are the pixel intensity values.\n\\(\\{x_t\\}_{t=1}^m\\) are the target pixel coordinates where we want to predict the pixel intensity values \\(y_j\\).\nSo for our case, the pixel locations \\(x = (x^1, x^2)\\) and the pixel intensity values \\(y = Img(x^1, x^2)\\) or just \\(I\\) denoting intensity for simplicity.\n\\[Model(\\{(x^1_c, x^2_c, I_c)\\}_{c=1}^n, \\{(x^1_t, x^2_t)\\}_{t=1}^m) \\rightarrow \\{I_t\\}_{t=1}^m\\]\nThe model is composed of two neural networks: the encoder and the decoder. The encoder takes pixel coordinates and pixel intensitiy value of that coordinate as input and encodes them into a fixed-size representation. The decoder takes the representation and the pixel coordinates and predicts the pixel intensity value of the given coordinate.\n\nTraining the model\nDuring model training we will sample a random number of pixel locations from the image and use them as context points. The model is trained to predict the pixel intensity value of the sampled pixel locations. The model is trained using Mean Squared Error loss.\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\nfrom torchsummary import summary\n\n\nclass Neural_Procss_Model(nn.Module):\n    \"\"\"\n    Neural Process Model has two parts: encoder and decoder\n\n    Encoder takes in the context pairs (X1_c, X2_c, PixelIntensity) and encode them into a latent representation\n\n    Decoder takes in the latent representation and the target pairs (X1_t, X2_t) and output the predicted target pairs\n    \"\"\"\n    def __init__(self):\n        super(Neural_Procss_Model, self).__init__()\n\n        # Encoder takes in the context pairs (X1c,X2c,PixelIntensity) and encode them into a latent representation\n        self.encoder = nn.Sequential(\n            nn.Linear(3,64),\n            nn.ReLU(),\n            nn.Linear(64,128),\n            nn.ReLU(),\n            nn.Linear(128,256),\n        )\n\n        # Decoder takes in the latent representation and the target pairs and output the predicted target pairs\n        self.decoder = nn.Sequential(\n            nn.Linear(256+2,128),\n            nn.ReLU(),\n            nn.Linear(128,64),\n            nn.ReLU(),\n            nn.Linear(64,32),\n            nn.ReLU(),\n            nn.Linear(32,1)\n        )\n\n    def forward(self, context_pairs, target_pairs):\n        \"\"\"\n        Forward pass of the Neural Process Model. \n        \n        It takes in the context cordinate pairs (X1_c, X2_c, PixelIntensity) and target cordinate pairs (X1_t, X2_t, PixelIntensity) and output the predicted target pixel intensity for the target cordinate pairs\n\n        Parameters:\n        context_pairs: torch.Tensor of shape (batch_size, num_context_pairs, 3)\n        target_pairs: torch.Tensor of shape (batch_size, num_target_pairs, 2)\n\n        Returns:\n        predicted_target_Pixel Intensity: torch.Tensor of shape (batch_size, num_target_pairs, 1)\n        \"\"\"\n        \n        # Encode the context pairs into a latent representation\n        latent_representation = self.encoder(context_pairs)\n\n        # Average the latent representation\n        latent_representation = torch.mean(latent_representation,dim=1)\n\n        # Repeat the latent representation for each target pair\n        latent_representation = latent_representation.unsqueeze(1).repeat(1,target_pairs.size(1),1)\n\n        # Concatenate the latent representation with the target pixel locations\n        target_pixel_locations = target_pairs[:,:,:2]\n        target = torch.cat([latent_representation,target_pixel_locations],dim=-1)\n\n        # Decode the target pairs to obtain the predicted target pixel intensity\n        predicted_target_pixel_intensity = self.decoder(target)\n\n        return predicted_target_pixel_intensity\n    \n\n    def train(self, train_dataloader, num_epochs=100, optim = torch.optim.Adam, lr=3e-4, criterion = nn.MSELoss(),verbose=True,device='cuda'):\n        \"\"\"\n        Train the Neural Process Model\n\n        Parameters:\n        train_dataloader: DataLoader object for the training data\n        num_epochs: int, number of epochs to train the model\n        optimer: str, optimer to use for training the model\n        lr: float, learning rate for the optimer\n        criterion: loss function to use for training the model\n        \"\"\"\n\n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the optimizer\n        optimizer = optim(self.parameters(), lr=lr)\n\n        for epoch in range(num_epochs):\n            for i, (context_pairs, target_pairs) in enumerate(train_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # print(context_pairs.shape, target_pairs.shape)\n\n                # Zero the gradients\n                optimizer.zero_grad()\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                # Backward pass\n                loss.backward()\n\n                # Update the weights\n                optimizer.step()\n\n            if verbose:\n                print(\"Epoch: {}/{} Loss: {:.5f}\".format(epoch+1,num_epochs,loss.item()))\n\n\n\n    def test(self, test_dataloader, criterion = nn.MSELoss(),device='cuda'):\n        \"\"\"\n        Test the Neural Process Model\n\n        Parameters:\n        test_dataloader: DataLoader object for the test data\n        \"\"\"\n    \n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the loss\n        test_loss = 0\n\n        with torch.no_grad():\n            for i, (context_pairs, target_pairs) in enumerate(test_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                test_loss += loss.item()\n\n        print(f'Test Loss: {test_loss/len(test_dataloader)}')\n\n        \n\n\n\nGenerating Trainset for the Problem\n\ntrain_data = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n\nno_context_points = 200\nAll_corrdinates = np.array([(i,j) for i in range(28) for j in range(28)])\n\ntrain_set = []\nfor i in range(len(train_data)):\n    image, _ = train_data[i]\n    image = image.squeeze().numpy()\n\n    train_idx = np.random.choice(range(len(All_corrdinates)), no_context_points, replace=False)\n\n    context_points = All_corrdinates[train_idx]\n    context_pixels = image[context_points[:,0], context_points[:,1]]\n\n    context_set = np.concatenate([context_points, context_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n    target_set = context_set.copy()\n\n    train_set.append((context_set, target_set))\n\n\ntrain_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in train_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\ntorch.Size([32, 200, 3]) torch.Size([32, 200, 3])\n\n\n\n\nGenerating testset for the problem\n\ntest_data = MNIST(root='./data', train=False, download=True, transform= ToTensor())\n\nno_context_points = 200\ntest_set = []\nfor i in range(len(test_data)):\n    image, _ = test_data[i]\n    image = image.squeeze().numpy()\n\n    test_idx = np.random.choice(range(len(All_corrdinates)), no_context_points, replace=False)\n\n    context_points = All_corrdinates[test_idx]\n    context_pixels = image[context_points[:,0], context_points[:,1]]\n\n    context_set = np.concatenate([context_points, context_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n    target_points = np.array([(i,j) for i in range(28) for j in range(28)])\n    target_pixels = image[target_points[:,0], target_points[:,1]]\n\n    target_set = np.concatenate([target_points, target_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n    test_set.append((context_set, target_set))\n\ntest_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in test_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])\n\n\n\n\nTraining the model\n\nmodel = Neural_Procss_Model()\n\nmodel.to('cuda')\n\nmodel.train(train_dataloader, num_epochs=30)\n\nEpoch: 1/30 Loss: 0.06666\nEpoch: 2/30 Loss: 0.06497\nEpoch: 3/30 Loss: 0.06212\nEpoch: 4/30 Loss: 0.06163\nEpoch: 5/30 Loss: 0.05931\nEpoch: 6/30 Loss: 0.05229\nEpoch: 7/30 Loss: 0.04560\nEpoch: 8/30 Loss: 0.04370\nEpoch: 9/30 Loss: 0.04431\nEpoch: 10/30 Loss: 0.04166\nEpoch: 11/30 Loss: 0.04205\nEpoch: 12/30 Loss: 0.03904\nEpoch: 13/30 Loss: 0.03821\nEpoch: 14/30 Loss: 0.04091\nEpoch: 15/30 Loss: 0.03898\nEpoch: 16/30 Loss: 0.03534\nEpoch: 17/30 Loss: 0.04038\nEpoch: 18/30 Loss: 0.03608\nEpoch: 19/30 Loss: 0.03051\nEpoch: 20/30 Loss: 0.03762\nEpoch: 21/30 Loss: 0.03269\nEpoch: 22/30 Loss: 0.02833\nEpoch: 23/30 Loss: 0.02647\nEpoch: 24/30 Loss: 0.03339\nEpoch: 25/30 Loss: 0.02954\nEpoch: 26/30 Loss: 0.03007\nEpoch: 27/30 Loss: 0.02750\nEpoch: 28/30 Loss: 0.02566\nEpoch: 29/30 Loss: 0.02912\nEpoch: 30/30 Loss: 0.03134\n\n\n\nmodel.test(test_dataloader)\n\nTest Loss: 0.034481238389310366\n\n\n\nfor context_pairs, target_pairs in test_dataloader:\n    context_pairs = context_pairs.to('cuda')\n    target_pairs = target_pairs.to('cuda')\n\n    predicted_target_pixel_intensity = model(context_pairs, target_pairs)\n\n    for i in range(5):\n        predicted_pixel = predicted_target_pixel_intensity[i].detach().cpu().numpy()\n        predicted_image = predicted_pixel.reshape(28,28)\n\n        actual_pixel = target_pairs[i][:,2].detach().cpu().numpy()\n        actual_image = actual_pixel.reshape(28,28)\n\n        context_image = np.zeros((28,28))\n        context_pixel_locations = context_pairs[i][:,:2].detach().cpu().numpy().astype(int) \n        context_image[context_pixel_locations[:,0], context_pixel_locations[:,1]] = context_pairs[i][:,2].detach().cpu().numpy()\n\n        plt.figure(figsize=(9,4))\n\n        plt.subplot(1,3,1)\n        plt.imshow(context_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Context Image')\n        \n        plt.subplot(1,3,2)\n        plt.imshow(predicted_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Predicted Image')\n\n        plt.subplot(1,3,3)\n        plt.imshow(actual_image, cmap='gray')\n        plt.axis('off')\n        plt.title('Actual Image')\n        \n        plt.tight_layout()\n        plt.show()\n\n    break"
  }
]