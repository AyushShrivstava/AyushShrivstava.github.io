[
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Gallery",
    "section": "",
    "text": "Welcome to my gallery! Here you’ll find a collection of images from my life, including memorable moments and photos I’ve taken. I’ll continue to add more images as I capture new experiences and milestones along my journey.\n\n\n\n\n\n\nTaking a session Object Detection on Day-1 of Summer School.\n\n\n\n\n\nTaking a session on Day-5 on “Seeing Red”. Estimating Heart rate from Smartphone Camera\n\n\n\n\n\n\n\n\n\n\nExplaining our project at the innovation expo I-Inventive\n\n\n\n\n\nPosing with Dr. Rishiraj (left) infront of the Stall, IIT Madras\n\n\n\n\n\nExplaining Apnea project (Right)\n\n\n\n\n\nDeer from IIT Madras\n\n\n\n\n\n\n\n\n\n\nAttending COMSNETS 2025\n\n\n\n\n\nPresenting “Towards Sleep Apnea Screening via Thermal Imagery”\n\n\n\n\n\nInside IIIT Delhi Campus\n\n\n\n\n\n\n\n\n\n\n\nAttending ACM Compass with the Lab\n\n\n\n\n\nACM Compass - infront of conference hall\n\n\n\n\n\nInside IIIT Delhi Campus\n\n\n\n\n\n\n\n\n\n\nAwarded the MTech in Computer Science and Engineering at IITGN convocation June 2024\n\n\n\n\n\nOutside IITGN Convocation Hall June 2024\n\n\n\n\n\nOutside IITGN Convocation Hall with my family\n\n\n\n\n\n\n\n\n\n\nSuraj and I celebrating the successful defense of our MTech thesis\n\n\n\n\n\nCelebrating the successful defense of our MTech thesis"
  },
  {
    "objectID": "gallery.html#i-inventive-2025-iit-madras",
    "href": "gallery.html#i-inventive-2025-iit-madras",
    "title": "Gallery",
    "section": "",
    "text": "Explaining our project at the innovation expo I-Inventive\n\n\n\n\n\nPosing with Dr. Rishiraj (left) infront of the Stall, IIT Madras\n\n\n\n\n\nExplaining Apnea project (Right)\n\n\n\n\n\nDeer from IIT Madras"
  },
  {
    "objectID": "gallery.html#comsnets-2025bangalore",
    "href": "gallery.html#comsnets-2025bangalore",
    "title": "Gallery",
    "section": "",
    "text": "Attending COMSNETS 2025\n\n\n\n\n\nPresenting “Towards Sleep Apnea Screening via Thermal Imagery”\n\n\n\n\n\nInside IIIT Delhi Campus"
  },
  {
    "objectID": "gallery.html#acm-compass-2024-iiit-delhi",
    "href": "gallery.html#acm-compass-2024-iiit-delhi",
    "title": "Gallery",
    "section": "",
    "text": "Attending ACM Compass with the Lab\n\n\n\n\n\nACM Compass - infront of conference hall\n\n\n\n\n\nInside IIIT Delhi Campus"
  },
  {
    "objectID": "gallery.html#mtech-convocation-2024",
    "href": "gallery.html#mtech-convocation-2024",
    "title": "Gallery",
    "section": "",
    "text": "Awarded the MTech in Computer Science and Engineering at IITGN convocation June 2024\n\n\n\n\n\nOutside IITGN Convocation Hall June 2024\n\n\n\n\n\nOutside IITGN Convocation Hall with my family"
  },
  {
    "objectID": "gallery.html#mtech-thesis-defense-2024",
    "href": "gallery.html#mtech-thesis-defense-2024",
    "title": "Gallery",
    "section": "",
    "text": "Suraj and I celebrating the successful defense of our MTech thesis\n\n\n\n\n\nCelebrating the successful defense of our MTech thesis"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "NumPy Tutorial\n\n\n\nPython\n\n\nNumPy\n\n\nData Science\n\n\nTutorial\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJan 15, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting Up Your Environment\n\n\n\nPython\n\n\nAnaconda\n\n\nSetup\n\n\nVirtual Environments\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJan 8, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanny Edge Detector\n\n\n\nComputer Vision\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nDec 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPower of Sine Activation\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nNov 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLucas Kanade Optical Flow\n\n\n\nComputer Vision\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nOct 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Conda Environment for ML Projects\n\n\n\nHelpful Tips\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJun 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Processes\n\n\n\nDeep Learning\n\n\n\n\n\n\n\nAyush Shrivastava\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publication",
    "section": "",
    "text": "Under review in Nature BioSensing. Will be added soon.\nUnder review in KDD, 2026. Will be added soon."
  },
  {
    "objectID": "posts/2024/SirenDemo.html",
    "href": "posts/2024/SirenDemo.html",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "This notebook is a simple demonstration of the power of the sine activation function in neural networks. We will use a nueral network to learn a function mapping that goes from pixel locations to pixel values.\nIn nut shell we wish to map\n\\[f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\]\n\\[f(x_1, x_2) = R, G, B\\]\n\\[\\text{where } R, G, B \\in [0, 1] \\text{are the pixel intensities for red, green and blue channels respectively}\\] \\[\\text{and } x_1, x_2 \\in [0, 1] \\text{are the pixel locations}\\]\nWe will first use a ReLU activation function and then use a sine activation function to see the difference in the results. This demonstration is a simpler usecase of the paper SIREN: Implicit Neural Representations with Periodic Activation Functions by Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein. Available to read at https://arxiv.org/abs/2006.09661. Here is the blog post by the authors https://vsitzmann.github.io/siren/\nNOTE : This notebook does not implement the SIREN architecture. It is a simple demonstration of the power of sine activation function in neural networks.\n\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport numpy as np\nimport torch.nn as nn\nimport torch\nimport warnings\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Device in use \", device)\n\nDevice in use  cuda\n\n\n\n\nSay Hello to my CaT!\n\n# Read the Image\nimage = plt.imread('Chhotu.jpeg')\n\n# Create a copy of the image\nimage_rec = image.copy()\n\n# Draw a rectangle on the image (w1,h1) and (w2,h2)\nimage_rec = cv.rectangle(image_rec, (800, 300), (1400, 900), (255, 255, 0), 5)\nimage_rec = cv.putText(image_rec, 'Chhotu', (810, 1000), cv.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 0), 5)\n\n# Display the image\nprint(image_rec.shape)\nplt.figure(figsize=(8, 8))\nplt.imshow(image_rec)\nplt.tight_layout()\nplt.show()\n\n(1200, 1600, 3)\n\n\n\n\n\n\n\n\n\nThis Image is too big for my GPU to handle. So I will resize it to a smaller size of 300 \\(\\times\\) 300 pixels.\n\n# Crop the image\ncrop_image = image[300:900, 800:1400]\n\n# Resize the image to make it smaller\ncrop_image = cv.resize(crop_image, (300, 300))\n\n# Normalize the image to be in the range [0, 1]\ncrop_image = crop_image/255.0\n\nprint(crop_image.shape)\nplt.imshow(crop_image)\nplt.tight_layout()\nplt.show()\n\n(300, 300, 3)\n\n\n\n\n\n\n\n\n\n\n\n\nWe basically wish to create a dataset to train our neural network. The input to the neural network will be the pixel location and the output will be the pixel value. We will create a coordinate map of the image. The coordinate map will have the shape (\\(Height \\times Width\\), 2) and Output will have the shape (\\(Height \\times Width\\), 3).\n\\[\\text{Input Coordinate Map to Neural Network} =  \\begin{bmatrix} x_1, y_1 \\\\ x_2, y_2 \\\\ \\vdots \\\\ x_n, y_n \\end{bmatrix} \\text{,where } x_i, y_i \\text{ are the pixel locations}\\]\n\\[\\text{Output Pixel Values} =  \\begin{bmatrix} R_1, G_1, B_1 \\\\ R_2, G_2, B_2 \\\\ \\vdots \\\\ R_n, G_n, B_n \\end{bmatrix} \\text{,where } R_i, G_i, B_i \\text{ are the pixel intensities}\\]\n\n# Create a 2d grid of the image\nheight, width, channels = crop_image.shape\n\nx_coords = range(height)\ny_coords = range(width)\n\nxv, yv = np.meshgrid(y_coords, x_coords)\nxv=xv.flatten()\nyv=yv.flatten()\n\nX = np.vstack((yv, xv)).T\nY = crop_image.reshape((height*width, 3))\n\n\nX.shape, Y.shape\n\n((90000, 2), (90000, 3))\n\n\nLet have a look at what exactly the coordinate map looks like.\n\nX\n\narray([[  0,   0],\n       [  0,   1],\n       [  0,   2],\n       ...,\n       [299, 297],\n       [299, 298],\n       [299, 299]])\n\n\n\n\n\nWe will train a simple neural network with ReLU activation function to learn the mapping from pixel locations to pixel values. We will use the Adam optimizer and Mean Squared Error loss function.\n\\[ Neural Network(x_i, y_i) = \\begin{bmatrix} R_i, G_i, B_i \\end{bmatrix}\\]\nAll the layers in the neural network will have ReLU activation function except the last layer which will have a linear activation function.\n\n# Create a MLP with 5 hidden layers.\n# Input is (x, y) and output is (r, g, b)\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 512)\n        self.fc5 = nn.Linear(512, 256)\n        self.fc6 = nn.Linear(256, 3)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.fc1(x))\n        x = nn.functional.relu(self.fc2(x))\n        x = nn.functional.relu(self.fc3(x))\n        x = nn.functional.relu(self.fc4(x))\n        x = nn.functional.relu(self.fc5(x))\n        return self.fc6(x)\n\n\n\n\n# Training loop function to train the model\n# X: (num_xy, 2) tensor of (x, y) coordinates\n# y: (num_xy, 3) tensor of (r, g, b) pixel values\n# model: MLP model\n# lr: learning rate\n# epochs: number of epochs to train for\n# bs: batch size\n# print_every: print loss every print_every epochs\n# Logs losses\n# Saves the prediction frmo model every print_every epochs\n\ndef train(X, y, model, lr=0.01, epochs=1000, bs=1000, print_every=100):\n    \"\"\"\n    Model Trainer : Trains the model\n\n    Parameters:\n    X : (num_xy, 2) tensor of (x, y) coordinates\n    y : (num_xy, 3) tensor of (r, g, b) pixel values\n    model : MLP model \n    lr : learning rate\n    epochs : number of epochs to train for\n    bs : batch size\n    print_every : print loss every print_every epochs\n\n    Returns:\n    losses : list of losses\n    imgs : list of images predicted by the model every print_every epochs\n    \"\"\"\n    losses = []\n    imgs = []\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n\n        # chosing some random indexes to train the model\n        idx = np.random.choice(X.shape[0], bs)\n        X_batch = torch.tensor(X[idx], dtype=torch.float32).to(device)\n        y_batch = torch.tensor(y[idx], dtype=torch.float32).to(device)\n        y_pred = model(X_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        if epoch % print_every == 0:\n            print(f'Epoch {epoch}, Loss: {loss.item()}')\n            imgs.append(model(torch.tensor(X, dtype=torch.float32).to(device)).detach().cpu().numpy())\n\n    return losses, imgs\n\n\ndef plot_image(model, name=None):\n    \"\"\"\n    Plot the image predicted by the model\n    \"\"\"\n    # Predict the (r, g, b) values\n    pred_y = model(torch.tensor(X, dtype=torch.float32).to(device))\n\n    # Reshape the predictions to be (3, height, width)\n    pred_y = pred_y.transpose(0, 1).reshape(channels, height, width)\n\n    # plot the image\n    plt.imshow(pred_y.permute(1, 2, 0).detach().cpu())\n    if name:\n        plt.savefig(name)\n\n\nMLP_model = MLP()\nMLP_model.to(device)\n\nrelu_losses, relu_imgs = train(X,Y, \n                     MLP_model, lr=0.001, epochs=10000, bs=5000, print_every=1000)\n\nEpoch 0, Loss: 0.6576557755470276\nEpoch 1000, Loss: 0.0114674037322402\nEpoch 2000, Loss: 0.012357354164123535\nEpoch 3000, Loss: 0.008751491084694862\nEpoch 4000, Loss: 0.008101350627839565\nEpoch 5000, Loss: 0.007827701978385448\nEpoch 6000, Loss: 0.010159682482481003\nEpoch 7000, Loss: 0.008207915350794792\nEpoch 8000, Loss: 0.007177131250500679\nEpoch 9000, Loss: 0.006437341216951609\n\n\n\nplt.figure(figsize=(20, 8))\n\nfor i, img in enumerate(relu_imgs):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img.reshape(height, width, 3))\n    plt.axis('off')\n    plt.title(f'After Epoch {i*1000}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(crop_image)\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplot_image(MLP_model)\nplt.title('Reconstructed Image after training')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe will train a simple neural network with ReLU activation function to learn the mapping from pixel locations to pixel values. We will use the Adam optimizer and Mean Squared Error loss function.\n\\[ Neural Network(x_i, y_i) = \\begin{bmatrix} R_i, G_i, B_i \\end{bmatrix}\\]\nAll the layers in the neural network will have Sine activation function except the last layer which will have a linear activation function.\n\n# Create a MLP with 5 hidden layers with 256 neurons each and sine activations.\n# Input is (x, y) and output is (r, g, b)\n\nclass MLP_sin(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 512)\n        self.fc5 = nn.Linear(512, 256)\n        self.fc6 = nn.Linear(256, 3)\n\n    def forward(self, x):\n        x = torch.sin(self.fc1(x))\n        x = torch.sin(self.fc2(x))\n        x = torch.sin(self.fc3(x))\n        x = torch.sin(self.fc4(x))\n        x = torch.sin(self.fc5(x))\n        return self.fc6(x)\n\n\nsin_model = MLP_sin()\nsin_model.to(device)\n\nsin_losses, sin_imgs = train(X,Y,\n                     sin_model, lr=0.001, epochs=10000, bs=5000, print_every=1000)\n\nEpoch 0, Loss: 0.11556914448738098\nEpoch 1000, Loss: 0.0018598262686282396\nEpoch 2000, Loss: 0.001261448604054749\nEpoch 3000, Loss: 0.0007661026320420206\nEpoch 4000, Loss: 0.00033592403633520007\nEpoch 5000, Loss: 0.00014680986350867897\nEpoch 6000, Loss: 9.029130887938663e-05\nEpoch 7000, Loss: 8.067893941188231e-05\nEpoch 8000, Loss: 8.20108616608195e-05\nEpoch 9000, Loss: 7.174971688073128e-05\n\n\n\nplt.figure(figsize=(20, 9))\n\nfor i, img in enumerate(sin_imgs):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img.reshape(height, width, 3))\n    plt.axis('off')\n    plt.title(f'After Epoch {i*1000}', fontsize=14  )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(15, 6))\nplt.subplot(1, 3, 1)\nplt.imshow(crop_image)\nplt.title('Original Image', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplot_image(sin_model)\nplt.title('Reconstructed Image from Sine Activation', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplot_image(MLP_model)\nplt.title('Reconstructed Image from ReLU Activation', fontsize=14)\nplt.axis('off')\n\nplt.suptitle('Comparison of Image reconstruction from ReLU and Sine Activation', fontsize=18)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6),dpi=150)\nplt.plot(relu_losses, label='ReLU')\nplt.plot(sin_losses, label='Sine')\nplt.axhline(np.min(relu_losses), color='C0', linestyle='--', label='ReLU Min Loss')\nplt.axhline(np.min(sin_losses), color='C1', linestyle='--', label='Sine Min Loss')\nplt.annotate(f'Relu Min Loss: {np.min(relu_losses):.5f}', (len(relu_losses)-1, np.min(relu_losses)), textcoords=\"offset points\", xytext=(-25,-10), ha='center')\nplt.annotate(f'Sine Min Loss: {np.min(sin_losses):.5f}', (len(sin_losses)-1, np.min(sin_losses)), textcoords=\"offset points\", xytext=(-25,-10), ha='center')\nplt.xlabel('Epochs')\nplt.ylabel('MSE Loss or Reconstruction Error')\nplt.yscale('log')  # Set y-axis to log scale\nplt.legend()\nplt.title('Comparison of Losses between ReLU and Sine Activation (Log Scale)')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#lets-load-an-image",
    "href": "posts/2024/SirenDemo.html#lets-load-an-image",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "Say Hello to my CaT!\n\n# Read the Image\nimage = plt.imread('Chhotu.jpeg')\n\n# Create a copy of the image\nimage_rec = image.copy()\n\n# Draw a rectangle on the image (w1,h1) and (w2,h2)\nimage_rec = cv.rectangle(image_rec, (800, 300), (1400, 900), (255, 255, 0), 5)\nimage_rec = cv.putText(image_rec, 'Chhotu', (810, 1000), cv.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 0), 5)\n\n# Display the image\nprint(image_rec.shape)\nplt.figure(figsize=(8, 8))\nplt.imshow(image_rec)\nplt.tight_layout()\nplt.show()\n\n(1200, 1600, 3)\n\n\n\n\n\n\n\n\n\nThis Image is too big for my GPU to handle. So I will resize it to a smaller size of 300 \\(\\times\\) 300 pixels.\n\n# Crop the image\ncrop_image = image[300:900, 800:1400]\n\n# Resize the image to make it smaller\ncrop_image = cv.resize(crop_image, (300, 300))\n\n# Normalize the image to be in the range [0, 1]\ncrop_image = crop_image/255.0\n\nprint(crop_image.shape)\nplt.imshow(crop_image)\nplt.tight_layout()\nplt.show()\n\n(300, 300, 3)"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#creating-a-coordinate-map",
    "href": "posts/2024/SirenDemo.html#creating-a-coordinate-map",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "We basically wish to create a dataset to train our neural network. The input to the neural network will be the pixel location and the output will be the pixel value. We will create a coordinate map of the image. The coordinate map will have the shape (\\(Height \\times Width\\), 2) and Output will have the shape (\\(Height \\times Width\\), 3).\n\\[\\text{Input Coordinate Map to Neural Network} =  \\begin{bmatrix} x_1, y_1 \\\\ x_2, y_2 \\\\ \\vdots \\\\ x_n, y_n \\end{bmatrix} \\text{,where } x_i, y_i \\text{ are the pixel locations}\\]\n\\[\\text{Output Pixel Values} =  \\begin{bmatrix} R_1, G_1, B_1 \\\\ R_2, G_2, B_2 \\\\ \\vdots \\\\ R_n, G_n, B_n \\end{bmatrix} \\text{,where } R_i, G_i, B_i \\text{ are the pixel intensities}\\]\n\n# Create a 2d grid of the image\nheight, width, channels = crop_image.shape\n\nx_coords = range(height)\ny_coords = range(width)\n\nxv, yv = np.meshgrid(y_coords, x_coords)\nxv=xv.flatten()\nyv=yv.flatten()\n\nX = np.vstack((yv, xv)).T\nY = crop_image.reshape((height*width, 3))\n\n\nX.shape, Y.shape\n\n((90000, 2), (90000, 3))\n\n\nLet have a look at what exactly the coordinate map looks like.\n\nX\n\narray([[  0,   0],\n       [  0,   1],\n       [  0,   2],\n       ...,\n       [299, 297],\n       [299, 298],\n       [299, 299]])"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#training-a-simple-neural-network-with-relu-activation.",
    "href": "posts/2024/SirenDemo.html#training-a-simple-neural-network-with-relu-activation.",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "We will train a simple neural network with ReLU activation function to learn the mapping from pixel locations to pixel values. We will use the Adam optimizer and Mean Squared Error loss function.\n\\[ Neural Network(x_i, y_i) = \\begin{bmatrix} R_i, G_i, B_i \\end{bmatrix}\\]\nAll the layers in the neural network will have ReLU activation function except the last layer which will have a linear activation function.\n\n# Create a MLP with 5 hidden layers.\n# Input is (x, y) and output is (r, g, b)\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 512)\n        self.fc5 = nn.Linear(512, 256)\n        self.fc6 = nn.Linear(256, 3)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.fc1(x))\n        x = nn.functional.relu(self.fc2(x))\n        x = nn.functional.relu(self.fc3(x))\n        x = nn.functional.relu(self.fc4(x))\n        x = nn.functional.relu(self.fc5(x))\n        return self.fc6(x)\n\n\n\n\n# Training loop function to train the model\n# X: (num_xy, 2) tensor of (x, y) coordinates\n# y: (num_xy, 3) tensor of (r, g, b) pixel values\n# model: MLP model\n# lr: learning rate\n# epochs: number of epochs to train for\n# bs: batch size\n# print_every: print loss every print_every epochs\n# Logs losses\n# Saves the prediction frmo model every print_every epochs\n\ndef train(X, y, model, lr=0.01, epochs=1000, bs=1000, print_every=100):\n    \"\"\"\n    Model Trainer : Trains the model\n\n    Parameters:\n    X : (num_xy, 2) tensor of (x, y) coordinates\n    y : (num_xy, 3) tensor of (r, g, b) pixel values\n    model : MLP model \n    lr : learning rate\n    epochs : number of epochs to train for\n    bs : batch size\n    print_every : print loss every print_every epochs\n\n    Returns:\n    losses : list of losses\n    imgs : list of images predicted by the model every print_every epochs\n    \"\"\"\n    losses = []\n    imgs = []\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n\n        # chosing some random indexes to train the model\n        idx = np.random.choice(X.shape[0], bs)\n        X_batch = torch.tensor(X[idx], dtype=torch.float32).to(device)\n        y_batch = torch.tensor(y[idx], dtype=torch.float32).to(device)\n        y_pred = model(X_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        if epoch % print_every == 0:\n            print(f'Epoch {epoch}, Loss: {loss.item()}')\n            imgs.append(model(torch.tensor(X, dtype=torch.float32).to(device)).detach().cpu().numpy())\n\n    return losses, imgs\n\n\ndef plot_image(model, name=None):\n    \"\"\"\n    Plot the image predicted by the model\n    \"\"\"\n    # Predict the (r, g, b) values\n    pred_y = model(torch.tensor(X, dtype=torch.float32).to(device))\n\n    # Reshape the predictions to be (3, height, width)\n    pred_y = pred_y.transpose(0, 1).reshape(channels, height, width)\n\n    # plot the image\n    plt.imshow(pred_y.permute(1, 2, 0).detach().cpu())\n    if name:\n        plt.savefig(name)\n\n\nMLP_model = MLP()\nMLP_model.to(device)\n\nrelu_losses, relu_imgs = train(X,Y, \n                     MLP_model, lr=0.001, epochs=10000, bs=5000, print_every=1000)\n\nEpoch 0, Loss: 0.6576557755470276\nEpoch 1000, Loss: 0.0114674037322402\nEpoch 2000, Loss: 0.012357354164123535\nEpoch 3000, Loss: 0.008751491084694862\nEpoch 4000, Loss: 0.008101350627839565\nEpoch 5000, Loss: 0.007827701978385448\nEpoch 6000, Loss: 0.010159682482481003\nEpoch 7000, Loss: 0.008207915350794792\nEpoch 8000, Loss: 0.007177131250500679\nEpoch 9000, Loss: 0.006437341216951609\n\n\n\nplt.figure(figsize=(20, 8))\n\nfor i, img in enumerate(relu_imgs):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img.reshape(height, width, 3))\n    plt.axis('off')\n    plt.title(f'After Epoch {i*1000}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(crop_image)\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplot_image(MLP_model)\nplt.title('Reconstructed Image after training')\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#training-a-simple-neural-network-with-sine-activation.",
    "href": "posts/2024/SirenDemo.html#training-a-simple-neural-network-with-sine-activation.",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "We will train a simple neural network with ReLU activation function to learn the mapping from pixel locations to pixel values. We will use the Adam optimizer and Mean Squared Error loss function.\n\\[ Neural Network(x_i, y_i) = \\begin{bmatrix} R_i, G_i, B_i \\end{bmatrix}\\]\nAll the layers in the neural network will have Sine activation function except the last layer which will have a linear activation function.\n\n# Create a MLP with 5 hidden layers with 256 neurons each and sine activations.\n# Input is (x, y) and output is (r, g, b)\n\nclass MLP_sin(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 512)\n        self.fc5 = nn.Linear(512, 256)\n        self.fc6 = nn.Linear(256, 3)\n\n    def forward(self, x):\n        x = torch.sin(self.fc1(x))\n        x = torch.sin(self.fc2(x))\n        x = torch.sin(self.fc3(x))\n        x = torch.sin(self.fc4(x))\n        x = torch.sin(self.fc5(x))\n        return self.fc6(x)\n\n\nsin_model = MLP_sin()\nsin_model.to(device)\n\nsin_losses, sin_imgs = train(X,Y,\n                     sin_model, lr=0.001, epochs=10000, bs=5000, print_every=1000)\n\nEpoch 0, Loss: 0.11556914448738098\nEpoch 1000, Loss: 0.0018598262686282396\nEpoch 2000, Loss: 0.001261448604054749\nEpoch 3000, Loss: 0.0007661026320420206\nEpoch 4000, Loss: 0.00033592403633520007\nEpoch 5000, Loss: 0.00014680986350867897\nEpoch 6000, Loss: 9.029130887938663e-05\nEpoch 7000, Loss: 8.067893941188231e-05\nEpoch 8000, Loss: 8.20108616608195e-05\nEpoch 9000, Loss: 7.174971688073128e-05\n\n\n\nplt.figure(figsize=(20, 9))\n\nfor i, img in enumerate(sin_imgs):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img.reshape(height, width, 3))\n    plt.axis('off')\n    plt.title(f'After Epoch {i*1000}', fontsize=14  )\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/SirenDemo.html#side-by-side-comparison-of-relu-and-sine-activation",
    "href": "posts/2024/SirenDemo.html#side-by-side-comparison-of-relu-and-sine-activation",
    "title": "Power of Sine Activation",
    "section": "",
    "text": "plt.figure(figsize=(15, 6))\nplt.subplot(1, 3, 1)\nplt.imshow(crop_image)\nplt.title('Original Image', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplot_image(sin_model)\nplt.title('Reconstructed Image from Sine Activation', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplot_image(MLP_model)\nplt.title('Reconstructed Image from ReLU Activation', fontsize=14)\nplt.axis('off')\n\nplt.suptitle('Comparison of Image reconstruction from ReLU and Sine Activation', fontsize=18)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6),dpi=150)\nplt.plot(relu_losses, label='ReLU')\nplt.plot(sin_losses, label='Sine')\nplt.axhline(np.min(relu_losses), color='C0', linestyle='--', label='ReLU Min Loss')\nplt.axhline(np.min(sin_losses), color='C1', linestyle='--', label='Sine Min Loss')\nplt.annotate(f'Relu Min Loss: {np.min(relu_losses):.5f}', (len(relu_losses)-1, np.min(relu_losses)), textcoords=\"offset points\", xytext=(-25,-10), ha='center')\nplt.annotate(f'Sine Min Loss: {np.min(sin_losses):.5f}', (len(sin_losses)-1, np.min(sin_losses)), textcoords=\"offset points\", xytext=(-25,-10), ha='center')\nplt.xlabel('Epochs')\nplt.ylabel('MSE Loss or Reconstruction Error')\nplt.yscale('log')  # Set y-axis to log scale\nplt.legend()\nplt.title('Comparison of Losses between ReLU and Sine Activation (Log Scale)')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/Canny_Edge.html",
    "href": "posts/2024/Canny_Edge.html",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The Canny Edge detector is named after its inventor, John F. Canny. It is among the most popular edge detection algorithms due to its simplicity and effectiveness. The algorithm uses gradient information to detect edges. It follow the following steps:\n\n\n\n\nThe algorithm uses a Gaussian filter to smooth the image and reduce noise. This is done to prevent the algorithm from detecting false edges due to noise. It also smoothens the image to reduce the abrupt changes in pixel values. These abrupt changes may end up being detected as edges.\n\\[I'(x,y) = G(x,y) * I(x,y)\\]\nwhere \\(I(x,y)\\) is the input image, \\(G(x,y)\\) is the Gaussian filter and \\(I'(x,y)\\) is the smoothed image. ’*’ represents convolution operation between the image and the filter. Reading the image and convert it to grayscale. Image is converted to grayscale because the edge detection is done on the intensity values of the image. The color information is not required for edge detection.\n\n# Importing libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Read the image\nimage = cv.imread('Flower.jpeg')\n\nplt.figure(figsize=(12, 4))\n# Convert the image to RGB (OpenCV loads it into BGR by default)\nplt.subplot(1, 3, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\n# Convert the image to grayscale\ngray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nplt.subplot(1, 3, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.axis('off')\nplt.title('Grayscaled Image')\n\n# Apply Gaussian Blur\nblurred_image = cv.GaussianBlur(gray_image, (3, 3), 0)\nplt.subplot(1, 3, 3)\nplt.imshow(blurred_image, cmap='gray')\nplt.axis('off')\nplt.title('Gaussian Blurred Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then calculates the gradient of the image. The gradient is calculated using the Sobel operator. The Sobel operator is a discrete differentiation operator. It computes an approximation of gradient of the image. The operator uses two 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives - one for horizontal changes, and one for vertical. \\(G_x\\) and \\(G_y\\) are the horizontal and vertical Sobel operators respectively.\n\\[G_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\] \\[G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\]\n\\[I_y = I' * G_y\\] \\[I_x = I' * G_x\\]\nwhere \\(I_x\\) and \\(I_y\\) are the horizontal and vertical gradients respectively.\nNow the combined gradient magnitude could be calculated using two methods. * Using L1 Norm:\n\\[Magnitude = |I_x| + |I_y|\\]\n\nUsing L2 Norm:\n\n\\[Magnitude =  \\sqrt{I_x^2 + I_y^2}\\]\n\\[Direction = \\arctan(\\frac{I_y}{I_x})\\]\nMagnitude signifies the strength of the edge. The direction is the angle of the edge. The direction is rounded to one of four angles representing vertical, horizontal and two diagonals. The angles are 0, 45, 90 and 135 degrees. We are assuming the edges are in one of these four directions.\n\n# Compute the gradients by applying Sobel operator\nI_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=3)\nI_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=3)\n\n# Compute the magnitude of the gradients\nmagnitude = np.sqrt(I_x**2 + I_y**2)\n\n# Compute the magnitude of the gradients using the L1 norm\n# magnitude = np.abs(I_x) + np.abs(I_y)\n\ndirection = np.arctan2(I_y, I_x)\n\nplt.figure(figsize=(9, 7))\n\nplt.subplot(2, 2, 1)\nplt.imshow(cv.convertScaleAbs(I_x), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in X direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 2)\nplt.imshow(cv.convertScaleAbs(I_y), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in Y direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 3)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Combined Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 4)\nplt.imshow(direction)\nplt.axis('off')\nplt.title('Gradient Direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Direction')\ncbar.set_ticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\ncbar.set_ticklabels(['-180°', '-90°', '0°', '90°', '180°'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then performs non-maximum suppression. The idea is to thin out the edges. We want to keep only the local maxima in the gradient direction. The algorithm goes through all the points on the gradient magnitude image and keeps only the points which are local maxima in the direction of the gradient. The pixel are checked in the direction of the gradient. If the pixel is not the maximum, it is set to zero.\nLet \\(M(x, y)\\) be defined as:\n\\[\n\\\nM(x, y) =\n\\begin{cases}\n0, & \\text{if } M(x, y) &lt; M(x + \\Delta x, y + \\Delta y) \\text{ or } M(x, y) &lt; M(x - \\Delta x, y - \\Delta y), \\\\\nM(x, y), & \\text{otherwise.}\n\\end{cases}\n\\\n\\]\nwhere \\(M(x,y)\\) is the gradient magnitude image and \\(\\Delta x\\) and \\(\\Delta y\\) are the unit vectors in the direction of the gradient.\n\n\n\n\nEdge Direction\n\n\nThe image on the left shows the gradient direction. The image on the right shows the process of non-maximum suppression. The point B (marked in Red) is the local maxima in the direction of the gradient while the points A and C (marked in black) are not. The points A and C are set to zero and the point B is kept. This operation will result in thinning of the edges.\n\nnms_image = np.zeros_like(magnitude)\n\n# Define the window size\nfor i in range(1, magnitude.shape[0] - 1):\n    for j in range(1, magnitude.shape[1] - 1):\n        # Define the angle interval\n        angle = direction[i, j] if direction[i, j] &gt;= 0 else direction[i, j] + np.pi\n        angle = np.rad2deg(angle)\n        angle = angle % 180\n\n        # Perform Non-Maximum Suppression\n        if (0 &lt;= angle &lt; 22.5) or (157.5 &lt;= angle &lt;= 180):\n            prev = magnitude[i, j - 1]\n            nxt = magnitude[i, j + 1]\n        elif 22.5 &lt;= angle &lt; 67.5:\n            prev = magnitude[i + 1, j - 1]\n            nxt = magnitude[i - 1, j + 1]\n        elif 67.5 &lt;= angle &lt; 112.5:\n            prev = magnitude[i - 1, j]\n            nxt = magnitude[i + 1, j]\n        else:\n            prev = magnitude[i - 1, j - 1]\n            nxt = magnitude[i + 1, j + 1]\n\n        if magnitude[i, j] &gt;= prev and magnitude[i, j] &gt;= nxt:\n            nms_image[i, j] = magnitude[i, j]\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(1, 2, 2)\nplt.imshow(nms_image, cmap='gray')\nplt.axis('off')\nplt.title('Non-Maximum Suppression')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNext step is to apply hysteresis thresholding or also known as double thresholding. The algorithm uses two thresholds, a high threshold and a low threshold. All the points above the high threshold are considered to be strong edges and all the points below the low threshold are considered to be non-edges. The points between the two thresholds are considered to be weak edges. The algorithm then tracks the weak edges and checks if they are connected to strong edges. If they are connected to strong edges, they are considered to be edges. If they are not connected to strong edges, they are considered to be non-edges. This is done to remove the weak edges which are not connected to any of the strong edges.\nLet:\n\n\\(S\\) denote the set of strong edges\n\\(W\\) denote the set of weak edge\n\\(E(x, y)\\) represent whether a pixel at \\((x, y)\\) is classified as an edge \\(1\\) or not \\(0\\).\n\nThe classification can be expressed as:\n\\[\nE(x, y) =\n\\begin{cases}\n1, & \\text{if } (x, y) \\in S, \\text{ or } (x, y) \\in W \\text{ and connected to } S, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nConnected to \\(S\\) implies there exists a path from \\((x, y)\\) in \\(W\\) to some \\((x', y')\\) in \\(S\\) that satisfies the connectivity criteria (e.g., adjacency).\n\nEdges = np.zeros_like(nms_image)\n\nthreshold_high = 100\nthreshold_low = 50\nvicinity = 2\n\n# Define the weak and strong edges\nstrong_edges = nms_image &gt; threshold_high\nweak_edges = (nms_image &gt;= threshold_low) & (nms_image &lt;= threshold_high)\n\n\n# Define the edge map\nEdges[strong_edges] = 255\n\n# Define the weak edges that are connected to strong edges\nfor i in range(1, nms_image.shape[0] - 1):\n    for j in range(1, nms_image.shape[1] - 1):\n        if weak_edges[i, j]:\n            if np.sum(strong_edges[i-vicinity:i+vicinity+1, j-vicinity:j+vicinity+1]) &gt; 0:\n                Edges[i, j] = 255\n\n# Dilate the edges\nEdges = cv.dilate(Edges, np.ones((3, 3), np.uint8), iterations=1)\n\n\n# Display the Original Image and the Detected Edges\nplt.figure(figsize=(7, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(Edges, cmap='gray')\nplt.axis('off')\nplt.title('Detected Edges')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Final output is the edge image where the edges are detected. The edges are detected using the gradient information. The algorithm is very effective in detecting edges and is widely used in computer vision applications."
  },
  {
    "objectID": "posts/2024/Canny_Edge.html#algorithm",
    "href": "posts/2024/Canny_Edge.html#algorithm",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The algorithm uses a Gaussian filter to smooth the image and reduce noise. This is done to prevent the algorithm from detecting false edges due to noise. It also smoothens the image to reduce the abrupt changes in pixel values. These abrupt changes may end up being detected as edges.\n\\[I'(x,y) = G(x,y) * I(x,y)\\]\nwhere \\(I(x,y)\\) is the input image, \\(G(x,y)\\) is the Gaussian filter and \\(I'(x,y)\\) is the smoothed image. ’*’ represents convolution operation between the image and the filter. Reading the image and convert it to grayscale. Image is converted to grayscale because the edge detection is done on the intensity values of the image. The color information is not required for edge detection.\n\n# Importing libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Read the image\nimage = cv.imread('Flower.jpeg')\n\nplt.figure(figsize=(12, 4))\n# Convert the image to RGB (OpenCV loads it into BGR by default)\nplt.subplot(1, 3, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\n# Convert the image to grayscale\ngray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nplt.subplot(1, 3, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.axis('off')\nplt.title('Grayscaled Image')\n\n# Apply Gaussian Blur\nblurred_image = cv.GaussianBlur(gray_image, (3, 3), 0)\nplt.subplot(1, 3, 3)\nplt.imshow(blurred_image, cmap='gray')\nplt.axis('off')\nplt.title('Gaussian Blurred Image')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then calculates the gradient of the image. The gradient is calculated using the Sobel operator. The Sobel operator is a discrete differentiation operator. It computes an approximation of gradient of the image. The operator uses two 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives - one for horizontal changes, and one for vertical. \\(G_x\\) and \\(G_y\\) are the horizontal and vertical Sobel operators respectively.\n\\[G_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\] \\[G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\]\n\\[I_y = I' * G_y\\] \\[I_x = I' * G_x\\]\nwhere \\(I_x\\) and \\(I_y\\) are the horizontal and vertical gradients respectively.\nNow the combined gradient magnitude could be calculated using two methods. * Using L1 Norm:\n\\[Magnitude = |I_x| + |I_y|\\]\n\nUsing L2 Norm:\n\n\\[Magnitude =  \\sqrt{I_x^2 + I_y^2}\\]\n\\[Direction = \\arctan(\\frac{I_y}{I_x})\\]\nMagnitude signifies the strength of the edge. The direction is the angle of the edge. The direction is rounded to one of four angles representing vertical, horizontal and two diagonals. The angles are 0, 45, 90 and 135 degrees. We are assuming the edges are in one of these four directions.\n\n# Compute the gradients by applying Sobel operator\nI_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=3)\nI_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=3)\n\n# Compute the magnitude of the gradients\nmagnitude = np.sqrt(I_x**2 + I_y**2)\n\n# Compute the magnitude of the gradients using the L1 norm\n# magnitude = np.abs(I_x) + np.abs(I_y)\n\ndirection = np.arctan2(I_y, I_x)\n\nplt.figure(figsize=(9, 7))\n\nplt.subplot(2, 2, 1)\nplt.imshow(cv.convertScaleAbs(I_x), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in X direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 2)\nplt.imshow(cv.convertScaleAbs(I_y), cmap='gray')\nplt.axis('off')\nplt.title('Gradient in Y direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 3)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Combined Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(2, 2, 4)\nplt.imshow(direction)\nplt.axis('off')\nplt.title('Gradient Direction')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Direction')\ncbar.set_ticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\ncbar.set_ticklabels(['-180°', '-90°', '0°', '90°', '180°'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm then performs non-maximum suppression. The idea is to thin out the edges. We want to keep only the local maxima in the gradient direction. The algorithm goes through all the points on the gradient magnitude image and keeps only the points which are local maxima in the direction of the gradient. The pixel are checked in the direction of the gradient. If the pixel is not the maximum, it is set to zero.\nLet \\(M(x, y)\\) be defined as:\n\\[\n\\\nM(x, y) =\n\\begin{cases}\n0, & \\text{if } M(x, y) &lt; M(x + \\Delta x, y + \\Delta y) \\text{ or } M(x, y) &lt; M(x - \\Delta x, y - \\Delta y), \\\\\nM(x, y), & \\text{otherwise.}\n\\end{cases}\n\\\n\\]\nwhere \\(M(x,y)\\) is the gradient magnitude image and \\(\\Delta x\\) and \\(\\Delta y\\) are the unit vectors in the direction of the gradient.\n\n\n\n\nEdge Direction\n\n\nThe image on the left shows the gradient direction. The image on the right shows the process of non-maximum suppression. The point B (marked in Red) is the local maxima in the direction of the gradient while the points A and C (marked in black) are not. The points A and C are set to zero and the point B is kept. This operation will result in thinning of the edges.\n\nnms_image = np.zeros_like(magnitude)\n\n# Define the window size\nfor i in range(1, magnitude.shape[0] - 1):\n    for j in range(1, magnitude.shape[1] - 1):\n        # Define the angle interval\n        angle = direction[i, j] if direction[i, j] &gt;= 0 else direction[i, j] + np.pi\n        angle = np.rad2deg(angle)\n        angle = angle % 180\n\n        # Perform Non-Maximum Suppression\n        if (0 &lt;= angle &lt; 22.5) or (157.5 &lt;= angle &lt;= 180):\n            prev = magnitude[i, j - 1]\n            nxt = magnitude[i, j + 1]\n        elif 22.5 &lt;= angle &lt; 67.5:\n            prev = magnitude[i + 1, j - 1]\n            nxt = magnitude[i - 1, j + 1]\n        elif 67.5 &lt;= angle &lt; 112.5:\n            prev = magnitude[i - 1, j]\n            nxt = magnitude[i + 1, j]\n        else:\n            prev = magnitude[i - 1, j - 1]\n            nxt = magnitude[i + 1, j + 1]\n\n        if magnitude[i, j] &gt;= prev and magnitude[i, j] &gt;= nxt:\n            nms_image[i, j] = magnitude[i, j]\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(magnitude, cmap='gray')\nplt.axis('off')\nplt.title('Gradient Magnitude')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.subplot(1, 2, 2)\nplt.imshow(nms_image, cmap='gray')\nplt.axis('off')\nplt.title('Non-Maximum Suppression')\ncbar = plt.colorbar()\ncbar.set_label('Gradient Intensity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNext step is to apply hysteresis thresholding or also known as double thresholding. The algorithm uses two thresholds, a high threshold and a low threshold. All the points above the high threshold are considered to be strong edges and all the points below the low threshold are considered to be non-edges. The points between the two thresholds are considered to be weak edges. The algorithm then tracks the weak edges and checks if they are connected to strong edges. If they are connected to strong edges, they are considered to be edges. If they are not connected to strong edges, they are considered to be non-edges. This is done to remove the weak edges which are not connected to any of the strong edges.\nLet:\n\n\\(S\\) denote the set of strong edges\n\\(W\\) denote the set of weak edge\n\\(E(x, y)\\) represent whether a pixel at \\((x, y)\\) is classified as an edge \\(1\\) or not \\(0\\).\n\nThe classification can be expressed as:\n\\[\nE(x, y) =\n\\begin{cases}\n1, & \\text{if } (x, y) \\in S, \\text{ or } (x, y) \\in W \\text{ and connected to } S, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nConnected to \\(S\\) implies there exists a path from \\((x, y)\\) in \\(W\\) to some \\((x', y')\\) in \\(S\\) that satisfies the connectivity criteria (e.g., adjacency).\n\nEdges = np.zeros_like(nms_image)\n\nthreshold_high = 100\nthreshold_low = 50\nvicinity = 2\n\n# Define the weak and strong edges\nstrong_edges = nms_image &gt; threshold_high\nweak_edges = (nms_image &gt;= threshold_low) & (nms_image &lt;= threshold_high)\n\n\n# Define the edge map\nEdges[strong_edges] = 255\n\n# Define the weak edges that are connected to strong edges\nfor i in range(1, nms_image.shape[0] - 1):\n    for j in range(1, nms_image.shape[1] - 1):\n        if weak_edges[i, j]:\n            if np.sum(strong_edges[i-vicinity:i+vicinity+1, j-vicinity:j+vicinity+1]) &gt; 0:\n                Edges[i, j] = 255\n\n# Dilate the edges\nEdges = cv.dilate(Edges, np.ones((3, 3), np.uint8), iterations=1)\n\n\n# Display the Original Image and the Detected Edges\nplt.figure(figsize=(7, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(Edges, cmap='gray')\nplt.axis('off')\nplt.title('Detected Edges')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024/Canny_Edge.html#final-output",
    "href": "posts/2024/Canny_Edge.html#final-output",
    "title": "Canny Edge Detector",
    "section": "",
    "text": "The Final output is the edge image where the edges are detected. The edges are detected using the gradient information. The algorithm is very effective in detecting edges and is widely used in computer vision applications."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "2026\n\n\nSpring : Probability, Statistics and Data Visualization\n\n\nProfessor Incharge: Mayank Singh  Course Website: ES-114: Probability, Statistics and Data Visualization   Position : Graduate Teaching Fellow (GTF)  Taught concepts of probability, statistics, and data visualization using Python to a classroom of 360 students. Conducted weekly lab sessions, assisted in grading assignments, and provided support during exams.\n\n\n2025\n\n\nFall : Machine Learning\n\n\nProfessor Incharge: Nipun Batra  Course Website: ES-335: Machine Learning  Position : Head Teaching Assistant (TA)  Designed and graded quizzes, assignments, and viva examinations for a large undergraduate class of 280 students, and contributed to managing instructional logistics.\n\n\nSpring : Probability, Statistics and Data Visualization\n\n\nProfessor Incharge: Nipun Batra  Course Website: ES-114: Probability, Statistics and Data Visualization  Position : Head Teaching Assistant (TA)  Organised and evaluated quizzes, assignments, took vivas for a classroom of 320 students, while also playing a pivotal role in supporting classroom logistics.\n\n\n2024\n\n\nSpring and Fall : Machine Learning,\n\n\nProfessor Incharge: Nipun Batra  Course Website: ES-335: Machine Learning (Fall 2024) , ES-335: Machine Learning (Spring 2024)  Position : Teaching Assistant (TA)  Prepared and evaluated assessment components for 240 students, including quizzes, assignments, and viva examinations, while assisting with course administration.\n\n\n2023\n\n\nFall : Computer Systems\n\n\nProfessor Incharge: Abhishek Bichhawat, Sameer G Kulkarni  Position : Teaching Assistant (TA)  Graded assignments and assisted the professor run a class of 40 students smoothly.\n\n\nSummer : World of Engineering\n\n\nProfessor Incharge: Udit Bhatia  Position : Teaching Assistant (TA)  Mentored a group of 30 students in the identification, conceptualization, and modeling of a prototype to address a real-world problem, fostering their problem-solving abilities and teamwork skills.\n\n\nSpring : Probability, Statistics and Data Visualization\n\n\nProfessor Incharge: Shanmuganathan Raman  Position : Teaching Assistant (TA)  Successfully guided over 30 students in Python libraries, including Numpy, Pandas, Matplotlib, Scipy, and Scikit-learn, enhancing their data visualization skills.\n\n\n2022\n\n\nWinter : Introduction to Computing\n\n\nProfessor Incharge: Nipun Batra, Balagopal Komarath  Course Website: ES-112: Introduction to Computing  Position : Teaching Assistant (TA)  Led a Python lab for 30 students, teaching them essential Python concepts, and helped manage logistics, invigilation, and quiz evaluation for a class of 300 students."
  },
  {
    "objectID": "posts/2024/Neural_Process.html",
    "href": "posts/2024/Neural_Process.html",
    "title": "Neural Processes",
    "section": "",
    "text": "This notebook demonstrates the implementation of the Neural Processes. The model is trained to predict the distribution of the target function given a few examples of the function. In this notebook we will attempt to reconstruct an entire image given a few pixels of the image.\n\\[Model(\\{x_c, y_c\\}_{c=1}^n, \\{x_t\\}_{t=1}^m) \\rightarrow \\{y_t\\}_{t=1}^m\\]\n\\(\\{x_c, y_c\\}_{c=1}^n\\) are the context points where \\(x_i\\) are the input features and \\(y_i\\) are the labels.\n\\(\\{x_t\\}_{t=1}^m\\) are the target features where we want to predict the label values \\(y_j\\).\n\n\n\nNeural Processes Block Diagram\n\n\n\nThe model is composed of two neural networks: the encoder and the decoder. The encoder takes the context set as the input and encodes them into a fixed-size representation. The decoder takes the representation and the target features and predicts the pixel labels of the given target features."
  },
  {
    "objectID": "posts/2024/Neural_Process.html#generating-tasks-from-the-mnist-dataset",
    "href": "posts/2024/Neural_Process.html#generating-tasks-from-the-mnist-dataset",
    "title": "Neural Processes",
    "section": "Generating Tasks from the MNIST Dataset",
    "text": "Generating Tasks from the MNIST Dataset\n\nDownloading the MNIST Dataset\nWe will be using the MNIST dataset for this task. The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9). We will download the dataset using the torchvision library. We will convert the images to tensors and no further preprocessing is being performed on the images.\n\n# Downlaoding the MNIST dataset\ntrain_data = MNIST(root='./data', train=True, download=True, transform=ToTensor())\ntest_data = MNIST(root='./data', train=False, download=True, transform= ToTensor())\n\n\n\nCreating Context and Target sets\nWe will sample a random number of pixel coordinates \\((x^1, x^2)\\) and the pixel intensity values \\(I\\) for these coordinates. These coordinates and pixel intensity values will be used as context points. The Test set will contain all the pixel coordinates and Intensities of the image. The model will predict the pixel intensity values for all the pixel coordinates in the image.\n\ndef get_Context_Target_Sets(images_data, no_context_points,Image_shape):\n\n    m,n = Image_shape\n\n    # All possible coordinates in the image. Size of MNIST image is 28x28\n    All_corrdinates = np.array([(i,j) for i in range(m) for j in range(n)])\n\n    # Iterate over the dataset and create the context set and target set\n    task_set = []\n    for i in tqdm(range(len(images_data))):\n\n        # Images in the dataset are of shape (1, 28, 28). We need to remove the channel dimension to get the shape (28, 28)\n        image, _ = images_data[i]\n        image = image.squeeze().numpy()     # un-squeeze the image to remove the channel dimension\n\n        # Sample random context points indices\n        context_idx = np.random.choice(range(len(All_corrdinates)), no_context_points, replace=False)\n\n        # Get the context points and their corresponding pixel values from the generated context indices\n        context_points = All_corrdinates[context_idx]\n        context_pixels = image[context_points[:,0], context_points[:,1]]\n\n        # Concatenate the context points and their corresponding pixel values\n        context_set = np.concatenate([context_points, context_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n        # For training the model, we need to predict the pixel values for all the pixels in the image\n        target_points = All_corrdinates\n        target_pixels = image[target_points[:,0], target_points[:,1]]\n\n        # Concatenate the target points and their corresponding pixel values\n        target_set = np.concatenate([target_points, target_pixels.reshape(-1,1)], axis=-1).astype(np.float32)\n\n        # Append the context set and target set to the train_set\n        task_set.append((context_set, target_set))\n\n    return task_set\n\n\ntrain_set = get_Context_Target_Sets(images_data=train_data, no_context_points=200, Image_shape=(28,28))\ntrain_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in train_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\n100%|██████████| 60000/60000 [00:04&lt;00:00, 12578.65it/s]\n\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])\n\n\n\ntest_set = get_Context_Target_Sets(images_data=test_data, no_context_points=200, Image_shape=(28,28))\ntest_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)\n\nfor context_pairs, target_pairs in test_dataloader:\n    print(context_pairs.shape, target_pairs.shape)\n    break\n\n100%|██████████| 10000/10000 [00:00&lt;00:00, 11855.53it/s]\n\n\ntorch.Size([32, 200, 3]) torch.Size([32, 784, 3])"
  },
  {
    "objectID": "posts/2024/Neural_Process.html#intializing-the-neural-process-class",
    "href": "posts/2024/Neural_Process.html#intializing-the-neural-process-class",
    "title": "Neural Processes",
    "section": "Intializing the Neural Process Class",
    "text": "Intializing the Neural Process Class\n\nEncoder and Decoder Networks\nOur Model class will contain the Encoder and Decoder networks. The Encoder network will take the context points and encode them into a fixed-size representation. The Decoder network will take the representation and the pixel coordinates and predict the pixel intensity value of the given coordinate.\n\n\nTraining Method\nMethod to train the model. The model is trained to predict the pixel intensity values of the target pixel coordinates given the context points. The loss function is the Mean Squared Error loss between the predicted pixel intensity values and the actual pixel intensity values.\n\n\nTesting Method\nMethod to test the model. The model is tested on the test set. The model is provided with the context points and the model predicts the pixel intensity values of the target pixel coordinates. The Mean Squared Error loss is calculated between the predicted pixel intensity values and the actual pixel intensity values.\n\nclass Neural_Procss_Model(nn.Module):\n    \"\"\"\n    Neural Process Model has two parts: encoder and decoder\n\n    Encoder takes in the context pairs (X1_c, X2_c, PixelIntensity) and encode them into a latent representation\n\n    Decoder takes in the latent representation and the target pairs (X1_t, X2_t) and output the predicted target pairs\n    \"\"\"\n    def __init__(self,device):\n        super(Neural_Procss_Model, self).__init__()\n\n        if device == 'cuda':\n            if torch.cuda.is_available():\n                device = torch.device('cuda')\n        \n        if device == 'mps':\n            device = torch.device('mps')\n\n        self.device = device\n\n        # Encoder takes in the context pairs (X1c,X2c,PixelIntensity) and encode them into a latent representation\n        self.encoder = nn.Sequential(\n            nn.Linear(3,64),\n            nn.ReLU(),\n            nn.Linear(64,128),\n            nn.ReLU(),\n            nn.Linear(128,256),\n        )\n\n        # Decoder takes in the latent representation and the target pairs and output the predicted target pairs\n        self.decoder = nn.Sequential(\n            nn.Linear(256+2,128),\n            nn.ReLU(),\n            nn.Linear(128,64),\n            nn.ReLU(),\n            nn.Linear(64,32),\n            nn.ReLU(),\n            nn.Linear(32,1)\n        )\n\n    def forward(self, context_pairs, target_pairs):\n        \"\"\"\n        Forward pass of the Neural Process Model. \n        \n        It takes in the context cordinate pairs (X1_c, X2_c, PixelIntensity) and target cordinate pairs (X1_t, X2_t, PixelIntensity) and output the predicted target pixel intensity for the target cordinate pairs\n\n        Parameters:\n        context_pairs: torch.Tensor of shape (batch_size, num_context_pairs, 3)\n        target_pairs: torch.Tensor of shape (batch_size, num_target_pairs, 2)\n\n        Returns:\n        predicted_target_Pixel Intensity: torch.Tensor of shape (batch_size, num_target_pairs, 1)\n        \"\"\"\n        \n        # Encode the context pairs into a latent representation\n        latent_representation = self.encoder(context_pairs)\n\n        # Average the latent representation\n        latent_representation = torch.mean(latent_representation,dim=1)\n\n        # Repeat the latent representation for each target pair\n        latent_representation = latent_representation.unsqueeze(1).repeat(1,target_pairs.size(1),1)\n\n        # Concatenate the latent representation with the target pixel locations\n        target_pixel_locations = target_pairs[:,:,:2]\n        target = torch.cat([latent_representation,target_pixel_locations],dim=-1)\n\n        # Decode the target pairs to obtain the predicted target pixel intensity\n        predicted_target_pixel_intensity = self.decoder(target)\n\n        return predicted_target_pixel_intensity\n    \n\n    def train(self, train_dataloader, num_epochs=100, optim = torch.optim.Adam, lr=3e-4, criterion = nn.MSELoss(),verbose=True):\n        \"\"\"\n        Train the Neural Process Model\n\n        Parameters:\n        train_dataloader: DataLoader object for the training data\n        num_epochs: int, number of epochs to train the model\n        optimer: str, optimer to use for training the model\n        lr: float, learning rate for the optimer\n        criterion: loss function to use for training the model\n        \"\"\"\n\n        device = self.device\n\n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the optimizer\n        optimizer = optim(self.parameters(), lr=lr)\n\n        for epoch in range(num_epochs):\n            for i, (context_pairs, target_pairs) in enumerate(train_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Zero the gradients\n                optimizer.zero_grad()\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                # Backward pass\n                loss.backward()\n\n                # Update the weights\n                optimizer.step()\n\n            if verbose:\n                print(\"Epoch: {}/{} Loss: {:.5f}\".format(epoch+1,num_epochs,loss.item()))\n\n\n\n    def test(self, test_dataloader, criterion = nn.MSELoss()):\n        \"\"\"\n        Test the Neural Process Model\n\n        Parameters:\n        test_dataloader: DataLoader object for the test data\n        \"\"\"\n\n        device = self.device\n    \n        # Move the model to the device\n        self.to(device)\n\n        # Initialize the loss\n        test_loss = 0\n\n        with torch.no_grad():\n            for i, (context_pairs, target_pairs) in enumerate(test_dataloader):\n                context_pairs = context_pairs.to(device)\n                target_pairs = target_pairs.to(device)\n\n                # Forward pass\n                predicted_target_pixel_intensity = self(context_pairs, target_pairs)\n\n                # Calculate the loss\n                loss = criterion(predicted_target_pixel_intensity.reshape(-1,1), target_pairs[:,:,2].reshape(-1,1))\n\n                test_loss += loss.item()\n\n        print(f'Test Loss: {test_loss/len(test_dataloader)}')\n\n        \n\n\n\nTraining the Neural Process model\n\n# Set the device. If cuda is available, use cuda. \nif torch.cuda.is_available():\n    device = torch.device('cuda')\n\n# If mps is available, use mps. \nelif torch.mps.is_available():\n    device = torch.device('mps')\n\n# Else use cpu\nelse:\n    device = torch.device('cpu')\n\nprint(\"Device in use: \", device)\n\nDevice in use:  mps\n\n\n\nmodel = Neural_Procss_Model(device=device)\n\n# model.to('cuda')\n\nmodel.train(train_dataloader, num_epochs=30)\n\nEpoch: 1/30 Loss: 0.06018\nEpoch: 2/30 Loss: 0.06170\nEpoch: 3/30 Loss: 0.05950\nEpoch: 4/30 Loss: 0.06060\nEpoch: 5/30 Loss: 0.05958\nEpoch: 6/30 Loss: 0.06148\nEpoch: 7/30 Loss: 0.05642\nEpoch: 8/30 Loss: 0.05425\nEpoch: 9/30 Loss: 0.05090\nEpoch: 10/30 Loss: 0.04823\nEpoch: 11/30 Loss: 0.04689\nEpoch: 12/30 Loss: 0.04165\nEpoch: 13/30 Loss: 0.04482\nEpoch: 14/30 Loss: 0.04253\nEpoch: 15/30 Loss: 0.03567\nEpoch: 16/30 Loss: 0.04409\nEpoch: 17/30 Loss: 0.03556\nEpoch: 18/30 Loss: 0.03798\nEpoch: 19/30 Loss: 0.03793\nEpoch: 20/30 Loss: 0.03933\nEpoch: 21/30 Loss: 0.03644\nEpoch: 22/30 Loss: 0.03336\nEpoch: 23/30 Loss: 0.03567\nEpoch: 24/30 Loss: 0.03594\nEpoch: 25/30 Loss: 0.03620\nEpoch: 26/30 Loss: 0.03798\nEpoch: 27/30 Loss: 0.03487\nEpoch: 28/30 Loss: 0.03350\nEpoch: 29/30 Loss: 0.03534\nEpoch: 30/30 Loss: 0.03392\n\n\n\nmodel.test(test_dataloader)\n\nTest Loss: 0.03589645992762174"
  },
  {
    "objectID": "posts/2024/Lucas_Kanade.html",
    "href": "posts/2024/Lucas_Kanade.html",
    "title": "Lucas Kanade Optical Flow",
    "section": "",
    "text": "Optical Flow\nOptical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movement of object or camera. It is vector field where each vector denotes the movement of points from first frame to second. Generally, optical flow is used to track the motion of objects in a video. Lets assume you have captured two frames with a small time difference \\(\\Delta t\\). You wish to track the motion of a pixel located at \\((x, y)\\) in first frame. In the second frame, the pixel has moved to \\((x + \\Delta x, y + \\Delta y)\\). The vector \\((\\Delta x, \\Delta y)\\) is the optical flow vector.\n\n\n\n\n\nOptical Flow : Brightness Constancy Constraint\n\n\n\\[I(x, y, t) = I(x + \\Delta x, y + \\Delta y, t + \\Delta t)\\]\nThe above equation is the basic assumption in optical flow. It assumes that intensity of an object does not change between two consecutive frames. This constraint is called brightness constancy constraint. This equation can be expanded using taylor series to get the optical flow equation.\n\\[I(x + \\Delta x, y + \\Delta y, t + \\Delta t) \\approx I(x, y, t) + \\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t = I(x, y, t)\\]\n\\[\\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t = 0\\]\n\\[\\frac{\\partial I}{\\partial x} u + \\frac{\\partial I}{\\partial y} v + \\frac{\\partial I}{\\partial t} = 0\\]\nwhere \\(u = \\frac{\\Delta x}{\\Delta t}\\) and \\(v = \\frac{\\Delta y}{\\Delta t}\\) are the optical flow velocities in x and y directions respectively.\nThe above equation is called optical flow equation. It is a single equation with two unknowns \\(u\\) and \\(v\\). Hence, it is an ill-posed problem. To solve this problem, we need to make some more assumptions. One of the assumption to arrive here was brightness constancy assumption. It assumes that the intensity of an object does not change between two consecutive frames. The other assumption made was given by lucas and kanade. They assumed that the optical flow is same for all the pixels in a neighborhood. This assumption is called spatial coherence assumption.\n\n\nLucas Kanade Optical Flow\nThe Lucas-Kanade method is a widely used differential method for optical flow estimation developed by Bruce D. Lucas and Takeo Kanade. It assumes that the flow is essentially constant in a local neighbourhood of the pixel under consideration, and solves the basic optical flow equations for all the pixels in that neighbourhood by the least squares criterion.\n\n\n\nLucas Kanade : spatial coherence assumption\n\n\n\\[\\frac{\\partial I}{\\partial x} u + \\frac{\\partial I}{\\partial y} v + \\frac{\\partial I}{\\partial t} = 0\\]\n\\[E_x = \\frac{\\partial I}{\\partial x}, E_y = \\frac{\\partial I}{\\partial y}, E_t = \\frac{\\partial I}{\\partial t}\\]\n\\[E_x u + E_y v + E_t = 0\\]\n\\[\\begin{bmatrix} E_x & E_y \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = -E_t\\]\n\\[ \\begin{bmatrix} E_x(i-1,j-1) & E_y(i-1,j-1) \\\\ E_x(i-1,j) & E_y(i-1,j) \\\\ E_x(i-1,j+1) & E_y(i-1,j+1) \\\\ E_x(i,j-1) & E_y(i,j-1) \\\\ E_x(i,j) & E_y(i,j) \\\\ E_x(i,j+1) & E_y(i,j+1) \\\\ E_x(i+1,j-1) & E_y(i+1,j-1) \\\\ E_x(i+1,j) & E_y(i+1,j) \\\\ E_x(i+1,j+1) & E_y(i+1,j+1) \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} -E_t(i-1,j-1) \\\\ -E_t(i-1,j) \\\\ -E_t(i-1,j+1) \\\\ -E_t(i,j-1) \\\\ -E_t(i,j) \\\\ -E_t(i,j+1) \\\\ -E_t(i+1,j-1) \\\\ -E_t(i+1,j) \\\\ -E_t(i+1,j+1) \\end{bmatrix}\\]\n\\[A_{(9,2)}  x_{(2,1)} = b_{(9,1)}\\]\n\\[A \\overrightarrow{x} = \\overrightarrow{b}\\]\n\\[x = (A^T A)^{-1} A^T b\\]\nSo for a chosen patch we can estimate the optical flow using above equation. This could also be written as\n\\[\\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} \\sum E_x E_x & \\sum E_x E_y \\\\ \\sum E_y E_x & \\sum E_y E_y \\end{bmatrix}^{-1} \\begin{bmatrix} -\\sum E_x E_t \\\\ -\\sum E_y E_t \\end{bmatrix}\\]\nThis is the final equation used to estimate the optical flow using Lucas Kanade method. The above equation is solved for each patch in the image to get the optical flow vectors. Optical flow algorithm could be summarized as follows:\n\nCompute image gradients \\(E_x\\) and \\(E_y\\).\nCompute temporal gradient \\(E_t\\).\nCompute the matrix \\(A\\) and vector \\(b\\) for each patch.\nSolve the equation \\(A \\overrightarrow{x} = \\overrightarrow{b}\\) to get the optical flow vectors.\n\n\n\nLucas Kanade Implementation\nUNDER CONSTRUCTION !!\n\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.signal import convolve2d\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport imageio\nimport cv2 \n\n\ndef compute_derivatives(frame1, frame2):\n    \"\"\"\n    Compute spatial and temporal derivatives between two consecutive frames for optical flow estimation.\n    \n    This function calculates the following derivatives:\n    - Ix: Spatial derivative in x direction (horizontal)\n    - Iy: Spatial derivative in y direction (vertical)\n    - It: Temporal derivative between frames\n    \n    The spatial derivatives are computed using Sobel operators on the average of both frames.\n    The temporal derivative is computed as the difference between frames.\n    \n    Args:\n        frame1 (numpy.ndarray): First frame (earlier time point)\n        frame2 (numpy.ndarray): Second frame (later time point)\n        \n    Returns:\n        tuple: A tuple containing three numpy.ndarray:\n            - Ix: Spatial derivative in x direction\n            - Iy: Spatial derivative in y direction\n            - It: Temporal derivative\n    \n    Note:\n        Both input frames should have the same dimensions and be in grayscale format.\n    \"\"\"\n    # Define Sobel operator for x direction (horizontal)\n    kernel_x = np.array([[1, 0, -1], \n                        [2, 0, -2], \n                        [1, 0, -1]]) /8.0\n    \n    # Create y direction kernel by transposing x direction kernel\n    kernel_y = kernel_x.T\n    \n    # Calculate average frame to reduce noise in derivative estimation\n    calc_frame = (frame1 + frame2)//2\n    \n    # Compute spatial derivatives using convolution with Sobel operators\n    # 'same' mode maintains original dimensions, 'symm' handles boundaries by reflection\n    Ix = convolve2d(calc_frame, kernel_x, mode='same', boundary='symm')\n    Iy = convolve2d(calc_frame, kernel_y, mode='same', boundary='symm')\n    \n    # Compute temporal derivative as difference between frames\n    # Convert to float to avoid integer overflow\n    It = frame2.astype(float) - frame1.astype(float)\n    \n    return Ix, Iy, It\n\n\ndef lk_flow(frame1, frame2, window_size=16, tau=1e-2):\n    \"\"\"\n    Compute optical flow using the Lucas-Kanade method with improved robustness features.\n    \n    This function implements the Lucas-Kanade algorithm for computing optical flow between\n    two consecutive frames. It includes several improvements for robustness:\n    - Gaussian smoothing for noise reduction\n    - Eigenvalue-based condition checking\n    - Flow magnitude thresholding\n    - Optional outlier removal (commented out in post-processing)\n    \n    Args:\n        frame1 (numpy.ndarray): First frame (earlier time point)\n        frame2 (numpy.ndarray): Second frame (later time point)\n        window_size (int, optional): Size of the local window for flow computation. \n                                   Should be odd. Defaults to 16.\n        tau (float, optional): Threshold for eigenvalue condition checking. \n                             Higher values mean stricter filtering. Defaults to 1e-2.\n    \n    Returns:\n        tuple: Two numpy.ndarray containing:\n            - u: Horizontal component of the optical flow\n            - v: Vertical component of the optical flow\n            \n    Note:\n        - Frames should be in grayscale format\n        - The function includes eigenvalue-based filtering to handle aperture problem\n        - Flow vectors with magnitude &gt; 50 are filtered out\n    \"\"\"\n    # Apply Gaussian smoothing to reduce noise\n    # Higher sigma (2) for more aggressive smoothing\n    frame1 = gaussian_filter(frame1, sigma=2)\n    frame2 = gaussian_filter(frame2, sigma=2)\n    \n    # Compute spatial and temporal derivatives\n    Ix, Iy, It = compute_derivatives(frame1, frame2)\n    \n    # Initialize flow fields with zeros\n    u = np.zeros_like(frame1, dtype=float)\n    v = np.zeros_like(frame1, dtype=float)\n    \n    # Pad images to handle border pixels\n    # Using edge padding to avoid border artifacts\n    pad = window_size // 2\n    Ix_pad = np.pad(Ix, ((pad, pad), (pad, pad)), mode='edge')\n    Iy_pad = np.pad(Iy, ((pad, pad), (pad, pad)), mode='edge')\n    It_pad = np.pad(It, ((pad, pad), (pad, pad)), mode='edge')\n    \n    # Iterate through each pixel in the frame\n    for y in range(pad, frame1.shape[0] + pad):\n        for x in range(pad, frame1.shape[1] + pad):\n            # Extract local windows for derivatives\n            # Flatten to create system of equations\n            Ix_win = Ix_pad[y-pad:y+pad+1, x-pad:x+pad+1].flatten()\n            Iy_win = Iy_pad[y-pad:y+pad+1, x-pad:x+pad+1].flatten()\n            It_win = It_pad[y-pad:y+pad+1, x-pad:x+pad+1].flatten()\n            \n            # Construct system of equations Av = b\n            # A: spatial gradients, b: negative temporal gradient\n            A = np.vstack((Ix_win, Iy_win)).T\n            b = -It_win\n            \n            # Compute ATA for solving least squares\n            ATA = np.dot(A.T, A)\n            \n            # Check eigenvalues for aperture problem\n            eigenvalues = np.linalg.eigvals(ATA)\n            \n            # Skip if eigenvalues indicate ill-conditioned system\n            # Two conditions: minimum eigenvalue threshold and condition number check\n            if np.min(eigenvalues) &lt; tau or np.max(eigenvalues) / (np.min(eigenvalues) + 1e-10) &gt; 100:\n                continue\n                \n            # Solve the system of equations\n            try:\n                flow = np.linalg.solve(ATA, np.dot(A.T, b))\n                \n                # Apply flow magnitude threshold to filter out large motions\n                if np.sqrt(flow[0]**2 + flow[1]**2) &lt; 50:  # Threshold can be adjusted\n                    u[y-pad, x-pad] = flow[0]\n                    v[y-pad, x-pad] = flow[1]\n                    \n            except np.linalg.LinAlgError:\n                continue\n\n    return u, v\n\n\nfile = 0\ngif_path = \"../../images/blogs/Lucas-Kanade/\"+str(file)+\".gif\"\ngif_frames = np.array(imageio.mimread(gif_path))        # Read GIF file\n\nwriter = cv2.VideoWriter(\"../../images/blogs/Lucas-Kanade/Flow\"+str(file)+\".AVI\",\n                         cv2.VideoWriter_fourcc(*'XVID'), 10, (2*gif_frames[0].shape[1], 2*gif_frames[0].shape[0]))\n\nfor i in range(1, len(gif_frames)):\n    # Convert frames to grayscale\n    frame1_gray = cv2.cvtColor(gif_frames[i-1], cv2.COLOR_RGB2GRAY)\n    frame2_gray = cv2.cvtColor(gif_frames[i], cv2.COLOR_RGB2GRAY)\n    \n    # Compute optical flow using Lucas-Kanade method.\n    u, v = lk_flow(frame1_gray, frame2_gray, window_size=16, tau=1e-2)\n    u,v = u*2, v*2\n    \n    # Display optical flow vectors on frame.\n    flow_img = gif_frames[i-1].copy()\n    flow_img = cv2.cvtColor(flow_img, cv2.COLOR_RGB2BGR)\n    step = 8\n\n    for y in range(0, flow_img.shape[0], step):\n        for x in range(0, flow_img.shape[1], step):\n            cv2.arrowedLine(flow_img, (x, y), (int(x+u[y, x]), int(y+v[y, x])), (0, 0, 255), 1)\n\n    flow_img = cv2.resize(flow_img, (2*flow_img.shape[1], 2*flow_img.shape[0]))\n    # Display frame with optical flow vectors.\n    cv2.imshow('Optical Flow', flow_img)\n\n    # Write frame to output video\n    writer.write(flow_img)\n    \n    # Press 'q' to quit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        cv2.destroyAllWindows()\n        cv2.waitKey(1)\n\ncv2.destroyAllWindows()\ncv2.waitKey(1)"
  },
  {
    "objectID": "posts/2024/CondaPost.html",
    "href": "posts/2024/CondaPost.html",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "",
    "text": "This blogpost is written for myself as I keep forgetting the steps to set up a conda environment for my ML projects."
  },
  {
    "objectID": "posts/2024/CondaPost.html#install-conda",
    "href": "posts/2024/CondaPost.html#install-conda",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Install Conda",
    "text": "Install Conda\nThese four commands quickly and quietly install the latest 64-bit version of the installer and then clean up after themselves. To install a different version or architecture of Miniconda for Linux, change the name of the .sh installer in the wget command.\nVisit the Miniconda website to find the latest version of Miniconda for Linux.\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\nAfter installing, initialize your newly-installed Miniconda. The following commands initialize for bash and zsh shells:\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\nyou can run conda -V to check if conda is installed correctly."
  },
  {
    "objectID": "posts/2024/CondaPost.html#creating-conda-environment",
    "href": "posts/2024/CondaPost.html#creating-conda-environment",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Creating Conda Environment",
    "text": "Creating Conda Environment\nThe below command will create the conda environment with the name EnvName and python version 3.9.\nconda create --name EnvName python=3.9 jupyter\nTo activate the environment, run the below command:\nconda activate EnvName"
  },
  {
    "objectID": "posts/2024/CondaPost.html#installing-pytorch",
    "href": "posts/2024/CondaPost.html#installing-pytorch",
    "title": "Setting up Conda Environment for ML Projects",
    "section": "Installing Pytorch",
    "text": "Installing Pytorch\nVisit the Pytorch website to find the latest version of Pytorch. You can choose your system configuration and get the command to install Pytorch.\n\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\ncheck if everything is installed correctly by running the below command:\npython -c \"import torch; print(torch.__version__)\"\nInstalling other libraries and packages."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ayush Shrivastava",
    "section": "",
    "text": "I am a first-year PhD student in Computer Science and Engineering at IIT Gandhinagar, working at the intersection of machine learning, sustainability, and health sensing. I am an active member of the Sustainability Lab, led by Prof. Nipun Batra, and the Smash Lab, led by Prof. Mayank Goel.\nI hold a master’s degree in Computer Science and Engineering from IIT Gandhinagar and a bachelor’s degree in Electronics and Telecommunications from Jabalpur Engineering College. My research focuses on applying machine learning to develop innovative solutions in sustainability and health sensing. I am passionate about solving challenging problems using machine learning to create meaningful impact."
  },
  {
    "objectID": "index.html#my-journey-so-far",
    "href": "index.html#my-journey-so-far",
    "title": "Ayush Shrivastava",
    "section": "My Journey So Far",
    "text": "My Journey So Far\n\n\n\n\n\n\n15 Oct 2025\n\n\nSuccessfully defended the PhD research proposal at IIT Gandhinagar.\n\n\n\n\n18 Aug 2025\n\n\nCompleted Certification in Scientific Writing at IIT Gandhinagar with Distinction (top 10%) for the year 2024–2025.\n\n\n\n\n05 Jun 2025\n\n\nDelivered tutorial on “Heart Rate Prediction” at ACM Summer School on AI for Social Good 2025, IIT Gandhinagar.\n\n\n\n\n01 Jun 2025\n\n\nDelivered tutorial on “Object Detection” at ACM Summer School on AI for Social Good 2025, IIT Gandhinagar.\n\n\n\n\n15 Apr 2025\n\n\nQualified the PhD Qualifying Examination at IIT Gandhinagar.\n\n\n\n\n27-01 Mar 2025\n\n\nShowcased our work in Global Tech Expo iInventive 2025, IIT Madras\n\n\n\n\n10 Jan 2025\n\n\nPresented my work Towards Sleep Apnea Screening via Thermal Imagery” in Graduate Forum, COMSNETS 2025\n\n\n\n\n31 Aug 2024\n\n\nJoined Sustainability Lab and Smash Lab as a PhD student\n\n\n\n\n\n08-11 Jul 2024\n\n\nAttended 7th ACM SIGCAS/SIGCHI Conference of Computing and Sustainable Societies ACM COMPASS 2024\n\n\n\n\n29 Jun 2024\n\n\nConvocation! Got Masters degree from IIT Gandhinagar\n\n\n\n\n\n\n\n\n\n\n18 July 2022\n\n\nJoined M.Tech in Computer Science at IIT Gandhinagar and became part of the Sustainability Lab.\n\n\n\n\n\n17 June 2019\n\n\nJoined IBM India as an Application Developer where I gained 2.5 years of corporate experience before moving on.\n\n\n\n\n15 May 2019\n\n\nCompleted B.E in Electronics and Telecommunication from Jabalpur Engineering College, Jabalpur"
  },
  {
    "objectID": "gallery.html#comsnets-2025-bangalore",
    "href": "gallery.html#comsnets-2025-bangalore",
    "title": "Gallery",
    "section": "",
    "text": "Attending COMSNETS 2025\n\n\n\n\n\nPresenting “Towards Sleep Apnea Screening via Thermal Imagery”\n\n\n\n\n\nInside IIIT Delhi Campus"
  },
  {
    "objectID": "gallery.html#acm-summer-school-2025-iit-gandhinagar",
    "href": "gallery.html#acm-summer-school-2025-iit-gandhinagar",
    "title": "Gallery",
    "section": "",
    "text": "Taking a session Object Detection on Day-1 of Summer School.\n\n\n\n\n\nTaking a session on Day-5 on “Seeing Red”. Estimating Heart rate from Smartphone Camera"
  },
  {
    "objectID": "posts/2026/initial_install.html",
    "href": "posts/2026/initial_install.html",
    "title": "Setting Up Your Environment",
    "section": "",
    "text": "Think of a virtual environment as a separate workspace for each of your projects. Imagine you have two different projects:\n\nProject A needs Python 3.9 and version 1.0 of a package called “data-tools”\nProject B needs Python 3.11 and version 2.0 of the same “data-tools” package\n\nWithout virtual environments, you could only have one version installed at a time, and switching between projects would be a nightmare. Virtual environments solve this problem by creating isolated spaces where each project can have its own Python version and packages without interfering with each other.\nKey benefits of virtual environments:\n\nKeep different projects separate and organized\nAvoid conflicts between package versions\nMake it easy to share your project setup with others\nPrevent accidentally breaking one project while working on another"
  },
  {
    "objectID": "posts/2026/initial_install.html#general-instructions",
    "href": "posts/2026/initial_install.html#general-instructions",
    "title": "Setting Up Your Python Development Environment",
    "section": "",
    "text": "Before you begin, please read these important guidelines:\nGetting Help:\n\nIf you encounter any issues or have questions at any point during this lab, raise your hand and contact the nearest Teaching Assistant (TA) immediately.\nDo not spend more than 5 minutes stuck on a single step. Ask for help early rather than falling behind.\nTAs are here to assist you. No question is too small or too basic.\n\nWorking Through the Lab:\n\nFollow the instructions in order. Do not skip sections unless instructed by your TA.\nRead each step carefully before executing commands.\nPay attention to your operating system (Windows, macOS, or Linux) as instructions may differ.\nTest each step before moving to the next one to ensure everything is working correctly.\n\nImportant Notes:\n\nTake notes of any errors you encounter and the solutions that worked for you.\nKeep this document open for reference during future assignments.\nMake sure you complete the verification steps to confirm your installation is working.\nDo not leave the lab until you have a fully functional Python environment.\n\nLab Etiquette:\n\nBe patient during downloads and installations as they may take several minutes.\nHelp your neighbors if they are struggling and you have completed a section.\nKeep your workspace organized and save any important files you create.\nBefore leaving, ensure you can run Python code and that all required packages are installed."
  },
  {
    "objectID": "posts/2026/initial_install.html#lab-overview",
    "href": "posts/2026/initial_install.html#lab-overview",
    "title": "Setting Up Your Python Development Environment",
    "section": "Lab Overview",
    "text": "Lab Overview\nDuration: 1.5 hours\nObjectives: By the end of this lab, you will be able to:\n\nInstall Anaconda on your computer\nUnderstand and configure environment variables\nCreate and manage Python virtual environments\nRun Python code and Jupyter notebooks\nComplete course assignments using your setup"
  },
  {
    "objectID": "posts/2026/initial_install.html#part-1-installing-anaconda",
    "href": "posts/2026/initial_install.html#part-1-installing-anaconda",
    "title": "Setting Up Your Environment",
    "section": "Part 1: Installing Anaconda",
    "text": "Part 1: Installing Anaconda\n\nWhat is Anaconda?\nAnaconda is a distribution of Python that comes with many pre-installed packages for data science and scientific computing. It also includes conda, a package manager that makes it easy to install additional packages and manage different environments.\n\n\nStep 1: Download Anaconda\n\nGo to https://www.anaconda.com/download\nChoose the installer for your operating system (Windows, macOS, or Linux)\nDownload the latest Python 3.x version (not Python 2.x)\n\n\n\nStep 2: Install Anaconda\n\nWindows Installation\n\nRun the downloaded .exe file\nClick “Next” through the setup wizard\nImportant: On the “Advanced Installation Options” screen:\n\nCheck “Add Anaconda3 to my PATH environment variable” (even though it’s not recommended, it makes things easier for beginners)\nCheck “Register Anaconda3 as my default Python 3.x”\n\nClick “Install” and wait for the installation to complete (this may take 10-15 minutes)\nClick “Finish”\n\n\n\nmacOS Installation\n\nOpen the downloaded .pkg file\nFollow the installation prompts\nClick “Continue” and “Install”\nEnter your password when prompted\nClick “Close” when installation is complete\n\n\n\nLinux Installation\n\nOpen Terminal\nNavigate to the directory where you downloaded the file\nRun the following command (replace with your actual filename):\n\nbash Anaconda3-2024.xx-Linux-x86_64.sh\n\nPress Enter to read the license agreement, then type “yes” to accept\nPress Enter to confirm the installation location\nType “yes” when asked to initialize Anaconda\n\n\n\n\nStep 3: Verify Installation\nOpen a new terminal or command prompt and run:\nconda --version\nYou should see output like conda 24.x.x. If you get an error, proceed to the next section."
  },
  {
    "objectID": "posts/2026/initial_install.html#part-2-setting-up-environment-variables",
    "href": "posts/2026/initial_install.html#part-2-setting-up-environment-variables",
    "title": "Setting Up Your Environment",
    "section": "Part 2: Setting Up Environment Variables",
    "text": "Part 2: Setting Up Environment Variables\nIf the conda --version command didn’t work, you need to add Anaconda to your system’s PATH.\n\nWindows: Adding to PATH\n\nPress Win + X and select “System”\nClick “Advanced system settings” on the right\nClick “Environment Variables” button\nUnder “User variables”, find and select “Path”, then click “Edit”\nClick “New” and add these paths (adjust if you installed in a different location):\n\nC:\\Users\\YourUsername\\anaconda3\nC:\\Users\\YourUsername\\anaconda3\\Scripts\nC:\\Users\\YourUsername\\anaconda3\\Library\\bin\n\nClick “OK” on all windows\nClose and reopen your command prompt\nTest again with conda --version\n\n\n\nmacOS/Linux: Adding to PATH\n\nOpen Terminal\nCheck which shell you’re using:\n\necho $SHELL\n\nEdit your shell configuration file:\n\nIf using bash: nano ~/.bashrc\nIf using zsh: nano ~/.zshrc\n\nAdd this line at the end of the file:\n\nexport PATH=\"$HOME/anaconda3/bin:$PATH\"\n\nSave the file (Ctrl+O, Enter, then Ctrl+X in nano)\nReload the configuration:\n\nsource ~/.bashrc  # or source ~/.zshrc\n\nTest with conda --version"
  },
  {
    "objectID": "posts/2026/initial_install.html#part-3-creating-your-first-environment",
    "href": "posts/2026/initial_install.html#part-3-creating-your-first-environment",
    "title": "Setting Up Your Environment",
    "section": "Part 3: Creating Your First Environment",
    "text": "Part 3: Creating Your First Environment\n\nWhy Use Environments?\nVirtual environments let you have different versions of Python and packages for different projects. This prevents conflicts and keeps your projects organized.\n\n\nCreate a Course Environment\nOpen your terminal/command prompt and run these commands:\n# Create a new environment named 'course_env' with Python 3.11\nconda create -n course_env python=3.11 -y\nThis will take a few minutes as it downloads and installs Python.\n\n\nActivate Your Environment\n# Windows\nconda activate course_env\n\n# macOS/Linux\nconda activate course_env\nYour prompt should now show (course_env) at the beginning, indicating the environment is active.\n\n\nInstall Essential Packages\nWith your environment activated, install the packages you’ll need:\n# Install data science packages\nconda install numpy pandas matplotlib jupyter -y\n\n# Install additional useful packages\nconda install scipy scikit-learn seaborn -y\n\n\nVerify Your Installation\nCheck that packages are installed:\n# Check Python version\npython --version\n\n# Check installed packages\nconda list"
  },
  {
    "objectID": "posts/2026/initial_install.html#part-4-running-python",
    "href": "posts/2026/initial_install.html#part-4-running-python",
    "title": "Setting Up Your Environment",
    "section": "Part 4: Running Python",
    "text": "Part 4: Running Python\n\nMethod 1: Python Interactive Shell\nWith your environment activated, simply type:\npython\nYou’ll see the Python prompt &gt;&gt;&gt;. Try some commands:\nprint(\"Hello, Python!\")\nx = 5 + 3\nprint(x)\nexit()\n\n\nMethod 2: Running Python Scripts\n\nCreate a file called test.py with this content:\n\n# test.py\nprint(\"This is my first Python script!\")\n\n# Do some calculation\nnumbers = [1, 2, 3, 4, 5]\ntotal = sum(numbers)\nprint(f\"The sum is: {total}\")\n\nRun it from the terminal:\n\npython test.py\n\n\nMethod 3: Jupyter Notebook\nJupyter notebooks are interactive documents that mix code, text, and visualizations.\n\nStart Jupyter:\n\njupyter notebook\n\nYour browser will open automatically\nClick “New” → “Python 3” to create a new notebook\nTry this in a cell:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.title(\"My First Plot\")\nplt.show()\n\nPress Shift + Enter to run the cell"
  },
  {
    "objectID": "posts/2026/initial_install.html#part-5-essential-commands-reference",
    "href": "posts/2026/initial_install.html#part-5-essential-commands-reference",
    "title": "Setting Up Your Environment",
    "section": "Part 5: Essential Commands Reference",
    "text": "Part 5: Essential Commands Reference\n\nConda Environment Commands\n# List all environments\nconda env list\n\n# Create new environment\nconda create -n myenv python=3.11\n\n# Activate environment\nconda activate myenv\n\n# Deactivate environment\nconda deactivate\n\n# Delete environment\nconda env remove -n myenv\n\n# Export environment (for sharing)\nconda env export &gt; environment.yml\n\n# Create environment from file\nconda env create -f environment.yml\n\n\nPackage Management Commands\n# Install a package\nconda install package_name\n\n# Install specific version\nconda install package_name=1.2.3\n\n# Update a package\nconda update package_name\n\n# Remove a package\nconda remove package_name\n\n# List installed packages\nconda list\n\n# Search for a package\nconda search package_name\n\n\nJupyter Commands\n# Start Jupyter Notebook\njupyter notebook\n\n# Start Jupyter Lab (more advanced interface)\njupyter lab\n\n# List running notebooks\njupyter notebook list\n\n# Stop all notebooks\njupyter notebook stop"
  },
  {
    "objectID": "posts/2026/initial_install.html#part-6-practice-exercise",
    "href": "posts/2026/initial_install.html#part-6-practice-exercise",
    "title": "Setting Up Your Environment",
    "section": "Part 6: Practice Exercise",
    "text": "Part 6: Practice Exercise\nLet’s put everything together! Complete this exercise to confirm your setup is working.\n\nExercise: Create and Run Your First Python Script\n\nMake sure your course_env is activated\nCreate a new file called hello.py\nCopy this code into the file:\n\n# hello.py - My first Python script\n\n# Print a welcome message\nprint(\"Welcome to Python Programming!\")\nprint(\"=\" * 40)\n\n# Basic arithmetic\nprint(\"\\nLet's do some basic math:\")\na = 10\nb = 5\nprint(f\"{a} + {b} = {a + b}\")\nprint(f\"{a} - {b} = {a - b}\")\nprint(f\"{a} * {b} = {a * b}\")\nprint(f\"{a} / {b} = {a / b}\")\n\n# Working with lists\nprint(\"\\nWorking with a list of numbers:\")\nnumbers = [1, 2, 3, 4, 5]\nprint(f\"My numbers: {numbers}\")\nprint(f\"Sum of numbers: {sum(numbers)}\")\nprint(f\"Largest number: {max(numbers)}\")\n\n# Your turn - add your name\nyour_name = \"Your Name Here\"  # Change this to your actual name\nprint(f\"\\nThis script was created by: {your_name}\")\nprint(\"\\nCongratulations! Your Python environment is working correctly!\")\n\nBefore running, change “Your Name Here” to your actual name\nRun the script:\n\npython hello.py\n\nYou should see output showing the calculations and your name\n\nExpected Output:\nWelcome to Python Programming!\n========================================\n\nLet's do some basic math:\n10 + 5 = 15\n10 - 5 = 5\n10 * 5 = 50\n10 / 5 = 2.0\n\nWorking with a list of numbers:\nMy numbers: [1, 2, 3, 4, 5]\nSum of numbers: 15\nLargest number: 5\n\nThis script was created by: [Your Name]\n\nCongratulations! Your Python environment is working correctly!\nCongratulations! You’ve successfully set up your Python environment and run your first Python script."
  },
  {
    "objectID": "posts/2026/initial_install.html#troubleshooting-common-issues",
    "href": "posts/2026/initial_install.html#troubleshooting-common-issues",
    "title": "Setting Up Your Environment",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\nIssue: “conda: command not found”\nSolution: Anaconda is not in your PATH. Go back to Part 2 and set up environment variables.\n\n\nIssue: “Permission denied” errors on macOS/Linux\nSolution: Add sudo before the command, or check file permissions with chmod.\n\n\nIssue: Packages fail to install\nSolution: Try updating conda first:\nconda update conda\nconda update --all\n\n\nIssue: Jupyter won’t start\nSolution: Make sure jupyter is installed in your active environment:\nconda install jupyter -y\n\n\nIssue: Environment activation doesn’t work\nSolution: - Windows: Use Anaconda Prompt instead of regular Command Prompt - Mac/Linux: Make sure you’ve run conda init after installation"
  },
  {
    "objectID": "posts/2026/initial_install.html#preparation-for-next-lab",
    "href": "posts/2026/initial_install.html#preparation-for-next-lab",
    "title": "Setting Up Your Environment",
    "section": "Preparation for Next Lab",
    "text": "Preparation for Next Lab\nBefore the next lab session:\n\nPractice creating a new environment called practice_env\nInstall numpy and matplotlib packages in it\nCreate a simple Python script that prints your name and the current date\nExplore the Jupyter notebook interface and try creating cells with markdown text"
  },
  {
    "objectID": "posts/2026/initial_install.html#assignment-submission-setup",
    "href": "posts/2026/initial_install.html#assignment-submission-setup",
    "title": "Setting Up Your Environment",
    "section": "Assignment Submission Setup",
    "text": "Assignment Submission Setup\nFor course assignments:\n\nAlways activate course_env before starting work\nOrganize your work in folders by week/assignment\nUse Jupyter notebooks for assignments that require visualization\nUse .py scripts for pure code assignments\nInclude comments in your code explaining what each section does"
  },
  {
    "objectID": "posts/2026/initial_install.html#quick-start-checklist",
    "href": "posts/2026/initial_install.html#quick-start-checklist",
    "title": "Setting Up Your Environment",
    "section": "Quick Start Checklist",
    "text": "Quick Start Checklist\nUse this checklist at the start of each assignment:\n\nOpen terminal/command prompt\nNavigate to your assignment folder: cd path/to/assignment\nActivate environment: conda activate course_env\nStart Jupyter (if needed): jupyter notebook\nBegin working on your assignment"
  },
  {
    "objectID": "posts/2026/initial_install.html#additional-resources",
    "href": "posts/2026/initial_install.html#additional-resources",
    "title": "Setting Up Your Environment",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nAnaconda Documentation\nConda Cheat Sheet\nJupyter Notebook Documentation\nPython Official Tutorial"
  },
  {
    "objectID": "posts/2026/initial_install.html#questions",
    "href": "posts/2026/initial_install.html#questions",
    "title": "Setting Up Your Environment",
    "section": "Questions?",
    "text": "Questions?\nIf you encounter any issues during the lab, raise your hand and ask for help. Make sure you have a working setup before leaving today, as you’ll need it for all future assignments.\n\nLab Complete! You’re now ready to start programming in Python for this course."
  },
  {
    "objectID": "posts/2026/initial_install.html#understanding-the-basics",
    "href": "posts/2026/initial_install.html#understanding-the-basics",
    "title": "Setting Up Your Python Development Environment",
    "section": "Understanding the Basics",
    "text": "Understanding the Basics\nBefore we start installing software, let’s understand some key concepts that will help you throughout this course.\n\nWhat are Virtual Environments?\nThink of a virtual environment as a separate workspace for each of your projects. Imagine you have two different projects:\n\nProject A needs Python 3.9 and version 1.0 of a package called “data-tools”\nProject B needs Python 3.11 and version 2.0 of the same “data-tools” package\n\nWithout virtual environments, you could only have one version installed at a time, and switching between projects would be a nightmare. Virtual environments solve this problem by creating isolated spaces where each project can have its own Python version and packages without interfering with each other.\nKey benefits of virtual environments:\n\nKeep different projects separate and organized\nAvoid conflicts between package versions\nMake it easy to share your project setup with others\nPrevent accidentally breaking one project while working on another\n\n\n\nPython Scripts vs Jupyter Notebooks\nPython code can be written and executed in different ways. The two most common formats you’ll use are:\nPython Scripts (.py files):\nPython scripts are plain text files containing Python code, saved with a .py extension. They are:\n\nSimple text files you can edit in any text editor\nRun from start to finish in one go\nGood for automation, data processing, and final production code\nExample: my_program.py\n\nWhen you run a script, Python executes all the code from top to bottom and then exits.\nJupyter Notebooks (.ipynb files):\nJupyter Notebooks are interactive documents that combine code, text, and visualizations. They are:\n\nDivided into “cells” that can be run independently\nInteractive - you can run code, see results immediately, and modify your approach\nGreat for data exploration, learning, and creating reports\nCan include formatted text, equations, images, and plots alongside your code\nExample: data_analysis.ipynb\n\nIn a notebook, you can run one cell at a time, experiment with different approaches, and see the results right away. This makes them perfect for learning and data analysis.\nWhen to use each:\n\nUse scripts for: homework assignments that just need final code, automated tasks, programs that run repeatedly\nUse notebooks for: exploring data, learning new concepts, creating reports with visualizations, step-by-step analysis\n\nIn this course, you may use both depending on the assignment requirements."
  },
  {
    "objectID": "posts/2026/numpy_intro.html",
    "href": "posts/2026/numpy_intro.html",
    "title": "NumPy Tutorial",
    "section": "",
    "text": "NumPy stands for Numerical Python. Think of it as a super-powered calculator for Python that can work with large amounts of numbers very quickly.\nWhy do we need NumPy?\n\nPython lists are slow when working with lots of numbers\nNumPy is much faster (sometimes 100x faster!)\nMakes mathematical operations easy\nUsed in Data Science, Machine Learning, and AI\n\n\n\n\nOpen your terminal or command prompt and type:\npip install numpy\nThat’s it! NumPy is now installed."
  },
  {
    "objectID": "posts/2026/numpy_intro.html#what-is-numpy",
    "href": "posts/2026/numpy_intro.html#what-is-numpy",
    "title": "NumPy Tutorial",
    "section": "",
    "text": "NumPy stands for Numerical Python. Think of it as a super-powered calculator for Python that can work with large amounts of numbers very quickly.\nWhy do we need NumPy?\n\nPython lists are slow when working with lots of numbers\nNumPy is much faster (sometimes 100x faster!)\nMakes mathematical operations easy\nUsed in Data Science, Machine Learning, and AI"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#installing-numpy",
    "href": "posts/2026/numpy_intro.html#installing-numpy",
    "title": "NumPy Tutorial",
    "section": "",
    "text": "Open your terminal or command prompt and type:\npip install numpy\nThat’s it! NumPy is now installed."
  },
  {
    "objectID": "posts/2026/numpy_intro.html#what-is-an-array",
    "href": "posts/2026/numpy_intro.html#what-is-an-array",
    "title": "NumPy Tutorial",
    "section": "What is an Array?",
    "text": "What is an Array?\nAn array is like a list, but designed specifically for numbers. Let’s see how to create them:\n# First, we need to import NumPy\n# 'np' is a short name we use instead of typing 'numpy' every time\nimport numpy as np\n\n# Check if NumPy is installed correctly\nprint(\"NumPy version:\", np.__version__)\nOutput:\nNumPy version: 1.24.3"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#creating-arrays-from-lists",
    "href": "posts/2026/numpy_intro.html#creating-arrays-from-lists",
    "title": "NumPy Tutorial",
    "section": "Creating Arrays from Lists",
    "text": "Creating Arrays from Lists\n# Creating a simple 1D array (like a row of numbers)\nmy_list = [1, 2, 3, 4, 5]\nmy_array = np.array(my_list)\n\nprint(\"Original list:\", my_list)\nprint(\"NumPy array:\", my_array)\nprint(\"Type:\", type(my_array))\nOutput:\nOriginal list: [1, 2, 3, 4, 5]\nNumPy array: [1 2 3 4 5]\nType: &lt;class 'numpy.ndarray'&gt;\n# Creating a 2D array (like a table or matrix)\n# This is like having multiple rows\nmy_2d_list = [[1, 2, 3], \n              [4, 5, 6], \n              [7, 8, 9]]\n\nmy_2d_array = np.array(my_2d_list)\n\nprint(\"2D Array:\")\nprint(my_2d_array)\nOutput:\n2D Array:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#other-ways-to-create-arrays",
    "href": "posts/2026/numpy_intro.html#other-ways-to-create-arrays",
    "title": "NumPy Tutorial",
    "section": "Other Ways to Create Arrays",
    "text": "Other Ways to Create Arrays\n\nCreating Arrays of Zeros\nUseful when you need to initialize an array and fill values later.\n# Array of zeros\nzeros = np.zeros(5)\nprint(\"Array of zeros:\", zeros)\n\n# 2D array of zeros\nzeros_2d = np.zeros((3, 4))  # 3 rows, 4 columns\nprint(\"2D array of zeros:\")\nprint(zeros_2d)\nOutput:\nArray of zeros: [0. 0. 0. 0. 0.]\n2D array of zeros:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n\nCreating Arrays of Ones\nSimilar to zeros, but filled with ones.\n# Array of ones\nones = np.ones(5)\nprint(\"Array of ones:\", ones)\n\n# 2D array of ones\nones_2d = np.ones((2, 3))\nprint(\"2D array of ones:\")\nprint(ones_2d)\nOutput:\nArray of ones: [1. 1. 1. 1. 1.]\n2D array of ones:\n[[1. 1. 1.]\n [1. 1. 1.]]\n\n\nCreating Arrays with Range\nSimilar to Python’s range() function, but creates a NumPy array.\n# Array of numbers in a range\nnumbers = np.arange(0, 10, 2)  # Start at 0, end before 10, step by 2\nprint(\"Numbers 0 to 10 (step 2):\", numbers)\n\n# Another example\nnumbers2 = np.arange(5, 15)  # Default step is 1\nprint(\"Numbers 5 to 14:\", numbers2)\nOutput:\nNumbers 0 to 10 (step 2): [0 2 4 6 8]\nNumbers 5 to 14: [ 5  6  7  8  9 10 11 12 13 14]\n\n\nCreating Evenly Spaced Arrays\nCreates a specified number of evenly spaced values between a start and end point.\n# Array of 5 numbers between 0 and 1\nevenly_spaced = np.linspace(0, 1, 5)\nprint(\"5 numbers from 0 to 1:\", evenly_spaced)\n\n# Array of 7 numbers between 0 and 100\nevenly_spaced2 = np.linspace(0, 100, 7)\nprint(\"7 numbers from 0 to 100:\", evenly_spaced2)\nOutput:\n5 numbers from 0 to 1: [0.   0.25 0.5  0.75 1.  ]\n7 numbers from 0 to 100: [  0.          16.66666667  33.33333333  50.          66.66666667\n  83.33333333 100.        ]\n\n\nCreating Random Arrays\nGenerate arrays with random values from a uniform distribution between 0 and 1. Each value has an equal probability of being selected. Useful for testing and simulations.\n# Array of random numbers between 0 and 1\nrandom_array = np.random.rand(5)\nprint(\"Random array (5 elements):\", random_array)\nOutput:\nRandom array (5 elements): [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548 ]\nNote: Values are drawn from a uniform distribution where every number between 0 and 1 has equal probability.\n\n\nCreating 2D Random Arrays\nCreate multi-dimensional arrays with random values from a uniform distribution between 0 and 1.\n# 2D array of random numbers\nrandom_2d = np.random.rand(3, 3)\nprint(\"Random 2D array (3x3):\")\nprint(random_2d)\nOutput:\nRandom 2D array (3x3):\n[[0.64589411 0.43758721 0.891773  ]\n [0.96366276 0.38344152 0.79172504]\n [0.52889492 0.56804456 0.92559664]]\n\n\nCreating Random Arrays from Normal Distribution\nGenerate arrays with random values from a standard normal distribution (mean = 0, standard deviation = 1). This is useful for statistical simulations and machine learning.\n# Array from standard normal distribution\nnormal_array = np.random.randn(5)\nprint(\"Normal distribution array (5 elements):\", normal_array)\n\n# 2D array from normal distribution\nnormal_2d = np.random.randn(3, 3)\nprint(\"Normal distribution 2D array (3x3):\")\nprint(normal_2d)\nOutput:\nNormal distribution array (5 elements): [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]\nNormal distribution 2D array (3x3):\n[[-0.23413696  1.57921282  0.76743473]\n [-0.46947439  0.54256004 -0.46341769]\n [-0.46572975  0.24196227 -1.91328024]]\nNote: Values are drawn from a normal (Gaussian) distribution. Most values cluster around 0, with approximately 68% of values between -1 and 1.\n\n\nCreating Random Integer Arrays\nGenerate random integers within a specified range using a discrete uniform distribution (each integer in the range has equal probability).\n# Random integers between a range\nrandom_ints = np.random.randint(1, 100, size=10)  # 10 random integers between 1 and 99\nprint(\"Random integers (1-99):\", random_ints)\n\n# Random integers in a 2D array\nrandom_ints_2d = np.random.randint(0, 10, size=(3, 4))  # 3x4 array with values 0-9\nprint(\"Random 2D integers (0-9):\")\nprint(random_ints_2d)\nOutput:\nRandom integers (1-99): [44 47 64 67 84  9 83 21 36 87]\nRandom 2D integers (0-9):\n[[5 0 3 3]\n [7 9 3 5]\n [2 4 7 6]]\nNote: randint(low, high) generates integers from low (inclusive) to high (exclusive)."
  },
  {
    "objectID": "posts/2026/numpy_intro.html#shape---finding-the-size-of-your-array",
    "href": "posts/2026/numpy_intro.html#shape---finding-the-size-of-your-array",
    "title": "NumPy Tutorial",
    "section": "Shape - Finding the Size of Your Array",
    "text": "Shape - Finding the Size of Your Array\n# Create a 2D array\narr = np.array([[1, 2, 3, 4], \n                [5, 6, 7, 8]])\n\nprint(\"Array:\")\nprint(arr)\nprint(\"\\nShape (rows, columns):\", arr.shape)\nprint(\"Total number of elements:\", arr.size)\nprint(\"Number of dimensions:\", arr.ndim)\nOutput:\nArray:\n[[1 2 3 4]\n [5 6 7 8]]\n\nShape (rows, columns): (2, 4)\nTotal number of elements: 8\nNumber of dimensions: 2\nUnderstanding shape: If shape is (2, 4), it means 2 rows and 4 columns."
  },
  {
    "objectID": "posts/2026/numpy_intro.html#reshape---changing-the-shape",
    "href": "posts/2026/numpy_intro.html#reshape---changing-the-shape",
    "title": "NumPy Tutorial",
    "section": "Reshape - Changing the Shape",
    "text": "Reshape - Changing the Shape\n# Start with a 1D array\noriginal = np.array([1, 2, 3, 4, 5, 6])\nprint(\"Original array:\", original)\nprint(\"Original shape:\", original.shape)\n\n# Reshape to 2 rows and 3 columns\nreshaped = original.reshape(2, 3)\nprint(\"\\nReshaped to 2x3:\")\nprint(reshaped)\n\n# Reshape to 3 rows and 2 columns\nreshaped2 = original.reshape(3, 2)\nprint(\"\\nReshaped to 3x2:\")\nprint(reshaped2)\nOutput:\nOriginal array: [1 2 3 4 5 6]\nOriginal shape: (6,)\n\nReshaped to 2x3:\n[[1 2 3]\n [4 5 6]]\n\nReshaped to 3x2:\n[[1 2]\n [3 4]\n [5 6]]\nImportant: Total elements must remain the same! You can’t reshape 6 elements into a 2x4 array (that needs 8 elements)."
  },
  {
    "objectID": "posts/2026/numpy_intro.html#resize---similar-to-reshape",
    "href": "posts/2026/numpy_intro.html#resize---similar-to-reshape",
    "title": "NumPy Tutorial",
    "section": "Resize - Similar to Reshape",
    "text": "Resize - Similar to Reshape\n# Resize changes the array itself\narr = np.array([1, 2, 3, 4, 5, 6])\nprint(\"Original:\", arr)\n\narr.resize(2, 3)  # This modifies the array directly\nprint(\"After resize:\")\nprint(arr)\nOutput:\nOriginal: [1 2 3 4 5 6]\nAfter resize:\n[[1 2 3]\n [4 5 6]]\nDifference between reshape and resize:\n\nreshape() creates a new array with different shape\nresize() modifies the original array"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#statistical-methods---math-made-easy",
    "href": "posts/2026/numpy_intro.html#statistical-methods---math-made-easy",
    "title": "NumPy Tutorial",
    "section": "Statistical Methods - Math Made Easy!",
    "text": "Statistical Methods - Math Made Easy!\n# Create an array of test scores\nscores = np.array([85, 90, 78, 92, 88, 95, 73, 89])\n\nprint(\"Test Scores:\", scores)\nprint(\"\\nMean (Average):\", np.mean(scores))\nprint(\"Standard Deviation:\", np.std(scores))\nprint(\"Minimum Score:\", np.min(scores))\nprint(\"Maximum Score:\", np.max(scores))\nprint(\"Sum of all scores:\", np.sum(scores))\nOutput:\nTest Scores: [85 90 78 92 88 95 73 89]\n\nMean (Average): 86.25\nStandard Deviation: 6.89\nMinimum Score: 73\nMaximum Score: 95\nSum of all scores: 690\nWhat is Standard Deviation? It tells us how spread out the numbers are.\n\nSmall standard deviation = numbers are close together\nLarge standard deviation = numbers are spread apart"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#data-types-dtype",
    "href": "posts/2026/numpy_intro.html#data-types-dtype",
    "title": "NumPy Tutorial",
    "section": "Data Types (dtype)",
    "text": "Data Types (dtype)\n# NumPy automatically detects the data type\nint_array = np.array([1, 2, 3])\nprint(\"Integer array:\", int_array)\nprint(\"Data type:\", int_array.dtype)\n\nfloat_array = np.array([1.5, 2.7, 3.9])\nprint(\"\\nFloat array:\", float_array)\nprint(\"Data type:\", float_array.dtype)\n\n# You can force a specific type\nforced_float = np.array([1, 2, 3], dtype=np.float64)\nprint(\"\\nForced to float:\", forced_float)\nprint(\"Data type:\", forced_float.dtype)\nOutput:\nInteger array: [1 2 3]\nData type: int64\n\nFloat array: [1.5 2.7 3.9]\nData type: float64\n\nForced to float: [1. 2. 3.]\nData type: float64"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#difference-1-adding-two-collections",
    "href": "posts/2026/numpy_intro.html#difference-1-adding-two-collections",
    "title": "NumPy Tutorial",
    "section": "Difference 1: Adding Two Collections",
    "text": "Difference 1: Adding Two Collections\n# Python Lists: + means CONCATENATE (join together)\nlist1 = [1, 2, 3]\nlist2 = [4, 5, 6]\nresult_list = list1 + list2\nprint(\"List + List:\", result_list)\n\n# NumPy Arrays: + means ADD element by element\narr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\nresult_array = arr1 + arr2\nprint(\"Array + Array:\", result_array)\nOutput:\nList + List: [1, 2, 3, 4, 5, 6]\nArray + Array: [5 7 9]\nSee the difference?\n\nLists: [1,2,3] + [4,5,6] = [1,2,3,4,5,6] (joined together)\nArrays: [1,2,3] + [4,5,6] = [5,7,9] (added element-wise)"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#difference-2-multiplying-by-a-number",
    "href": "posts/2026/numpy_intro.html#difference-2-multiplying-by-a-number",
    "title": "NumPy Tutorial",
    "section": "Difference 2: Multiplying by a Number",
    "text": "Difference 2: Multiplying by a Number\n# Python Lists: * means REPEAT\nmy_list = [1, 2, 3]\nresult_list = my_list * 3\nprint(\"List * 3:\", result_list)\n\n# NumPy Arrays: * means MULTIPLY each element\nmy_array = np.array([1, 2, 3])\nresult_array = my_array * 3\nprint(\"Array * 3:\", result_array)\nOutput:\nList * 3: [1, 2, 3, 1, 2, 3, 1, 2, 3]\nArray * 3: [3 6 9]\nSee the difference?\n\nLists: [1,2,3] * 3 = [1,2,3,1,2,3,1,2,3] (repeated)\nArrays: [1,2,3] * 3 = [3,6,9] (each element multiplied)"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#python-lists---scattered-storage",
    "href": "posts/2026/numpy_intro.html#python-lists---scattered-storage",
    "title": "NumPy Tutorial",
    "section": "Python Lists - Scattered Storage",
    "text": "Python Lists - Scattered Storage\nimport sys\n\n# Create a list\nmy_list = [10, 20, 30, 40, 50]\n\n# Size of the list structure\nlist_size = sys.getsizeof(my_list)\nprint(f\"Size of list structure: {list_size} bytes\")\n\n# Size of one integer object\none_int_size = sys.getsizeof(my_list[0])\nprint(f\"Size of one integer: {one_int_size} bytes\")\n\n# Total approximate size\ntotal_size = list_size + (one_int_size * len(my_list))\nprint(f\"Total approximate size: {total_size} bytes\")\nOutput:\nSize of list structure: 120 bytes\nSize of one integer: 28 bytes\nTotal approximate size: 260 bytes\nHow Lists Store Data:\nList: [pointer] -&gt; [Integer Object 10]\n      [pointer] -&gt; [Integer Object 20]\n      [pointer] -&gt; [Integer Object 30]\n      ...\nEach number is a separate object in memory!"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#numpy-arrays---compact-storage",
    "href": "posts/2026/numpy_intro.html#numpy-arrays---compact-storage",
    "title": "NumPy Tutorial",
    "section": "NumPy Arrays - Compact Storage",
    "text": "NumPy Arrays - Compact Storage\n# Create equivalent NumPy array\nmy_array = np.array([10, 20, 30, 40, 50])\n\n# Size in bytes\narray_size = my_array.nbytes\nprint(f\"Size of entire array: {array_size} bytes\")\nprint(f\"Size per element: {my_array.itemsize} bytes\")\nprint(f\"Total elements: {my_array.size}\")\nOutput:\nSize of entire array: 40 bytes\nSize per element: 8 bytes\nTotal elements: 5\nHow Arrays Store Data:\nArray: [10][20][30][40][50]  (one continuous block)\nAll numbers are stored together in one block!"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#why-numpy-uses-less-memory",
    "href": "posts/2026/numpy_intro.html#why-numpy-uses-less-memory",
    "title": "NumPy Tutorial",
    "section": "Why NumPy Uses Less Memory",
    "text": "Why NumPy Uses Less Memory\n# Compare for larger data\nsize = 1000\nbig_list = list(range(size))\nbig_array = np.array(range(size))\n\n# List structure size\nlist_structure_size = sys.getsizeof(big_list)\n\n# Size of integer objects (sample first 100 and estimate)\nsample_int_size = sum(sys.getsizeof(big_list[i]) for i in range(min(100, size)))\navg_int_size = sample_int_size / min(100, size)\ntotal_int_size = avg_int_size * size\n\n# Total list size (structure + all integer objects)\ntotal_list_size = list_structure_size + total_int_size\n\nprint(f\"List structure size: {list_structure_size} bytes\")\nprint(f\"Average integer object size: {avg_int_size:.0f} bytes\")\nprint(f\"Total integer objects size: {total_int_size:.0f} bytes\")\nprint(f\"Total list size (structure + objects): {total_list_size:.0f} bytes\")\nprint(f\"\\nNumPy array size: {big_array.nbytes} bytes\")\nprint(f\"\\nNumPy uses approximately {total_list_size / big_array.nbytes:.1f}x less memory!\")\nOutput:\nList structure size: 8056 bytes\nAverage integer object size: 28 bytes\nTotal integer objects size: 27960 bytes\nTotal list size (structure + objects): 36016 bytes\n\nNumPy array size: 8000 bytes\n\nNumPy uses approximately 4.5x less memory!"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#matrix-multiplication-with-lists-slow-way",
    "href": "posts/2026/numpy_intro.html#matrix-multiplication-with-lists-slow-way",
    "title": "NumPy Tutorial",
    "section": "Matrix Multiplication with Lists (Slow Way)",
    "text": "Matrix Multiplication with Lists (Slow Way)\ndef multiply_matrices_with_lists(A, B):\n    \"\"\"Multiply two matrices using Python lists\"\"\"\n    rows_A = len(A)\n    cols_A = len(A[0])\n    cols_B = len(B[0])\n    \n    # Create result matrix filled with zeros\n    result = []\n    for i in range(rows_A):\n        row = []\n        for j in range(cols_B):\n            row.append(0)\n        result.append(row)\n    \n    # Perform multiplication\n    for i in range(rows_A):\n        for j in range(cols_B):\n            for k in range(cols_A):\n                result[i][j] += A[i][k] * B[k][j]\n    \n    return result\n\n# Test with small example\nA_list = [[1, 2], [3, 4]]\nB_list = [[5, 6], [7, 8]]\n\nresult = multiply_matrices_with_lists(A_list, B_list)\nprint(\"Result of matrix multiplication:\")\nfor row in result:\n    print(row)\nOutput:\nResult of matrix multiplication:\n[19, 22]\n[43, 50]"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#matrix-multiplication-with-numpy-fast-way",
    "href": "posts/2026/numpy_intro.html#matrix-multiplication-with-numpy-fast-way",
    "title": "NumPy Tutorial",
    "section": "Matrix Multiplication with NumPy (Fast Way)",
    "text": "Matrix Multiplication with NumPy (Fast Way)\n# Same multiplication with NumPy\nA_array = np.array([[1, 2], [3, 4]])\nB_array = np.array([[5, 6], [7, 8]])\n\nresult_np = np.dot(A_array, B_array)\nprint(\"Result of matrix multiplication:\")\nprint(result_np)\nOutput:\nResult of matrix multiplication:\n[[19 22]\n [43 50]]"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#performance-comparison",
    "href": "posts/2026/numpy_intro.html#performance-comparison",
    "title": "NumPy Tutorial",
    "section": "Performance Comparison",
    "text": "Performance Comparison\nimport time\n\n# Create larger matrices for timing\nsize = 100\n\n# Create random matrices\nlist_A = [[float(i+j) for j in range(size)] for i in range(size)]\nlist_B = [[float(i-j) for j in range(size)] for i in range(size)]\n\nnp_A = np.array(list_A)\nnp_B = np.array(list_B)\n\n# Number of runs for averaging\nnum_runs = 500\n\n# Time the list version (multiple runs)\nlist_times = []\nfor _ in range(num_runs):\n    start_time = time.time()\n    result_list = multiply_matrices_with_lists(list_A, list_B)\n    list_times.append(time.time() - start_time)\n\navg_list_time = sum(list_times) / num_runs\n\n# Time the NumPy version (multiple runs)\nnumpy_times = []\nfor _ in range(num_runs):\n    start_time = time.time()\n    result_np = np.dot(np_A, np_B)\n    numpy_times.append(time.time() - start_time)\n\navg_numpy_time = sum(numpy_times) / num_runs\n\nprint(f\"Matrix size: {size} x {size}\")\nprint(f\"Number of runs: {num_runs}\")\nprint(f\"\\nPython Lists:\")\nprint(f\"  Average time: {avg_list_time:.6f} seconds\")\nprint(f\"  Min time: {min(list_times):.6f} seconds\")\nprint(f\"  Max time: {max(list_times):.6f} seconds\")\n\nprint(f\"\\nNumPy:\")\nprint(f\"  Average time: {avg_numpy_time:.6f} seconds\")\nprint(f\"  Min time: {min(numpy_times):.6f} seconds\")\nprint(f\"  Max time: {max(numpy_times):.6f} seconds\")\n\nprint(f\"\\nNumPy is {avg_list_time/avg_numpy_time:.1f}x FASTER!\")\nExpected Output:\nMatrix size: 100 x 100\nNumber of runs: 500\n\nPython Lists:\n  Average time: 2.345678 seconds\n  Min time: 2.320145 seconds\n  Max time: 2.378923 seconds\n\nNumPy:\n  Average time: 0.003401 seconds\n  Min time: 0.003201 seconds\n  Max time: 0.003789 seconds\n\nNumPy is 689.9x FASTER!"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#use-numpy-arrays-when",
    "href": "posts/2026/numpy_intro.html#use-numpy-arrays-when",
    "title": "NumPy Tutorial",
    "section": "Use NumPy Arrays when:",
    "text": "Use NumPy Arrays when:\n\nWorking with numbers and math\nNeed speed and efficiency\nDoing calculations on large data\nWorking with matrices\nDoing scientific computing"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#use-python-lists-when",
    "href": "posts/2026/numpy_intro.html#use-python-lists-when",
    "title": "NumPy Tutorial",
    "section": "Use Python Lists when:",
    "text": "Use Python Lists when:\n\nNeed different types of data together\nSmall amount of data\nNeed to frequently add/remove items\nDon’t need mathematical operations"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#element-wise-multiplication-vs-matrix-multiplication",
    "href": "posts/2026/numpy_intro.html#element-wise-multiplication-vs-matrix-multiplication",
    "title": "NumPy Tutorial",
    "section": "Element-wise Multiplication vs Matrix Multiplication",
    "text": "Element-wise Multiplication vs Matrix Multiplication\nNumPy supports two types of multiplication:\n\nElement-wise Multiplication (using *)\n# Element-wise multiplication\narr1 = np.array([[1, 2], [3, 4]])\narr2 = np.array([[5, 6], [7, 8]])\n\nresult = arr1 * arr2\nprint(\"Element-wise multiplication:\")\nprint(result)\nOutput:\nElement-wise multiplication:\n[[ 5 12]\n [21 32]]\nExplanation: Each element is multiplied with the corresponding element in the same position. - 1 * 5 = 5 - 2 * 6 = 12 - 3 * 7 = 21 - 4 * 8 = 32\n\n\nMatrix Multiplication (using np.dot or @)\n# Matrix multiplication\narr1 = np.array([[1, 2], [3, 4]])\narr2 = np.array([[5, 6], [7, 8]])\n\n# Method 1: Using np.dot()\nresult1 = np.dot(arr1, arr2)\nprint(\"Matrix multiplication using np.dot():\")\nprint(result1)\n\n# Method 2: Using @ operator\nresult2 = arr1 @ arr2\nprint(\"\\nMatrix multiplication using @ operator:\")\nprint(result2)\nOutput:\nMatrix multiplication using np.dot():\n[[19 22]\n [43 50]]\n\nMatrix multiplication using @ operator:\n[[19 22]\n [43 50]]\nExplanation: This is proper matrix multiplication from linear algebra.\nFor the first element (row 1, col 1): - (1 * 5) + (2 * 7) = 5 + 14 = 19\nFor the second element (row 1, col 2): - (1 * 6) + (2 * 8) = 6 + 16 = 22\nFor the third element (row 2, col 1): - (3 * 5) + (4 * 7) = 15 + 28 = 43\nFor the fourth element (row 2, col 2): - (3 * 6) + (4 * 8) = 18 + 32 = 50\n\n\nQuick Comparison\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nprint(\"Original matrices:\")\nprint(\"A =\")\nprint(A)\nprint(\"\\nB =\")\nprint(B)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Element-wise multiplication (A * B):\")\nprint(A * B)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Matrix multiplication (A @ B):\")\nprint(A @ B)\nOutput:\nOriginal matrices:\nA =\n[[1 2]\n [3 4]]\n\nB =\n[[5 6]\n [7 8]]\n\n==================================================\nElement-wise multiplication (A * B):\n[[ 5 12]\n [21 32]]\n\n==================================================\nMatrix multiplication (A @ B):\n[[19 22]\n [43 50]]\nKey Differences:\n\n\n\n\n\n\n\n\nOperation\nSymbol\nWhat it does\n\n\n\n\nElement-wise\n*\nMultiplies corresponding elements\n\n\nMatrix multiplication\n@ or np.dot()\nProper linear algebra matrix multiplication"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#working-with-unique-values",
    "href": "posts/2026/numpy_intro.html#working-with-unique-values",
    "title": "NumPy Tutorial",
    "section": "Working with Unique Values",
    "text": "Working with Unique Values\n# Create an array with duplicate values\ndata = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5])\nprint(\"Original array:\", data)\n\n# Get unique values\nunique_values = np.unique(data)\nprint(\"Unique values:\", unique_values)\n\n# Count occurrences of each unique value\nunique_vals, counts = np.unique(data, return_counts=True)\nprint(\"\\nValue counts:\")\nfor val, count in zip(unique_vals, counts):\n    print(f\"  {val}: appears {count} times\")\nOutput:\nOriginal array: [1 2 2 3 3 3 4 4 4 4 5 5 5 5 5]\nUnique values: [1 2 3 4 5]\n\nValue counts:\n  1: appears 1 times\n  2: appears 2 times\n  3: appears 3 times\n  4: appears 4 times\n  5: appears 5 times\nPractical Example:\n# Student IDs with duplicates (some students enrolled in multiple courses)\nstudent_ids = np.array([101, 102, 103, 101, 104, 102, 105, 103, 101])\nprint(\"All enrollments:\", student_ids)\n\n# Find unique students\nunique_students = np.unique(student_ids)\nprint(\"Unique students:\", unique_students)\nprint(\"Total unique students:\", len(unique_students))\nOutput:\nAll enrollments: [101 102 103 101 104 102 105 103 101]\nUnique students: [101 102 103 104 105]\nTotal unique students: 5"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-1-array-creation",
    "href": "posts/2026/numpy_intro.html#question-1-array-creation",
    "title": "NumPy Tutorial",
    "section": "Question 1: Array Creation",
    "text": "Question 1: Array Creation\nCreate a 1D NumPy array containing the first 20 even numbers (2, 4, 6, …, 40).\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-2-2d-array-and-shape",
    "href": "posts/2026/numpy_intro.html#question-2-2d-array-and-shape",
    "title": "NumPy Tutorial",
    "section": "Question 2: 2D Array and Shape",
    "text": "Question 2: 2D Array and Shape\nCreate a 2D array of shape (4, 5) filled with random integers between 10 and 50. Print the shape, size, and number of dimensions.\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-3-statistical-analysis",
    "href": "posts/2026/numpy_intro.html#question-3-statistical-analysis",
    "title": "NumPy Tutorial",
    "section": "Question 3: Statistical Analysis",
    "text": "Question 3: Statistical Analysis\nCreate an array of 100 random numbers from a normal distribution. Calculate and print: - Mean - Standard deviation - Minimum value - Maximum value\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-4-array-reshaping",
    "href": "posts/2026/numpy_intro.html#question-4-array-reshaping",
    "title": "NumPy Tutorial",
    "section": "Question 4: Array Reshaping",
    "text": "Question 4: Array Reshaping\nCreate a 1D array with numbers from 1 to 24. Reshape it into: - A 2D array of shape (4, 6) - A 2D array of shape (6, 4) - A 3D array of shape (2, 3, 4)\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-5-element-wise-operations",
    "href": "posts/2026/numpy_intro.html#question-5-element-wise-operations",
    "title": "NumPy Tutorial",
    "section": "Question 5: Element-wise Operations",
    "text": "Question 5: Element-wise Operations\nCreate two arrays: one with [1, 2, 3, 4, 5] and another with [10, 20, 30, 40, 50]. Perform: - Element-wise addition - Element-wise subtraction - Element-wise multiplication - Element-wise division\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-6-finding-unique-values",
    "href": "posts/2026/numpy_intro.html#question-6-finding-unique-values",
    "title": "NumPy Tutorial",
    "section": "Question 6: Finding Unique Values",
    "text": "Question 6: Finding Unique Values\nCreate an array with the following values: [5, 2, 8, 2, 9, 5, 3, 8, 5, 1]. Find: - All unique values - How many times each unique value appears\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-7-array-slicing",
    "href": "posts/2026/numpy_intro.html#question-7-array-slicing",
    "title": "NumPy Tutorial",
    "section": "Question 7: Array Slicing",
    "text": "Question 7: Array Slicing\nCreate a 5x5 array with random integers between 1 and 100. Extract: - The first row - The last column - A 2x2 sub-array from the center - All elements greater than 50\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-8-matrix-multiplication",
    "href": "posts/2026/numpy_intro.html#question-8-matrix-multiplication",
    "title": "NumPy Tutorial",
    "section": "Question 8: Matrix Multiplication",
    "text": "Question 8: Matrix Multiplication\nCreate two 3x3 matrices with random integers between 1 and 10. Perform: - Element-wise multiplication - Matrix multiplication (using both np.dot() and @ operator) - Compare the results\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-9-array-comparison",
    "href": "posts/2026/numpy_intro.html#question-9-array-comparison",
    "title": "NumPy Tutorial",
    "section": "Question 9: Array Comparison",
    "text": "Question 9: Array Comparison\nCreate two arrays of shape (3, 4) with random integers between 1 and 20. Compare them to find: - Elements where array1 is greater than array2 - Elements where both arrays have the same value - The total count of elements where array1 &gt; array2\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#question-10-real-world-application",
    "href": "posts/2026/numpy_intro.html#question-10-real-world-application",
    "title": "NumPy Tutorial",
    "section": "Question 10: Real-world Application",
    "text": "Question 10: Real-world Application\nYou have test scores of 50 students stored in an array. The scores are: Create an array with random integers between 40 and 100 (representing scores). Calculate: - Class average - How many students scored above 75 - How many students failed (score &lt; 50) - The percentage of students who passed\n# Write your code here in your notebook"
  },
  {
    "objectID": "posts/2026/numpy_intro.html#indexing-and-slicing-arrays",
    "href": "posts/2026/numpy_intro.html#indexing-and-slicing-arrays",
    "title": "NumPy Tutorial",
    "section": "Indexing and Slicing Arrays",
    "text": "Indexing and Slicing Arrays\n\nBasic Indexing (1D Arrays)\nAccessing individual elements in a NumPy array works similarly to Python lists, using zero-based indexing.\nAccessing the first element:\narr = np.array([10, 20, 30, 40, 50, 60])\nprint(\"Array:\", arr)\nprint(\"First element:\", arr[0])\nOutput:\nArray: [10 20 30 40 50 60]\nFirst element: 10\nAccessing the third element:\narr = np.array([10, 20, 30, 40, 50, 60])\nprint(\"Third element:\", arr[2])\nOutput:\nThird element: 30\nAccessing from the end (negative indexing):\narr = np.array([10, 20, 30, 40, 50, 60])\nprint(\"Last element:\", arr[-1])\nprint(\"Second to last:\", arr[-2])\nOutput:\nLast element: 60\nSecond to last: 50\n\n\nSlicing (1D Arrays)\nExtract a portion of an array using the syntax array[start:stop:step].\nBasic slicing - extract elements from index 2 to 5:\narr = np.array([10, 20, 30, 40, 50, 60, 70, 80])\nprint(\"Original array:\", arr)\nprint(\"Elements from index 2 to 5:\", arr[2:6])  # 6 is exclusive\nOutput:\nOriginal array: [10 20 30 40 50 60 70 80]\nElements from index 2 to 5: [30 40 50 60]\nSlicing from the start:\narr = np.array([10, 20, 30, 40, 50, 60, 70, 80])\nprint(\"First 4 elements:\", arr[:4])\nOutput:\nFirst 4 elements: [10 20 30 40]\nSlicing to the end:\narr = np.array([10, 20, 30, 40, 50, 60, 70, 80])\nprint(\"Elements from index 3 to end:\", arr[3:])\nOutput:\nElements from index 3 to end: [40 50 60 70 80]\nUsing step to skip elements:\narr = np.array([10, 20, 30, 40, 50, 60, 70, 80])\nprint(\"Every second element:\", arr[::2])\nOutput:\nEvery second element: [10 30 50 70]\nReversing an array:\narr = np.array([10, 20, 30, 40, 50, 60, 70, 80])\nprint(\"Reverse the array:\", arr[::-1])\nOutput:\nReverse the array: [80 70 60 50 40 30 20 10]\n\n\nIndexing in 2D Arrays\nFor 2D arrays, use array[row, column] syntax.\nCreating a 2D array:\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12]])\nprint(\"2D Array:\")\nprint(arr_2d)\nOutput:\n2D Array:\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\nAccessing element at row 0, column 2:\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12]])\nprint(\"Element at row 0, column 2:\", arr_2d[0, 2])\nOutput:\nElement at row 0, column 2: 3\nAccessing element at row 2, column 3:\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12]])\nprint(\"Element at row 2, column 3:\", arr_2d[2, 3])\nOutput:\nElement at row 2, column 3: 12\nUsing negative indexing (last row, last column):\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12]])\nprint(\"Last row, last column:\", arr_2d[-1, -1])\nOutput:\nLast row, last column: 12\n\n\nSlicing in 2D Arrays\nExtract rows, columns, or sub-arrays from 2D arrays.\nCreating a 2D array for slicing examples:\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12],\n                   [13, 14, 15, 16]])\nprint(\"Original 2D Array:\")\nprint(arr_2d)\nOutput:\nOriginal 2D Array:\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]\n [13 14 15 16]]\nExtracting an entire row:\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12],\n                   [13, 14, 15, 16]])\nprint(\"First row:\", arr_2d[0, :])\nprint(\"Second row:\", arr_2d[1, :])\nOutput:\nFirst row: [1 2 3 4]\nSecond row: [5 6 7 8]\nExtracting an entire column:\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12],\n                   [13, 14, 15, 16]])\nprint(\"First column:\", arr_2d[:, 0])\nprint(\"Third column:\", arr_2d[:, 2])\nOutput:\nFirst column: [ 1  5  9 13]\nThird column: [ 3  7 11 15]\nExtracting a sub-array (first 2 rows, first 3 columns):\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12],\n                   [13, 14, 15, 16]])\nprint(\"First 2 rows, first 3 columns:\")\nprint(arr_2d[0:2, 0:3])\nOutput:\nFirst 2 rows, first 3 columns:\n[[1 2 3]\n [5 6 7]]\nExtracting specific rows and columns:\narr_2d = np.array([[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12],\n                   [13, 14, 15, 16]])\nprint(\"Rows 1-2, Columns 2-3:\")\nprint(arr_2d[1:3, 2:4])\nOutput:\nRows 1-2, Columns 2-3:\n[[ 7  8]\n [11 12]]\n\n\nBoolean Indexing (Conditional Selection)\nSelect elements based on conditions.\nSelecting elements greater than 40:\narr = np.array([10, 25, 30, 45, 50, 65, 70, 85])\nprint(\"Original array:\", arr)\nprint(\"Elements &gt; 40:\", arr[arr &gt; 40])\nOutput:\nOriginal array: [10 25 30 45 50 65 70 85]\nElements &gt; 40: [45 50 65 70 85]\nSelecting elements within a range:\narr = np.array([10, 25, 30, 45, 50, 65, 70, 85])\nprint(\"Original array:\", arr)\nprint(\"Elements between 20 and 60:\", arr[(arr &gt;= 20) & (arr &lt;= 60)])\nOutput:\nOriginal array: [10 25 30 45 50 65 70 85]\nElements between 20 and 60: [25 30 45 50]\nSelecting even numbers:\narr = np.array([10, 25, 30, 45, 50, 65, 70, 85])\nprint(\"Original array:\", arr)\nprint(\"Even numbers:\", arr[arr % 2 == 0])\nOutput:\nOriginal array: [10 25 30 45 50 65 70 85]\nEven numbers: [10 30 50 70]\n\n\nModifying Array Elements\nYou can modify array elements using indexing and slicing.\nModifying a single element:\narr = np.array([1, 2, 3, 4, 5])\nprint(\"Original:\", arr)\narr[2] = 99\nprint(\"After modifying index 2:\", arr)\nOutput:\nOriginal: [1 2 3 4 5]\nAfter modifying index 2: [ 1  2 99  4  5]\nModifying multiple elements:\narr = np.array([1, 2, 3, 4, 5])\nprint(\"Original:\", arr)\narr[0:3] = [10, 20, 30]\nprint(\"After modifying first 3:\", arr)\nOutput:\nOriginal: [1 2 3 4 5]\nAfter modifying first 3: [10 20 30  4  5]\nModifying using a condition:\narr = np.array([10, 20, 99, 4, 5])\nprint(\"Original:\", arr)\narr[arr &gt; 50] = 50  # Cap all values at 50\nprint(\"After capping at 50:\", arr)\nOutput:\nOriginal: [10 20 99  4  5]\nAfter capping at 50: [10 20 50  4  5]"
  },
  {
    "objectID": "gtf-position.html",
    "href": "gtf-position.html",
    "title": "Graduate Teaching Fellow (GTF)",
    "section": "",
    "text": "The Graduate Teaching Fellow (GTF) position is an institutional program at IIT Gandhinagar designed to provide PhD students with opportunities to develop teaching and leadership skills. The program is part of the Institute’s doctoral curriculum, which emphasizes holistic development alongside thesis research work."
  },
  {
    "objectID": "gtf-position.html#overview",
    "href": "gtf-position.html#overview",
    "title": "Graduate Teaching Fellow (GTF)",
    "section": "",
    "text": "The Graduate Teaching Fellow (GTF) position is an institutional program at IIT Gandhinagar designed to provide PhD students with opportunities to develop teaching and leadership skills. The program is part of the Institute’s doctoral curriculum, which emphasizes holistic development alongside thesis research work."
  },
  {
    "objectID": "gtf-position.html#position-description",
    "href": "gtf-position.html#position-description",
    "title": "Graduate Teaching Fellow (GTF)",
    "section": "Position Description",
    "text": "Position Description\nA Graduate Teaching Fellow is a PhD student who assumes the responsibility of teaching a course independently as an instructor or tutor. The Fellow performs duties comparable to those of faculty members teaching similar courses, including course delivery, content preparation, and student evaluation.\nThe position carries a tenure of six months at a time."
  },
  {
    "objectID": "gtf-position.html#eligibility-and-selection",
    "href": "gtf-position.html#eligibility-and-selection",
    "title": "Graduate Teaching Fellow (GTF)",
    "section": "Eligibility and Selection",
    "text": "Eligibility and Selection\n\nEligibility Requirements\n\nCompletion of at least one year in the PhD program\nConsent from the doctoral supervisor\nRecommendation through appropriate academic channels\n\n\n\nSelection Process\nThe Dean of Academic Affairs establishes a mechanism for candidate selection. Positions are approved by the Director based on recommendations from the Dean. The number of positions available is determined by institutional needs and proposals from various disciplines."
  },
  {
    "objectID": "gtf-position.html#responsibilities",
    "href": "gtf-position.html#responsibilities",
    "title": "Graduate Teaching Fellow (GTF)",
    "section": "Responsibilities",
    "text": "Responsibilities\nGraduate Teaching Fellows undertake the following responsibilities:\n\nIndependent instruction of assigned courses\nPreparation and delivery of lectures\nCourse material development\nStudent assessment and evaluation\nCourse coordination and administration\n\nEach Fellow works under the mentorship of a faculty member who provides training and guidance for efficient performance of assigned duties."
  },
  {
    "objectID": "gtf-position.html#compensation-and-recognition",
    "href": "gtf-position.html#compensation-and-recognition",
    "title": "Graduate Teaching Fellow (GTF)",
    "section": "Compensation and Recognition",
    "text": "Compensation and Recognition\n\nFinancial Compensation\nUpon satisfactory completion of a course, Fellows receive a one-time honorarium at the rate of Rs. 5,000 per credit. The amount is pro-rated in cases where course responsibility is shared.\n\n\nOutstanding Graduate Teaching Fellow Award\nThe Institute recognizes exceptional performance through the Outstanding Graduate Teaching Fellow Award. Typically, one award is given each semester.\nAward Benefits:\n\nGrant of up to Rs. 20,000 for travel and professional activities\nFunding from the Excellence Fund in the Endowment Account\n\nNomination Requirements:\n\nNomination letter (maximum one page) commenting on the Fellow’s teaching efforts, preparation, clarity of presentation, and student engagement\nTeaching statement from the Fellow (maximum one page) outlining teaching philosophy, goal achievement, and learning outcomes\nStudent feedback statements from at least two students\nCourse reaction survey summary provided by the Academic Office\n\nSelection Process:\nA committee comprising the Dean (Academics), Associate Dean (PG), and at least two faculty members reviews nominations. The committee may conduct interviews with nominees as needed and recommends one Fellow to the Director for approval."
  },
  {
    "objectID": "gtf-position.html#professional-development-outcomes",
    "href": "gtf-position.html#professional-development-outcomes",
    "title": "Graduate Teaching Fellow (GTF)",
    "section": "Professional Development Outcomes",
    "text": "Professional Development Outcomes\nThe GTF program is structured to provide PhD students with experience in several areas relevant to academic careers:\nTeaching Experience: Independent course instruction differs substantially from teaching assistant or grading duties. Fellows gain experience in all aspects of course management and delivery.\nMentorship: Faculty mentors provide guidance on pedagogical methods, course planning, and professional standards in academic instruction.\nSkill Development: The position offers opportunities to develop communication abilities, time management, and coordination skills through direct responsibility for course outcomes.\nDocumentation: Fellows receive an experience certificate from the Dean of Academic Affairs upon completion of their tenure."
  },
  {
    "objectID": "gtf-position.html#program-rationale",
    "href": "gtf-position.html#program-rationale",
    "title": "Graduate Teaching Fellow (GTF)",
    "section": "Program Rationale",
    "text": "Program Rationale\nThe doctoral program at IIT Gandhinagar places emphasis on comprehensive student development beyond research specialization. The curriculum includes expectations for students to attend seminars, organize conferences, write research proposals, and participate in institutional activities.\nThe GTF program aligns with this broader educational philosophy by providing structured opportunities for students to engage in teaching activities under mentorship. The experience is designed to prepare students for future academic positions at research and teaching institutions.\n\nInformation based on Academic Affairs Advisory 41 (Version 1.1, March 2019), as approved and amended by the Senate of IIT Gandhinagar."
  }
]